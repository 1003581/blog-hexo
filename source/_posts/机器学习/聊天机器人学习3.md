---
title: 聊天机器人学习3
date: 2017-09-14 16:04:36
tags: chatbot
categories: 机器学习
---

[原文](http://www.shareditor.com/bloglistbytag/?tagname=%E8%87%AA%E5%B7%B1%E5%8A%A8%E6%89%8B%E5%81%9A%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA)

<!-- more -->
## 比TF-IDF更好的隐含语义索引模型是个什么鬼

我曾经的一篇文章曾说到0字节存储海量语料资源，那么从海量语料资源中找寻信息需要依赖于信息检索的方法，信息检索无论是谷歌还是百度都离不开TF-IDF算法，但TF-IDF是万能的吗？并不是，它简单有效但缺乏语义特征，本节介绍比TF-IDF还牛逼的含有语义特征的信息检索方法 

### TF-IDF

TF(term frequency)，表示一个词在一个文档中出现的频率；IDF(inverse document frequency)，表示一个词出现在多少个文档中。

它的思路是这样的：同一个词在短文档中出现的次数和在长文档中出现的次数一样多时，对于短文档价值更大；一个出现概率很低的词一旦出现在文档中，其价值应该大于其他普遍出现的词。

这在信息检索领域的向量模型中做相似度计算非常有效，屡试不爽，曾经是google老大哥发家的必杀技。但是在开发聊天机器人这个事情上看到了它的软肋，那就是它只是考虑独立的词上的事情，并没有任何语义信息在里面，因此我们需要选择加入了语义特征的更有效的信息检索模型。

### 隐含语义索引模型

在TF-IDF模型中，所有词构成一个高维的语义空间，每个文档在这个空间中被映射为一个点，这种方法维数一般比较高而且每个词作为一维割裂了词与词之间的关系。所以为了解决这个问题，我们要把词和文档同等对待，构造一个维数不高的语义空间，每个词和每个文档都是被映射到这个空间中的一个点。用数学来表示这个思想就是说，我们考察的概率即包括文档的概率，也包括词的概率，以及他们的联合概率。

为了加入语义方面的信息，我们设计一个假想的隐含类包括在文档和词之间，具体思路是这样的：

（1）选择一个文档的概率是p(d);

（2）找到一个隐含类的概率是p(z|d);

（3）生成一个词w的概率为p(w|z);

以上是假设的条件概率，我们根据观测数据能估计出来的是p(d, w)联合概率，这里面的z是一个隐含变量，表达的是一种语义特征。那么我们要做的就是利用p(d, w)来估计p(d)、p(z|d)和p(w|z)，最终根据p(d)、p(z|d)和p(w|z)来求得更精确的p(w, d)，即词与文档之间的相关度。

为了做更精确的估计，设计优化的目标函数是对数似然函数：

L=∑∑n(d, w) log P(d, w)
 

那么如何来通过机器学习训练这些概率呢？首先我们知道：

p(d, w) = p(d) × p(w|d)

而

p(w|d) = ∑p(w|z)p(z|d)

同时又有：

p(z|d) = p(z)p(d|z)/∑p(z)p(d|z)

那么

p(d, w) =p(d)×∑p(w|z) p(z)p(d|z)/∑p(z)p(d|z)=∑p(z)×p(w|z)×p(d|z)

下面我们采取EM算法，EM算法的精髓就是按照最大似然的原理，先随便拍一个分布参数，让每个人都根据分布归类到某一部分，然后根据这些归类来重新统计数目，按照最大似然估计分布参数，然后再重新归类、调参、估计、归类、调参、估计，最终得出最优解

那么我们要把每一个训练数据做归类，即p(z|d,w)，那么这个概率值怎么计算呢？

我们先拍一个p(z)、p(d|z)、p(w|z)

然后根据

p(z|d,w)=p(z)p(d|z)p(w|z)/∑p(z)p(d|z)p(w|z)，其中分子是一个z，分母是所有的z的和

这样计算出来的值是p(z|d,w)的最大似然估计的概率估计（这是E过程）

然后根据这个估计来对每一个训练样本做归类

根据归类好的数据统计出n(d,w)

然后我再根据以下公式来更新参数

p(z) = 1/R  ∑n(d,w)p(z|d,w)

p(d|z)=∑n(d,w)p(z|d,w) / ∑n(d,w)p(z|d,w)，其中分子是一个d的和，分母是所有的d的和，这样计算出来的值是p(d|z)的最大似然估计

p(w|z)=∑n(d,w)p(z|d,w) / ∑n(d,w)p(z|d,w)，其中分子是一个w的和，分母是所有的w的和，这样计算出来的值是p(w|z)的最大似然估计

最后重新计算p(z|d,w)：

p(z|d,w)=p(z)p(d|z)p(w|z)/∑p(z)p(d|z)p(w|z)

这是M的过程

不断重复上面EM的过程使得对数似然函数最大：

L=∑∑n(d, w) log P(d, w)

通过以上迭代就能得出最终的p(w, d)，即词与文档之间的相关度，后面就是利用相关度做检索的过程了

为了得到词词之间的相关度，我们用p(w, d)乘以它的转置，即

p(w,w) = p(w,d)×trans(p(w,d))

当用户查询query的关键词构成词向量Wq, 而文档d表示成词向量Wd，那么query和文档d的相关度就是：

R(query, d) = Wq×p(w,w)×Wd

这样把所有文档算出来的相关度从大到小排序就是搜索的排序结果

### 总结

综上就是隐含语义索引模型的内容，相比TF-IDF来说它加进了语义方面的信息、考虑了词与词之间的关系，是根据语义做信息检索的方法，更适合于研发聊天机器人做语料训练和分析，而TF-IDF更适合于完全基于独立的词的信息检索，更适合于纯文本搜索引擎

## 神奇算法之人工神经网络

深度学习是机器学习中较为流行的一种，而深度学习的基础是人工神经网络，那么人工神经网络的功能是不是像它的名字一样神奇呢？答案是肯定的，让我们一起见证一下这一神奇算法 

### 人工神经网络

人工神经网络是借鉴了生物神经网络的工作原理形成的一种数学模型，有关人工神经网络的原理、公式推导以及训练过程请见我的文章《机器学习教程 十二-神经网络模型的原理》

### 神奇用法之一

我们这样来设计我们的神经网络：由n个输入特征得出与输入特征几乎相同的n个结果，这样训练出的隐藏层可以得到意想不到的信息。

![img](http://upload-images.jianshu.io/upload_images/5952841-1c51628d65b4953d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

比如，在信息检索领域，我们需要通过模型训练来得出合理的排序模型，那么输入的特征可能有：文档质量、文档点击历史、文档前链数目、文档锚文本信息……，为了能找出这些特征中隐藏的信息，我们把隐藏层的神经元数目设置的少于输入特征的数目，经过大量样本的训练出能还原原始特征的模型，这样相当于我们用少于输入特征数目的信息还原出了全部特征，表面上是一种压缩，实际上通过这种方式就可以发现某些特征之间存在隐含的相关性，或者有某种特殊的关系。

同样的，我们还可以让隐藏层中的神经元数目多余输入特征的数目，这样经过训练得出的模型还可以展示出特征之间某种细节上的关联，比如我们对图像识别做这样的模型训练，在得出的隐藏层中能展示出多种特征之间的细节信息，如鼻子一定在嘴和眼睛中间。

这种让输出和输入一致的用法就是传说中的自编码算法。

### 神奇用法之二

人工神经网络模型通过多层神经元结构建立而成，每一层可以抽象为一种思维过程，经过多层思考，最终得出结论。举一个实际的例子：识别美女图片

按照人的思维过程，识别美女图片要经过这样的判断：1）图片类别（人物、风景……）；2）图片人物性别（男、女、其他……）；3）相貌如何（美女、恐龙、5分……）

那么在人工神经网络中，这个思考过程可以抽象成多个层次的计算：第一层计算提取图片中有关类别的特征，比如是否有形如耳鼻口手的元素，是否有形如蓝天白云绿草地的元素；第二层提取是否有胡须、胸部、长发以及面部特征等来判断性别；第三层提取五官、肤质、衣着等信息来确定颜值。为了让神经网络每一层有每一层专门要做的事情，需要在每一层的神经元中添加特殊的约束条件才能做到。人类的大脑是经过上亿年进化而成的，它的功能深不可及，某些效率也极高，而计算机在某些方面效率比人脑要高很多，两种结合起来一切皆有可能。

这种通过很多层提取特定特征来做机器学习的方法就是传说中的深度学习。

### 神奇用法之三

讲述第三种用法之前我们先讲一下什么是卷积运算。卷积英文是convolution(英文含义是：盘绕、弯曲、错综复杂)，数学表达是：

![img](http://upload-images.jianshu.io/upload_images/5952841-f02401f824ac8160.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

数学上不好理解，我们可以通俗点来讲：卷积就相当于在一定范围内做平移并求平均值。比如说回声可以理解为原始声音的卷积结果，因为回声是原始声音经过很多物体反射回来声音揉在一起。再比如说回声可以理解为把信号分解成无穷多的冲击信号，然后再进行冲击响应的叠加。再比如说把一张图像做卷积运算，并把计算结果替换原来的像素点，可以实现一种特殊的模糊，这种模糊其实是一种新的特征提取，提取的特征就是图像的纹路。总之卷积就是先打乱，再叠加。

下面我们在看上面的积分公式，需要注意的是这里是对τ积分，不是对x积分。也就是说对于固定的x，找到x附近的所有变量，求两个函数的乘积，并求和。

下面回归正题，在神经网络里面，我们设计每个神经元计算输出的公式是卷积公式，这样相当于神经网络的每一层都会输出一种更高级的特征，比如说形状、脸部轮廓等。这种神经网络叫做卷积神经网络。

继续深入主题，在自然语言中，我们知道较近的上下文词语之间存在一定的相关性，由于标点、特殊词等的分隔使得在传统自然语言处理中会脱离词与词之间的关联，结果丢失了一部分重要信息，利用卷积神经网络完全可以做多元(n-gram)的计算，不会损失自然语言中的临近词的相关性信息。这种方法对于语义分析、语义聚类等都有非常好的效果。

这种神奇用法就是传说中的CNN

### 总结

神经网络因为其层次和扩展性的强大，有着非常多的神奇用法和非常广泛的应用，因为希望聊天机器人能够具有智能，就不得不寻找能够承载智能的方法，神经网络是其中一个，沿着这个网络，让我们继续探索。

## 用CNN做深度学习

自动问答系统中深度学习的应用较多是RNN，这归因于它天然利用时序建模。俗话说知己知彼百战不殆，为了理解RNN，我们先来了解一下CNN，通过手写数字识别案例来感受一下CNN最擅长的局部感知能力 

### 卷积神经网络(CNN)

卷积神经网络(Convolutional Neural Network,CNN)是将二维离散卷积运算和人工神经网络相结合的一种深度神经网络。它的特点是可以自动提取特征。有关卷积神经网络的数学原理和训练过程请见我的另一篇文章《机器学习教程 十五-细解卷积神经网络》。

### 手写数字识别

为了试验，我们直接采用http://yann.lecun.com/exdb/mnist/中的手写数据集，下载到的手写数据集数据文件是用二进制以像素为单位保存的几万张图片文件，通过我的github项目https://github.com/warmheartli/ChatBotCourse中的read_images.c把图片打印出来是像下面这样的输出：

具体文件格式和打印方式请见我的另一篇基于简单的softmax模型的机器学习算法文章《机器学习教程 十四-利用tensorflow做手写数字识别》中的讲解

### 多层卷积网络设计

为了对mnist手写数据集做训练，我们设计这样的多层卷积网络：

![img](http://upload-images.jianshu.io/upload_images/5952841-15b27baeb5baec3b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

第一层由一个卷积和一个max pooling完成，其中卷积运算的“视野”是5×5的像素范围，卷积使用1步长、0边距的模板(保证输入输出是同一个大小)，1个输入通道(因为图片是灰度的，单色)，32个输出通道(也就是设计32个特征)。由于我们通过上面read_images.c的打印可以看到每张图片都是28×28像素，那么第一次卷积输出也是28×28大小。max pooling采用2×2大小的模板，那么池化后输出的尺寸就是14×14，因为一共有32个通道，所以一张图片的输出一共是14×14×32=6272像素

第二层同样由一个卷积和一个max pooling完成，和第一层不同的是输入通道有32个(对应第一层的32个特征)，输出通道我们设计64个(即输出64个特征)，因为这一层的输入是每张大小14×14，所以这一个卷积层输出也是14×14，再经过这一层max pooling，输出大小就是7×7，那么一共输出像素就是7×7×64=3136

第三层是一个密集连接层，我们设计一个有1024个神经元的全连接层，这样就相当于第二层输出的7×7×64个值都作为这1024个神经元的输入

为了让算法更“智能”，我们把这些神经元的激活函数设计为ReLu函数，即如下图像中的蓝色(其中绿色是它的平滑版g(x)=log(1+e^x))：

![img](http://upload-images.jianshu.io/upload_images/5952841-b4794a1b1df7f87a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

最终的输出层，我们以第三层的1024个输出为输入，设计一个softmax层，输出10个概率值

### tensorflow代码实现

```python
# coding:utf-8

import sys
reload(sys)
sys.setdefaultencoding( "utf-8" )

from tensorflow.examples.tutorials.mnist import input_data
import tensorflow as tf

flags = tf.app.flags
FLAGS = flags.FLAGS
flags.DEFINE_string('data_dir', './', 'Directory for storing data')

mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)

# 初始化生成随机的权重(变量)，避免神经元输出恒为0
def weight_variable(shape):
    # 以正态分布生成随机值
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)

# 初始化生成随机的偏置项(常量)，避免神经元输出恒为0
def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)

# 卷积采用1步长，0边距，保证输入输出大小相同
def conv2d(x, W):
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')

# 池化采用2×2模板
def max_pool_2x2(x):
    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],
        strides=[1, 2, 2, 1], padding='SAME')

# 28*28=784
x = tf.placeholder(tf.float32, [None, 784])
# 输出类别共10个：0-9
y_ = tf.placeholder("float", [None,10])

# 第一层卷积权重，视野是5*5，输入通道1个，输出通道32个
W_conv1 = weight_variable([5, 5, 1, 32])
# 第一层卷积偏置项有32个
b_conv1 = bias_variable([32])

# 把x变成4d向量，第二维和第三维是图像尺寸，第四维是颜色通道数1
x_image = tf.reshape(x, [-1,28,28,1])

h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
h_pool1 = max_pool_2x2(h_conv1)

# 第二层卷积权重，视野是5*5，输入通道32个，输出通道64个
W_conv2 = weight_variable([5, 5, 32, 64])
# 第二层卷积偏置项有64个
b_conv2 = bias_variable([64])

h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
h_pool2 = max_pool_2x2(h_conv2)

# 第二层池化后尺寸编程7*7，第三层是全连接，输入是64个通道，输出是1024个神经元
W_fc1 = weight_variable([7 * 7 * 64, 1024])
# 第三层全连接偏置项有1024个
b_fc1 = bias_variable([1024])

h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])
h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)

# 按float做dropout，以减少过拟合
keep_prob = tf.placeholder("float")
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

# 最后的softmax层生成10种分类
W_fc2 = weight_variable([1024, 10])
b_fc2 = bias_variable([10])

y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)

cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))
# Adam优化器来做梯度最速下降
train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))

sess = tf.InteractiveSession()
sess.run(tf.initialize_all_variables())

for i in range(20000):
    batch = mnist.train.next_batch(50)
    if i%100 == 0:
        train_accuracy = accuracy.eval(feed_dict={
            x:batch[0], y_: batch[1], keep_prob: 1.0})
        print "step %d, training accuracy %g"%(i, train_accuracy)
    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})

print "test accuracy %g"%accuracy.eval(feed_dict={
    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})
```

输出结果

```
[root@centos $] python digital_recognition_cnn.py
Extracting ./train-images-idx3-ubyte.gz
Extracting ./train-labels-idx1-ubyte.gz
Extracting ./t10k-images-idx3-ubyte.gz
Extracting ./t10k-labels-idx1-ubyte.gz
step 0, training accuracy 0.14
step 100, training accuracy 0.86
step 200, training accuracy 0.9
step 300, training accuracy 0.86
step 400, training accuracy 1
step 500, training accuracy 0.92
step 600, training accuracy 1
step 700, training accuracy 0.96
step 800, training accuracy 0.88
step 900, training accuracy 1
step 1000, training accuracy 0.96
step 1100, training accuracy 0.98
step 1200, training accuracy 0.94
step 1300, training accuracy 0.92
step 1400, training accuracy 0.98
……
```

最终准确率大概能达到99.2%

## 将深度学习应用到NLP

由于语言相比于语音、图像来说，是一种更高层的抽象，因此不是那么适合于深度学习，但是经过人类不断探索，也发现无论多么高层的抽象总是能通过更多底层基础的累积而碰触的到，本文介绍如何将深度学习应用到NLP所必须的底层基础 

### 词向量

自然语言需要数学化才能够被计算机认识和计算。数学化的方法有很多，最简单的方法是为每个词分配一个编号，这种方法已经有多种应用，但是依然存在一个缺点：不能表示词与词的关系。

词向量是这样的一种向量[0.1, -3.31, 83.37, 93.0, -18.37, ……]，每一个词对应一个向量，词义相近的词，他们的词向量距离也会越近(欧氏距离、夹角余弦)

词向量有一个优点，就是维度一般较低，一般是50维或100维，这样可以避免维度灾难，也更容易使用深度学习

### 词向量如何训练得出呢？

首先要了解一下语言模型，语言模型相关的内容请见我另外一篇文章《自己动手做聊天机器人 十三-把语言模型探究到底》。语言模型表达的实际就是已知前n-1个词的前提下，预测第n个词的概率。

词向量的训练是一种无监督学习，也就是没有标注数据，给我n篇文章，我就可以训练出词向量。

基于三层神经网络构建n-gram语言模型(词向量顺带着就算出来了)的基本思路：

![img](http://upload-images.jianshu.io/upload_images/5952841-22d52378fe257663.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

最下面的w是词，其上面的C(w)是词向量，词向量一层也就是神经网络的输入层(第一层)，这个输入层是一个(n-1)×m的矩阵，其中n-1是词向量数目，m是词向量维度

第二层(隐藏层)是就是普通的神经网络，以H为权重，以tanh为激活函数

第三层(输出层)有|V|个节点，|V|就是词表的大小，输出以U为权重，以softmax作为激活函数以实现归一化，最终就是输出可能是某个词的概率。

另外，神经网络中有一个技巧就是增加一个从输入层到输出层的直连边(线性变换)，这样可以提升模型效果，这个变换矩阵设为W

假设C(w)就是输入的x，那么y的计算公式就是y = b + Wx + Utanh(d+Hx)

这个模型里面需要训练的有这么几个变量：C、H、U、W。利用梯度下降法训练之后得出的C就是生成词向量所用的矩阵，C(w)表示的就是我们需要的词向量

上面是讲解词向量如何“顺带”训练出来的，然而真正有用的地方在于这个词向量如何进一步应用。

### 词向量的应用

第一种应用是找同义词。具体应用案例就是google的word2vec工具，通过训练好的词向量，指定一个词，可以返回和它cos距离最相近的词并排序。

第二种应用是词性标注和语义角色标注任务。具体使用方法是：把词向量作为神经网络的输入层，通过前馈网络和卷积网络完成。

第三种应用是句法分析和情感分析任务。具体使用方法是：把词向量作为递归神经网络的输入。

第四种应用是命名实体识别和短语识别。具体使用方法是：把词向量作为扩展特征使用。

另外词向量有一个非常特别的现象：C(king)-C(queue)≈C(man)-C(woman)，这里的减法就是向量逐维相减，换个表达方式就是：C(king)-C(man)+C(woman)和它最相近的向量就是C(queue)，这里面的原理其实就是：语义空间中的线性关系。基于这个结论相信会有更多奇妙的功能出现。

## google的文本挖掘深度学习工具word2vec的实现原理

词向量是将深度学习应用到NLP的根基，word2vec是如今使用最广泛最简单有效的词向量训练工具，那么它的实现原理是怎样的呢？本文将从原理出发来介绍word2vec 

### 你是如何记住一款车的

问你这样一个问题：如果你大脑有很多记忆单元，让你记住一款白色奥迪Q7运动型轿车，你会用几个记忆单元？你也许会用一个记忆单元，因为这样最节省你的大脑。那么我们再让你记住一款小型灰色雷克萨斯，你会怎么办？显然你会用另外一个记忆单元来记住它。那么如果让你记住所有的车，你要耗费的记忆单元就不再是那么少了，这种表示方法叫做localist representation。这时你可能会换另外一种思路：我们用几个记忆单元来分别识别大小、颜色、品牌等基础信息，这样通过这几个记忆单元的输出，我们就可以表示出所有的车型了。这种表示方法叫做distributed representation，词向量就是一种用distributed representation表示的向量

### localist representation与distributed representation

localist representation中文释义是稀疏表达，典型的案例就是one hot vector，也就是这样的一种向量表示：

[1, 0, 0, 0, 0, 0……]表示成年男子

[0, 1, 0, 0, 0, 0……]表示成年女子

[0, 0, 1, 0, 0, 0……]表示老爷爷

[0, 0, 0, 1, 0, 0……]表示老奶奶

[0, 0, 0, 0, 1, 0……]表示男婴

[0, 0, 0, 0, 0, 1……]表示女婴

……

每一类型用向量中的一维来表示

 

而distributed representation中文释义是分布式表达，上面的表达方式可以改成：

性别 老年 成年 婴儿

[1,       0,      1,      0]表示成年男子

[0,       0,      1,      0]表示成年女子

[1,       1,      0,      0]表示老爷爷

[0,       1,      0,      0]表示老奶奶

[1,       0,      0,      1]表示男婴

[0,       0,      0,      1]表示女婴

如果我们想表达男童和女童，只需要增加一个特征维度即可

### word embedding

翻译成中文叫做词嵌入，这里的embedding来源于范畴论，在范畴论中称为morphism(态射)，态射表示两个数学结构中保持结构的一种过程抽象，比如“函数”、“映射”，他们都是表示一个域和另一个域之间的某种关系。

范畴论中的嵌入(态射)是要保持结构的，而word embedding表示的是一种“降维”的嵌入，通过降维避免维度灾难，降低计算复杂度，从而更易于在深度学习中应用。

理解了distributed representation和word embedding的概念，我们就初步了解了word2vec的本质，它其实是通过distributed representation的表达方式来表示词，而且通过降维的word embedding来减少计算量的一种方法

### word2vec中的神经网络

word2vec中做训练主要使用的是神经概率语言模型，这需要掌握一些基础知识，否则下面的内容比较难理解，关于神经网络训练词向量的基础知识我在《自己动手做聊天机器人 二十四-将深度学习应用到NLP》中有讲解，可以参考，这里不再赘述。

在word2vec中使用的最重要的两个模型是CBOW和Skip-gram模型，下面我们分别来介绍这两种模型

### CBOW模型

CBOW全称是Continuous Bag-of-Words Model，是在已知当前词的上下文的前提下预测当前词

![img](http://upload-images.jianshu.io/upload_images/5952841-ddf7d06f1f45bbdc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

CBOW模型的神经网络结构设计如下：

输入层：词w的上下文一共2c个词的词向量

投影层：将输入层的2c个向量做求和累加

输出层：一个霍夫曼树，其中叶子节点是语料中出现过的词，权重是出现的次数

我们发现这一设计相比《自己动手做聊天机器人 二十四-将深度学习应用到NLP》中讲到的神经网络模型把首尾相接改成了求和累加，这样减少了维度；去掉了隐藏层，这样减少了计算量；输出层由softmax归一化运算改成了霍夫曼树；这一系列修改对训练的性能有很大提升，而效果不减，这是独到之处。

### 基于霍夫曼树的Hierarchical Softmax技术

上面的CBOW输出层为什么要建成一个霍夫曼树呢？因为我们是要基于训练语料得到每一个可能的w的概率。那么具体怎么得到呢？我们先来看一下这个霍夫曼树的例子：

[图片上传失败...(image-c2e87d-1513912722638)]

在这个霍夫曼树中，我们以词足球为例，走过的路径图上容易看到，其中非根节点上的θ表示待训练的参数向量，也就是要达到这种效果：当在投射层产出了一个新的向量x，那么我通过逻辑回归公式：

σ(xTθ) = 1/(1+e^(-xTθ))

就可以得出在每一层被分到左节点(1)还是右节点(0)的概率分别是

p(d|x,θ) = 1-σ(xTθ)

和

p(d|x,θ) = σ(xTθ)

那么就有：

p(足球|Context(足球)) = ∏ p(d|x,θ)

现在模型已经有了，下面就是通过语料来训练v(Context(w))、x和θ的过程了

我们以对数似然函数为优化目标，盗取一个网上的推导公式：

![img](http://upload-images.jianshu.io/upload_images/5952841-32742753a6e71ee6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

假设两个求和符号里面的部分记作L(w, j)，那么有

![img](http://upload-images.jianshu.io/upload_images/5952841-8993e69017560921.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

于是θ的更新公式：

![img](http://upload-images.jianshu.io/upload_images/5952841-20181b009e70fc8f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

同理得出x的梯度公式：

![img](http://upload-images.jianshu.io/upload_images/5952841-0b879fd659cc70cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

因为x是多个v的累加，word2vec中v的更新方法是：

![img](http://upload-images.jianshu.io/upload_images/5952841-0e4d015cccb3bf4c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

想想机器学习真是伟大，整个模型从上到下全是未知数，竟然能算出来我真是服了

### Skip-gram模型

Skip-gram全称是Continuous Skip-gram Model，是在已知当前词的情况下预测上下文

![img](http://upload-images.jianshu.io/upload_images/5952841-68187e2d49755dd1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

Skip-gram模型的神经网络结构设计如下：

输入层：w的词向量v(w)

投影层：依然是v(w)，就是一个形式

输出层：和CBOW一样的霍夫曼树

后面的推导公式和CBOW大同小异，其中θ和v(w)的更新公式除了把符号名从x改成了v(w)之外完全一样，如下：

![img](http://upload-images.jianshu.io/upload_images/5952841-6dabe53669fe3ae5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 体验真实的word2vec

首先我们从网上下载一个源码，因为google官方的svn库已经不在了，所以只能从csdn下了，但是因为还要花积分才能下载，所以我干脆分享到了我的git上( https://github.com/warmheartli/ChatBotCourse/tree/master/word2vec )，大家可以直接下载

下载下来后直接执行make编译(如果是mac系统要把代码里所有的#include <malloc.h>替换成#include <sys/malloc.h>)

编译后生成word2vec、word2phrase、word-analogy、distance、compute-accuracy几个二进制文件

我们先用word2vec来训练

首先我们要有训练语料，其实就是已经切好词(空格分隔)的文本，比如我们已经有了这个文本文件叫做train.txt，内容是"人工 智能 一直 以来 是 人类 的 梦想 造 一台 可以 为 你 做 一切 事情 并且 有 情感 的 机器 人"并且重复100遍

执行

```
./word2vec -train train.txt -output vectors.bin -cbow 0 -size 200 -window 5 -negative 0 -hs 1 -sample 1e-3 -thread 12 -binary 1
```

会生成一个vectors.bin文件，这个就是训练好的词向量的二进制文件，利用这个文件我们可以求近义词了，执行：

```
./distance vectors.bin
Enter word or sentence (EXIT to break): 人类

Word: 人类  Position in vocabulary: 6

                                              Word       Cosine distance
------------------------------------------------------------------------
                                            可以              0.094685
                                               为            0.091899
                                            人工              0.088387
                                            机器              0.076216
                                            智能              0.073093
                                            情感              0.071088
                                               做            0.059367
                                            一直              0.056979
                                            以来              0.049426
                                            一切              0.042201
                                              </s>          0.025968
                                            事情              0.014169
                                               的            0.003633
                                               是            -0.012021
                                               有            -0.014790
                                            一台              -0.021398
                                               造            -0.031242
                                               人            -0.043759
                                               你            -0.072834
                                            梦想              -0.086062
                                            并且              -0.122795
……
```

如果你有很丰富的语料，那么结果会很漂亮

## 图解递归神经网络(RNN)

聊天机器人是需要智能的，而如果他记不住任何信息，就谈不上智能，递归神经网络是一种可以存储记忆的神经网络，LSTM是递归神经网络的一种，在NLP领域应用效果不错，本节我们来介绍RNN和LSTM 

### 递归神经网络

递归神经网络（RNN）是两种人工神经网络的总称。一种是时间递归神经网络（recurrent neural network），另一种是结构递归神经网络（recursive neural network）。时间递归神经网络的神经元间连接构成有向图，而结构递归神经网络利用相似的神经网络结构递归构造更为复杂的深度网络。两者训练的算法不同，但属于同一算法变体（百度百科）。本节我们重点介绍时间递归神经网络，下面提到RNN特指时间递归神经网络。

### 时间递归神经网络

传统的神经网络叫做FNN(Feed-Forward Neural Networks)，也就是前向反馈神经网络，有关传统神经网络的介绍请见《机器学习教程 十二-神经网络模型的原理》，RNN是在此基础上引入了定向循环，也就是已神经元为节点组成的图中存在有向的环，这种神经网络可以表达某些前后关联关系，事实上，真正的生物神经元之间也是存在这种环形信息传播的，RNN也是神经网络向真实生物神经网络靠近的一个进步。一个典型的RNN是这样的：

![img](http://upload-images.jianshu.io/upload_images/5952841-52c9d6bd450b16f3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

图中隐藏层中的节点之间构成了全连接，也就是一个隐藏层节点的输出可以作为另一个隐藏层节点甚至它自己的输入

这种结构可以抽象成：

![img](http://upload-images.jianshu.io/upload_images/5952841-da9dd2607ec5b290.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中U、V、W都是变换概率矩阵，x是输入，o是输出

比较容易看出RNN的关键是隐藏层，因为隐藏层能够捕捉到序列的信息，也就是一种记忆的能力

在RNN中U、V、W的参数都是共享的，也就是只需要关注每一步都在做相同的事情，只是输入不同，这样来降低参数个数和计算量

RNN在NLP中的应用比较多，因为语言模型就是在已知已经出现的词的情况下预测下一个词的概率的，这正是一个有时序的模型，下一个词的出现取决于前几个词，刚好对应着RNN中隐藏层之间的内部连接

### RNN的训练方法

RNN的训练方法和传统神经网络一样，都是使用BP误差反向传播算法来更新和训练参数。

因为从输入到最终的输出中间经过了几步是不确定的，因此为了计算方便，我们利用时序的方式来做前向计算，我们假设x表示输入值，s表示输入x经过U矩阵变换后的值，h表示隐藏层的激活值，o表示输出层的值, f表示隐藏层的激活函数，g表示输出层的激活函数：

当t=0时，输入为x0, 隐藏层为h0

当t=1时，输入为x1, s1 = Ux1+Wh0, h1 = f(s1), o1 = g(Vh1)

当t=2时，s2 = Ux2+Wh1, h2 = f(s2), o2 = g(Vh2)

以此类推，st = Uxt + Wh(t-1), ht = f(st), ot = g(Vht)

这里面h=f(现有的输入+过去记忆总结)是对RNN的记忆能力的全然体现

通过这样的前向推导，我们是不是可以对RNN的结构做一个展开，成如下的样子：

![img](http://upload-images.jianshu.io/upload_images/5952841-3a9495b69db1f534.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这样从时序上来看更直观明了

下面就是反向修正参数的过程了，每一步输出o和实际的o值总会有误差，和传统神经网络反向更新的方法一样，用误差来反向推导，利用链式求导求出每层的梯度，从而更新参数，反向推导过程中我们还是把神经网络结构看成展开后的样子：

![img](http://upload-images.jianshu.io/upload_images/5952841-ebac8e8ab5bbf81b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

根据链式求导法则，得出隐藏层的残差计算公式为：

![img](http://upload-images.jianshu.io/upload_images/5952841-fcbe147dd6474084.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

因此W和U的梯度就是：

![img](http://upload-images.jianshu.io/upload_images/5952841-ac1a36d80fd00a92.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

LSTM(Long Short Tem Momery networks)

特别讲解一下LSTM是因为LSTM是一种特别的RNN，它是RNN能得到成功应用的关键，当下非常流行。RNN存在一个长序列依赖(Long-Term Dependencies)的问题：下一个词的出现概率和非常久远的之前的词有关，但考虑到计算量的问题，我们会对依赖的长度做限制，LSTM很好的解决了这个问题，因为它专门为此而设计。

借用 http://colah.github.io/posts/2015-08-Understanding-LSTMs/ 中经典的几张图来说明下，第一张图是传统RNN的另一种形式的示意图，它只包含一个隐藏层，以tanh为激发函数，这里面的“记忆”体现在t的滑动窗口上，也就是有多少个t就有多少记忆，如下图

![img](http://upload-images.jianshu.io/upload_images/5952841-f513773fbbbbaa7d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

那么我们看LSTM的设计，如下，这里面有一些符号，其中黄色方框是神经网络层(意味着有权重系数和激活函数，σ表示sigmoid激活函数，tanh表示tanh激活函数)，粉红圆圈表示矩阵运算(矩阵乘或矩阵加)

![img](http://upload-images.jianshu.io/upload_images/5952841-6063968ed749a934.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这里需要分部分来说，下面这部分是一个历史信息的传递和记忆，其中粉红×是就像一个能调大小的阀门(乘以一个0到1之间的系数)，下面的第一个sigmoid层计算输出0到1之间的系数，作用到粉红×门上，这个操作表达上一阶段传递过来的记忆保留多少，忘掉多少

其中的sigmoid公式如下：

![img](http://upload-images.jianshu.io/upload_images/5952841-7f65b783dadb50ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

可以看出忘掉记忆多少取决于上一隐藏层的输出h{t-1}和本层的输入x{t}

下面这部分是由上一层的输出h{t-1}和本层的输入x{t}得出的新信息，存到记忆中：

![img](http://upload-images.jianshu.io/upload_images/5952841-c8c5c0ce4325b883.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中包括计算输出值Ct部分的tanh神经元和计算比例系数的sigmoid神经元（这里面既存在sigmoid又存在tanh原因在于sigmoid取值范围是[0,1]天然作为比例系数，而tanh取值范围是[-1,1]可以作为一个输出值）。其中i{t}和Ct计算公式如下：

![img](http://upload-images.jianshu.io/upload_images/5952841-246fddea5f62d32c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

那么Ct输出就是：

![img](http://upload-images.jianshu.io/upload_images/5952841-0b25dc57f67f4781.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

下面部分是隐藏层输出h的计算部分，它考虑了当前拥有的全部信息（上一时序隐藏层的输出、本层的输入x和当前整体的记忆信息），其中本单元状态部分C通过tanh激活并做一个过滤(上一时序输出值和当前输入值通过sigmoid激活后的系数)

![img](http://upload-images.jianshu.io/upload_images/5952841-7160b20ee062f2ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

计算公式如下：

![img](http://upload-images.jianshu.io/upload_images/5952841-68b2f15f3c2efd9e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

LSTM非常适合在NLP领域应用，比如一句话出现的词可以认为是不同时序的输入x，而在某一时间t出现词A的概率可以通过LSTM计算，因为词A出现的概率是取决于前面出现过的词的，但取决于前面多少个词是不确定的，这正是LSTM所做的存储着记忆信息C，使得能够得出较接近的概率。

### 总结

RNN就是这样一种神经网络，它让隐藏层自身之间存在有向环，从而更接近生物神经网络，也具有了存储记忆的能力，而LSTM作为RNN中更有实用价值的一种，通过它特殊的结构设计实现了永久记忆留存，更适合于NLP，这也为将深度学习应用到自然语言处理开了先河，有记忆是给聊天机器人赋予智能的前提，这也为我们的聊天机器人奠定了实践基础。
