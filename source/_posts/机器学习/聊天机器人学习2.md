---
title: 聊天机器人学习2
date: 2017-09-14 16:04:36
tags: chatbot
categories: 机器学习
---

[原文](http://www.shareditor.com/bloglistbytag/?tagname=%E8%87%AA%E5%B7%B1%E5%8A%A8%E6%89%8B%E5%81%9A%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA)

<!-- more -->
## 0字节存储海量语料资源

基于语料做机器学习需要海量数据支撑，如何能不存一点数据获取海量数据呢？我们可以以互联网为强大的数据后盾，搜索引擎为我们提供了高效的数据获取来源，结构化的搜索结果展示为我们实现了天然的特征基础，唯一需要我们做的就是在海量结果中选出我们需要的数据，本节我们来探索如何利用互联网拿到我们所需的语料资源 

### 关键词提取

互联网资源无穷无尽，如何获取到我们所需的那部分语料库呢？这需要我们给出特定的关键词，而基于问句的关键词提取上一节已经做了介绍，利用pynlpir库可以非常方便地实现关键词提取，比如：

```python
# coding:utf-8

import sys
reload(sys)
sys.setdefaultencoding( "utf-8" )

import pynlpir

pynlpir.open()
s = '怎么才能把电脑里的垃圾文件删除'

key_words = pynlpir.get_key_words(s, weighted=True)
for key_word in key_words:
    print key_word[0], '\t', key_word[1]

pynlpir.close()
```

提取出的关键词如下：

```
电脑     2.0
垃圾     2.0
文件     2.0
删除     1.0
```

我们基于这四个关键词来获取互联网的资源就可以得到我们所需要的语料信息

### 充分利用搜索引擎

有了关键词，想获取预料信息，还需要知道几大搜索引擎的调用接口，首先我们来探索一下百度，百度的接口是这样的：

[https://www.baidu.com/s?wd=机器学习 数据挖掘 信息检索](https://www.baidu.com/s?wd=机器学习 数据挖掘 信息检索)

把wd参数换成我们的关键词就可以拿到相应的结果，我们用程序来尝试一下：

首先创建scrapy工程，执行：

```
scrapy startproject baidu_search
```

自动生成了baidu_search目录和下面的文件（不知道怎么使用scrapy，请见我的文章[教你成为全栈工程师(Full Stack Developer) 三十-十分钟掌握最强大的python爬虫）](http://www.shareditor.com/blogshow/?blogId=43)

创建`baidu_search/baidu_search/spiders/baidu_search.py`文件，内容如下：

```python
# coding:utf-8

import sys
reload(sys)
sys.setdefaultencoding( "utf-8" )

import scrapy

class BaiduSearchSpider(scrapy.Spider):
    name = "baidu_search"
    allowed_domains = ["baidu.com"]
    start_urls = [
            "https://www.baidu.com/s?wd=机器学习"
    ]

    def parse(self, response):
        print response.body
```

这样我们的抓取器就做好了，进入`baidu_search/baidu_search/`目录，执行：

```
scrapy crawl baidu_search
```

我们发现返回的数据是空，下面我们修改配置来解决这个问题，修改`settings.py`文件，把`ROBOTSTXT_OBEY`改为

```
ROBOTSTXT_OBEY = False
```

并把USER_AGENT设置为：

```
USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'
```

为了避免抓取hang住，我们添加如下超时设置：

```
DOWNLOAD_TIMEOUT = 5
```

再次执行

```
scrapy crawl baidu_search
```

这次终于可以看到大片大片的html了，我们临时把他写到文件中，修改parse()函数如下：

```
    def parse(self, response):
        filename = "result.html"
        with open(filename, 'wb') as f:
            f.write(response.body)
```

重新执行后生成了result.html，我们用浏览器打开本地文件。

### 语料提取

上面得到的仅是搜索结果，它只是一种索引，真正的内容需要进入到每一个链接才能拿到，下面我们尝试提取出每一个链接并继续抓取里面的内容，那么如何提取链接呢，我们来分析一下result.html这个抓取百度搜索结果文件

我们可以看到，每一条链接都是嵌在class=c-container这个div里面的一个h3下的a标签的href属性

所以我们的提取规则就是：

```
hrefs = response.selector.xpath('//div[contains(@class, "c-container")]/h3/a/@href').extract()
```

修改parse()函数并添加如下代码：

```python
        hrefs = response.selector.xpath('//div[contains(@class, "c-container")]/h3/a/@href').extract()
        for href in hrefs:
            print href
```

执行打印出：

```
......
2017-08-21 22:26:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?wd=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0> (referer: None)
http://www.baidu.com/link?url=evTTw58FTb9_-gGNUuOgv3_coiSBhpi-4ZQKoLQZvPXbsj3kfzROuH4cm1CxPqSqWl_E1vamGMyOHAO1G3jXEMyXoF0fHSbGmWzX99tDkCnTLIrR8-oysqnEb7VzI2EC
http://www.baidu.com/link?url=T0QwDAL_1ypuWLshTeFSh9Tkl3wQVwQFaY_sjIRY1G7TaBq2YC1SO2T2mLsLTiC2tFJ878OihtiIkEgepJNG-q
http://www.baidu.com/link?url=xh1H8nE-T4CeAq_A3hGbNwHfDKs6K7HCprH6DrT29yv6vc3t_DSk6zq7_yekiL_iyd9rGxMSONN_wJDwpjqNAK
http://www.baidu.com/link?url=LErVCtq1lVKbh9QAAjdg37GW1_B3y8g_hjoafChZ90ycuG3razgc9X_lE4EgiibkjzCPImQOxTl-b5LOwZshtxSf7sCTOlBpLRjcMyG2Fc7
http://www.baidu.com/link?url=vv_KA9CNJidcGTV1SE096O9gXqVC7yooCDMVvCXg9Vg22nZW2eBIq9twWSFh17VVYqNJ26wkRJ7XKuTsD3-qFDdi5_v-AZZrDeNI07aZaYG
http://www.baidu.com/link?url=dvMowOWWPV3kEZxzy1q7W2OOBuph0kI7FuZTwp5-ejsU-f-Aiif-Xh7U4zx-qoKW_O1fWVwutJuOtEWr2A7cwq
http://www.baidu.com/link?url=evTTw58FTb9_-gGNUuOgvHC_aU-RmA0z8vc7dTH6-tgzMKuehldik7N_vi0s4njGvLo13id-kuGnVhhmfLV3051DpQ7CLO22rKxCuCicyTe
http://www.baidu.com/link?url=QDkm6sUGxi-qYC6XUYR2SWB_joBm_-25-EXUSyLm9852gQRu41y-u_ZPG7SKhjs6U_l99ZcChBNXz4Ub5a0RJa
http://www.baidu.com/link?url=Y9Qfk4m6Hm8gQ-7XgUzAl5b-bgxNOBn_e_7v9g6XHmsZ_4TK8Mm1E7ddQaDUAdSCxjgJ_Ao-vYA6VZjfVuIaX58PlWo_rV8AqhVrDA1Bd0W
http://www.baidu.com/link?url=X7eU-SPPEylAHBTIGhaET0DEaLEaTYEjknjI2_juK7XZ2D2eb90b1735tVu4vORourw_E68sZF8P2O4ghTVcQa
2017-08-21 22:26:17 [scrapy.core.engine] INFO: Closing spider (finished)
...
```

下面我们把这些url添加到抓取队列中继续抓取，修改baidu_search.py文件，如下：

```
    def parse(self, response):
        hrefs = response.selector.xpath('//div[contains(@class, "c-container")]/h3/a/@href').extract()
        for href in hrefs:
            yield scrapy.Request(href, callback=self.parse_url)

    def parse_url(self, response):
        print len(response.body)
```

抓取效果如下：

```
2017-08-21 22:29:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.baidu.com/s?wd=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0> (referer: None)
2017-08-21 22:29:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.leiphone.com/news/201609/SJGulTsdGcisR8Wz.html> from <GET http://www.baidu.com/link?url=iQ6rC78zm48BmQQ2GcK8ffphKCITxraUEcyS1waz7_yn5JLl5ZJgKerMXO1yQozfC9vxN0C89iU0Rd2nwEFXoEj1doqsbCupuDARBEtlHW3>
2017-08-21 22:29:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://blog.jobbole.com/tag/machinelearning/> from <GET http://www.baidu.com/link?url=XIi6gyYcL8XtxD7ktWfZssm0Z2nO-TY5xzrHI8TLOMnUWj8a9u4swB3KI66yhT4wrqXhjxyRq95s5PkHlsplwq>
2017-08-21 22:29:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.zhihu.com/question/33892253> from <GET http://www.baidu.com/link?url=tP0PBScNvaht7GL1qoJCQQzfNpdmDK_Cw5FNF3xVwluaYwlLjWxEzgtFalHtai1KNd7XD4h54LlrmI2ZGgottK>
2017-08-21 22:29:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://open.163.com/special/opencourse/machinelearning.html> from <GET http://www.baidu.com/link?url=vZdsoRD6urZDhxJGRHNJJ7vSeTfI8mdkH0F01gkG24x9hj5HjiWPU7bsdDtJJMvEi-x4QIjX-hG5pQ4AWpeIq2u7NddTwiDDrXwRZF9_Sxe>
2017-08-21 22:29:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://tieba.baidu.com/f?kw=%BB%FA%C6%F7%D1%A7%CF%B0&fr=ala0&tpl=5> from <GET http://www.baidu.com/link?url=nSVlWumopaJ_gz-bWMvVTQBtLY8E0LkwP3gPc86n26XQ9WDdlsI_1pNAVGa_4YSYoKpHiUy2qcBdJOvQuxcvEmBFPGufpbHsCA3ia2t_-HS>
2017-08-21 22:29:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://tech.163.com/16/0907/07/C0BHQND400097U80.html> from <GET http://www.baidu.com/link?url=g7VePX8O7uHmJphvogYc6U8uMKbIbVSuFQAUw05fmD-tPTr4T9yvS4sbDZCZ8FYBelGq95nCpAJhghsiQf_hoq>
2017-08-21 22:29:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.zhihu.com/question/20691338> from <GET http://www.baidu.com/link?url=pjwLpE4UAddN9It0yK3-Ypr6MDcAciWoNMBb5GOnX0-Xi-vV3A1ZbWv32oCRwMoIKwa__pPdOxVTzrCu7d9zz_>
```

看起来能够正常抓取啦，下面我们把抓取下来的网页提取出正文并尽量去掉标签，如下：

```python
    def parse_url(self, response):
        print remove_tags(response.selector.xpath('//body').extract()[0])
```

下面，我们希望把百度搜索结果中的摘要也能够保存下来作为我们语料的一部分，如下：

```python
    def parse(self, response):
        hrefs = response.selector.xpath('//div[contains(@class, "c-container")]/h3/a/@href').extract()
        containers = response.selector.xpath('//div[contains(@class, "c-container")]')
        for container in containers:
            href = container.xpath('h3/a/@href').extract()[0]
            title = remove_tags(container.xpath('h3/a').extract()[0])
            c_abstract = container.xpath('div/div/div[contains(@class, "c-abstract")]').extract()
            abstract = ""
            if len(c_abstract) > 0:
                abstract = remove_tags(c_abstract[0])
            request = scrapy.Request(href, callback=self.parse_url)
            request.meta['title'] = title
            request.meta['abstract'] = abstract
            yield request

    def parse_url(self, response):
        print "url:", response.url
        print "title:", response.meta['title']
        print "abstract:", response.meta['abstract']
        content = remove_tags(response.selector.xpath('//body').extract()[0])
        print "content_len:", len(content)
```

解释一下，首先我们在提取url的时候顺便把标题和摘要都提取出来，然后通过scrapy.Request的meta传递到处理函数parse_url中，这样在抓取完成之后也能接到这两个值，然后提取出content，这样我们想要的数据就完整了：url、title、abstract、content

百度搜索数据几乎是整个互联网的镜像，所以你想要得到的答案，我们的语料库就是整个互联网，而我们完全借助于百度搜索引擎，不必提前存储任何资料，互联网真是伟大！

之后这些数据想保存在什么地方就看后面我们要怎么处理了，欲知后事如何，且听下回分解

## 教你如何利用强大的中文语言技术平台做依存句法和语义依存分析

句法分析是自然语言处理中非常重要的环节，没有句法分析是无法让计算机理解语言的含义的，依存句法分析由法国语言学家在1959年提出，影响深远，并且深受计算机行业青睐，依存句法分析也是做聊天机器人需要解决的最关键问题之一，语义依存更是对句子更深层次的分析，当然，有可用的工具我们就不重复造轮子，本节介绍如何利用国内领先的中文语言技术平台实现句法分析 

### 什么是依存句法分析呢？

叫的晦涩的术语，往往其实灰常简单，句法就是句子的法律规则，也就是句子里成分都是按照什么法律规则组织在一起的。而依存句法就是这些成分之间有一种依赖关系。什么是依赖：没有你的话，我存在就是个错误。“北京是中国的首都”，如果没有“首都”，那么“中国的”存在就是个错误，因为“北京是中国的”表达的完全是另外一个意思了。

### 什么是语义依存分析呢？

“语义”就是说句子的含义，“张三昨天告诉李四一个秘密”，那么语义包括：谁告诉李四秘密的？张三。张三告诉谁一个秘密？李四。张三什么时候告诉的？昨天。张三告诉李四什么？秘密。

### 语义依存和依存句法的区别

依存句法强调介词、助词等的划分作用，语义依存注重实词之间的逻辑关系

另外，依存句法随着字面词语变化而不同，语义依存不同字面词语可以表达同一个意思，句法结构不同的句子语义关系可能相同。

### 依存句法分析和语义依存分析对我们的聊天机器人有什么意义呢？

依存句法分析和语义分析相结合使用，对对方说的话进行依存和语义分析后，一方面可以让计算机理解句子的含义，从而匹配到最合适的回答，另外如果有已经存在的依存、语义分析结果，还可以通过置信度匹配来实现聊天回答。

### 依存句法分析到底是怎么分析的呢？

依存句法分析的基本任务是确定句式的句法结构(短语结构)或句子中词汇之间的依存关系。依存句法分析最重要的两棵树：

依存树：子节点依存于父节点

依存投射树：实线表示依存联结关系，位置低的成分依存于位置高的成分，虚线为投射线

![image](http://upload-images.jianshu.io/upload_images/5952841-1620cecb2b2c3b7e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 依存关系的五条公理

1. 一个句子中只有一个成分是独立的

2. 其他成分直接依存于某一成分

3. 任何一个成分都不能依存于两个或两个以上的成分

4. 如果A成分直接依存于B成分，而C成分在句子中位于A和B之间，那么C或者直接依存于B，或者直接依存于A和B之间的某一成分

5. 中心成分左右两面的其他成分相互不发生关系

什么地方存在依存关系呢？比如合成词（如：国内）、短语（如：英雄联盟）很多地方都是

### LTP依存关系标记

关系 | 简称 | 全程 | 示例
--- | --- | --- | ---
主谓关系 | SBV | subject-verb | 我送她一束花 (我 <-- 送)
动宾关系 | VOB 直接宾语 | verb-object | 我送她一束花 (送 --> 花)
间宾关系 | IOB 间接宾语 | indirect-object | 我送她一束花 (送 --> 她)
前置宾语 | FOB 前置宾语 | fronting-object | 他什么书都读 (书 <-- 读)
兼语 | DBL | double | 他请我吃饭 (请 --> 我)
定中关系 | ATT | attribute | 红苹果 (红 <-- 苹果)
状中结构 | ADV | adverbial | 非常美丽 (非常 <-- 美丽)
动补结构 | CMP | complement | 做完了作业 (做 --> 完)
并列关系 | COO | coordinate | 大山和大海 (大山 --> 大海)
介宾关系 | POB | preposition-object | 在贸易区内 (在 --> 内)
左附加关系 | LAD | left adjunct | 大山和大海 (和 <-- 大海)
右附加关系 | RAD | right adjunct | 孩子们 (孩子 --> 们)
独立结构 | IS | independent structure | 两个单句在结构上彼此独立
核心关系 | HED | head | 指整个句子的核心

### 那么依存关系是怎么计算出来的呢？

是通过机器学习和人工标注来完成的，机器学习依赖人工标注，那么都哪些需要我们做人工标注呢？分词词性、依存树库、语义角色都需要做人工标注，有了这写人工标注之后，就可以做机器学习来分析新的句子的依存句法了

### LTP云平台怎么用？

http://www.ltp-cloud.com/

## 把语言模型探究到底

无论什么做自然语言处理的工具，都是基于计算机程序实现的，而计算机承担了数学计算的职责，那么自然语言和数学之间的联系就是语言模型，只有理解语言模型才能理解各种工具的实现原理，本节让我们深究语言模型的世界 

### 什么是数学模型

数学模型是运用数理逻辑方法和数学语言建构的科学或工程模型。说白了，就是用数学的方式来解释事实。举个简单的例子：你有一只铅笔，又捡了一只，一共是两只，数学模型就是1+1=2。举个复杂的例子：你在路上每周能捡到3只铅笔，数学模型就是P(X)=3/7，这个数学模型可以帮你预测明天捡到铅笔的可能性。当然解释实事的数学模型不是唯一的，比如每周捡三只铅笔的数学模型还可能是P(qt=sj|qt-1=si,qt-2=sk,...)，s=0,1，也就是有两个状态的马尔可夫模型，意思就是明天是否捡到铅笔取决于前几天有没有捡到铅笔

### 什么是数学建模

数学建模就是通过计算得到的结果来解释实际问题，并接受实际的检验，来建立数学模型的全过程。

### 什么是语言模型

语言模型是根据语言客观事实而进行的语言抽象数学建模。说白了，就是找到一个数学模型，让它来解释自然语言的事实。

### 业界认可的语言模型

业界目前比较认可而且有效的语言模型是n元语法模型(n-gram model)，它本质上是马尔可夫模型，简单来描述就是：一句话中下一个词的出现和最近n个词有关(包括它自身)。详细解释一下：

如果这里的n=1时，那么最新一个词只和它自己有关，也就是它是独立的，和前面的词没关系，这叫做一元文法

如果这里的n=2时，那么最新一个词和它前面一个词有关，比如前面的词是“我”，那么最新的这个词是“是”的概率比较高，这叫做二元文法，也叫作一阶马尔科夫链

依次类推，工程上n=3用的是最多的，因为n越大约束信息越多，n越小可靠性更高

n元语法模型实际上是一个概率模型，也就是出现一个词的概率是多少，或者一个句子长这个样子的概率是多少。

这就又回到了之前文章里提到的自然语言处理研究的两大方向：基于规则、基于统计。n元语法模型显然是基于统计的方向。

### 概率是如何统计的

说到基于统计，那么就要说概率是如何估计的了，通常都是使用最大似然估计，怎么样理解“最大似然估计”，最大似然就是最最最最最相似的，那么和谁相似，和历史相似，历史是什么样的？10个词里出现过2次，所以是2/10=1/5，所以经常听说过的“最大似然估计”就是用历史出现的频率来估计概率的方法。这么说就懂了吧？

### 语言模型都有哪些困难

#### 1. 千变万化的自然语言导致的0概率问题

基于统计的自然语言处理需要基于大量语料库进行，而自然语言千变万化，可以理解所有词汇的笛卡尔积，数量大到无法想象，有限的语料库是难以穷举语言现象的，因此n元语法模型会出现某一句话出现的概率为0的情况，比如我这篇博客在我写出来之前概率就是0，因为我是原创。那么这个0概率的问题如何解决呢？这就是业界不断在研究的数据平滑技术，也就是通过各种数学方式来让每一句话的概率都大于0。具体方法不列举，都是玩数学的，比较简单，无非就是加个数或者减个数或者做个插值平滑一下，效果上应用在不同特点的数据上各有千秋。平滑的方法确实有效，各种自然语言工具中都实现了，直接用就好了。

#### 2. 特定领域的特定词概率偏大问题

每一种领域都会有一些词汇比正常概率偏大，比如计算机领域会经常出现“性能”、“程序”等词汇，这个解决办法可以通过缓存一些刚刚出现过的词汇来提高后面出现的概率来解决。当然这里面是有很多技巧的，我们并不是认为所有出现过的词后面概率都较大，而是会考虑这些词出现的频率和规律(如：词距)来预测。

#### 3. 单一语言模型总会有弊端

还是因为语料库的不足，我们会融合多种语料库，但因为不同语料库之间的差异，导致我们用单一语言模型往往不够准确，因此，有一种方法可以缓和这种不准确性，那就是把多种语言模型混到一起来计算，这其实是一种折中，这种方法low且有效。

还有一种方法就是用多种语言模型来分别计算，最后选择熵最大的一种，这其实也是一种折中，用在哪种地方就让哪种模型生效。

### 神经网络语言模型

21世纪以来，统计学习领域无论什么都要和深度学习搭个边，毕竟计算机计算能力提升了很多，无论多深都不怕。神经网络语言模型可以看做是一种特殊的模型平滑方式，本质上还是在计算概率，只不过通过深层的学习来得到更正确的概率。

### 语言模型的应用

这几乎就是自然语言处理的应用了，有：中文分词、机器翻译、拼写纠错、语音识别、音子转换、自动文摘、问答系统、OCR等

## 探究中文分词的艺术

中文是世界语言界的一朵奇葩，它天生把词连在一起，让计算机望而却步，一句#他说的确实在理#让计算机在#的确#、#实在#、#确实#里面挣扎，但是统计自然语言处理却让计算机有了智能 

### 中文分词是怎么走到今天的

话说上个世纪，中文自动分词还处于初级阶段，每句话都要到汉语词表中查找，有没有这个词？有没有这个词？所以研究集中在：怎么查找最快、最全、最准、最狠......，所以就出现了正向最大匹配法、逆向最大匹配法、双向扫描法、助词遍历法......，用新世纪比较流行的一个词来形容就是：你太low了！

中文自动分词最难的两个问题：1）歧义消除；2）未登陆词识别。说句公道话，没有上个世纪那么low的奠定基础，也就没有这个世纪研究重点提升到这两个高级的问题

ps:未登录词就是新词，词表里没有的词

本世纪计算机软硬件发展迅猛，计算量存储量都不再是问题，因此基于统计学习的自动分词技术成为主流，所以就出现了各种新分词方法，也更适用于新世纪文本特点

### 从n元语法模型开始说起

上节讲到了n元语法模型，在前n-1个词出现的条件下，下一个词出现的概率是有统计规律的，这个规律为中文自动分词提供了统计学基础，所以出现了这么几种统计分词方法：N-最短路径分词法、基于n元语法模型的分词法

N-最短路径分词法其实就是一元语法模型，每个词成为一元，独立存在，出现的概率可以基于大量语料统计得出，比如“确实”这个词出现概率的0.001（当然这是假设，别当真），我们把一句话基于词表的各种切词结果都列出来，因为字字组合可能有很多种，所以有多个候选结果，这时我们利用每个词出现的概率相乘起来，得到的最终结果，谁最大谁就最有可能是正确的，这就是N-最短路径分词法。

这里的N的意思是说我们计算概率的时候最多只考虑前N个词，因为一个句子可能很长很长，词离得远，相关性就没有那么强了

这里的最短路径其实是传统最短路径的一种延伸，由加权延伸到了概率乘积

而基于n元语法模型的分词法就是在N-最短路径分词法基础上把一元模型扩展成n元模型，也就是统计出的概率不再是一个词的概率，而是基于前面n个词的条件概率

### 人家基于词，我来基于字

由字构词的分词方法出现可以说是一项突破，发明者也因此得到了各项第一和很多奖项，那么这个著名的分词法是怎么做的呢？

每个字在词语中都有一个构词位置：词首、词中、词尾、单独构词。根据一个字属于不同的构词位置，我们设计出来一系列特征，比如：前一个词、前两个词、前面词长度、前面词词首、前面词词尾、前面词词尾加上当前的字组成的词……

我们基于大量语料库，利用平均感知机分类器对上面特征做打分，并训练权重系数，这样得出的模型就可以用来分词了，句子右边多出来一个字，用模型计算这些特征的加权得分，得分最高的就是正确的分词方法

### 分词方法纵有千万种，一定有适合你的那一个

分词方法很多，效果上一定是有区别的，基于n元语法模型的方法的优势在于词表里已有的词的分词效果，基于字构词的方法的优势在于未登陆词的识别，因此各有千秋，你适合哪个就用哪个。

### 异性相吸，优势互补

既然两种分词各有优缺点，那么就把他们结合起来吧，来个插值法折中一下，用过的人都说好

### 流行分词工具都是用的什么分词方法

#### jieba中文分词

官方描述：

- 基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)
- 采用了动态规划查找最大概率路径, 找出基于词频的最大切分 组合
- 对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法

前两句话是说它是基于词表的分词，最后一句是说它也用了由字构词，所以它结合了两种分词方法

#### ik分词器

基于词表的最短路径切词

#### ltp云平台分词

主要基于机器学习框架并部分结合词表的方法

## 一篇文章读懂拿了图灵奖和诺贝尔奖的概率图模型

概率图模型是概率论和图论的结合，经常见到的贝叶斯网络、马尔可夫模型、最大熵模型、条件随机场都属于概率图模型，这些模型有效的解决了很多实际问题，比如自然语言处理中的词性标注、实体识别等，书里的描述都公式纵横、晦涩难懂，我们不妨试试轻轻松松的来说一说概率图模型 

### 首先我们说说什么是图论

能点进这篇文章说明你一定是有一定数学基础的，所以我做个比喻，你来看看是不是这么回事。糖葫芦吃过吧？几个山楂串在一根杆上，这其实就是一个图。

稍稍正式一点说：图就是把一些孤立的点用线连起来，任何点之间都有可能连着。它区别于树，树是有父子关系，图没有。

再深入一点点：从质上来说，图可以表达的某些事物之间的关联关系，也可以表达的是一种转化关系；从量上来说，它能表达出关联程度，也能表达出转化的可能性大小

图论一般有什么用途呢？著名的七桥问题、四色问题、欧拉定理都是在图论基础上说事儿的

### 再说说概率论

概率论从中学到大学再到工作中都在学，它原理很简单：投个硬币出现人头的概率是1/2，最常用的就是条件概率P(B|A)，联合概率P(A,B)，贝叶斯公式:P(B|A)=P(A|B)P(B)/P(A)，各种估计方法。

### 提前解释一下概率图模型里的几个常见词汇

贝叶斯(Bayes)：无论什么理论什么模型，只要一提到他，那么里面一定是基于条件概率P(B|A)来做文章的。ps：贝叶斯老爷爷可是18世纪的人物，他的理论到现在还这么火，可见他的影响力绝不下于牛顿、爱因斯坦

马尔可夫(Markov)：无论什么理论什么模型，只要一提到他，那么里面一定有一条链式结构或过程，前n个值决定当前这个值，或者说当前这个值跟前n个值有关

熵(entropy)：熵有火字旁，本来是一个热力学术语，表示物质系统的混乱状态。延伸数学上表达的是一种不确定性。延伸到信息论上是如今计算机网络信息传输的基础理论，不确定性函数是f(p)=-logp，信息熵H(p)=-∑plogp。提到熵必须要提到信息论鼻祖香农(Shannon)

场(field)：只要在数学里见到场，它都是英文里的“域”的概念，也就是取值空间，如果说“随机场”，那么就表示一个随机变量能够赋值的全体空间

### 再说概率图模型

概率图模型一般是用图来说明，用概率来计算的。所以为了清晰的说明，我们每一种方法我尽量配个图，并配个公式。

首先，为了脑子里有个体系，我们做一个分类，分成有向图模型和无向图模型，顾名思义，就是图里面的边是否有方向。那么什么样的模型的边有方向，而什么样的没方向呢？这个很好想到，有方向的表达的是一种推演关系，也就是在A的前提下出现了B，这种模型又叫做生成式模型。而没有方向表达的是一种“这样就对了”的关系，也就是A和B同时存在就对了，这种模型又叫做判别式模型。生成式模型一般用联合概率计算(因为我们知道A的前提了，可以算联合概率)，判别式模型一般用条件概率计算(因为我们不知道前提，所以只能"假设"A条件下B的概率)。生成式模型的代表是：n元语法模型、隐马尔可夫模型、朴素贝叶斯模型等。判别式模型的代表是：最大熵模型、支持向量机、条件随机场、感知机模型等

### 贝叶斯网络

按照前面说的，提到贝叶斯就是条件概率，所以也就是生成式模型，也就是有向图模型。

为了说明什么是贝叶斯网络，我从网上盗取一个图

![image](http://upload-images.jianshu.io/upload_images/5952841-ba148154c00eab8c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

图中每一个点都可能未True或False，他们的概率是已知的，比如x7的概率需要有x4和x5来决定，可能是这样的

x4|x5|T|F
---|---|---|---
T|T|   0.5|0.5
T|  F|   0.4| 0.6
F | T |  0.7 |0.3
F | F |  0.2| 0.8

那么可以通过上面的贝叶斯网络来估计如果x1为False情况下x6为True的概率：

P(x6=T|x1=F)=P(x6=T,x1=F)/P(x1=F)

这个值继续推导，最终可以由每个节点的概率数据计算求得，这么说来，贝叶斯网络模型可以通过样本学习来估计每个节点的概率，从而达到可以预测各种问题的结果

贝叶斯网络能够在已知有限的、不完整的、不确定信息条件下进行学习推理，所以广泛应用在故障诊断、维修决策、汉语自动分词、词义消歧等问题上

### 马尔可夫模型和隐马尔可夫模型

按照前面说的，提到马尔可夫就是一个值跟前面n个值有关，所以也就是条件概率，也就是生成式模型，也就是有向图模型。

继续盗图

![png](http://upload-images.jianshu.io/upload_images/5952841-a8ddaf908fa9dec1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

音乐的每一个音不是随意作出来的，是根据曲子的风格、和弦、大小调式等来决定的，但是因为可选的音高有多种，也就出现了无数美妙的旋律。因为有约束，所以其实可以说新的音和前面的n个音有关，这其实是一个马尔可夫模型可以解释的事情。

马尔可夫模型还可以看成是一个关于时间t的状态转换过程，也就是随机的有限状态机，那么状态序列的概率可以通过计算形成该序列所有状态之间转移弧上的概率乘积得出。

如果说这个马尔可夫是两阶的，那么转移概率可能是这个样子：

[图片上传失败...(image-42d131-1513911488050)]

当然后面的概率只是举了个例子，这种情况由前两列决定的第三列任意值都会有一个概率

我们通过训练样本来得出每一个概率值，这样就可以通过训练出的模型来根据前两个音是什么而预测下一个音是1、2、3、4、5任意一个的概率是多少了，也就是可以自动作曲了，当然这样做出的曲子肯定是一个无线循环的旋律，你猜猜为什么。

 

那么我们再说隐马尔可夫模型，这里的“隐”指的是其中某一阶的信息我们不知道，就像是我们知道人的祖先是三叶虫，但是由三叶虫经历了怎样的演变过程才演变到人的样子我们是不知道的，我们只能通过化石资料了解分布信息，如果这类资料很多，那么就可以利用隐马尔可夫模型来建模，因为缺少的信息较多，所以这一模型的算法比较复杂，比如前向算法、后向算法之类晦涩的东西就不说了。相对于原理，我们更关注它的应用，隐马尔可夫模型广泛应用在词性标注、中文分词等，为什么能用在这两个应用上呢？仔细想一下能看得出来，比如中文分词，最初你是不知道怎么分词的，前面的词分出来了，你才之后后面的边界在哪里，但是当你后面做了分词之后还要验证前面的分词是否正确，这样前后有依赖关系，而不确定中间状态的情况最适合用隐马尔可夫模型来解释

### 最大熵模型

按照前面所说的，看到熵那么一定会用到H(p)=-∑plogp，怎么理解最大熵模型呢？我们的最终目的是想知道在某一个信息条件B下，得出某种可能的结果A的最大的概率，也就是条件概率P(A|B)最大的候选结果。因为最大熵就是不确定性最大，其实也就是条件概率最大，所以求最大的条件概率等同于求最大熵，而我们这里的熵其实是H(p)=H(A|B)=-∑p(b)p(a|b)log(p(a|b))，为了使用训练数据做估计，这里的p(a|b)可以通过训练数据的某些特征来估计，比如这些特征是fi(a,b)，那么做模型训练的过程就编程了训练∑λf(a,b)中的λ参数的过程，至此就有些像机器学习的线性回归了，该怎么做就清晰了。所以其实最大熵模型就是利用熵的原理和熵的公式来用另外一种形式来描述具有概率规律的现实的

### 条件随机场

场表示取值范围，随机场表示随机变量有取值范围，也就是每个随机变量有固定的取值，条件指的是随机变量的取值由一定的条件概率决定，而这里的条件来自于我们有一些观察值，这是它区别于其他随机场的地方。条件随机场也可以看做是一个无向图模型，它特殊就特殊在给定观察序列X时某个特定的标记序列Y的概率是一个指数函数exp(∑λt+∑μs)，其中t是转移函数，s是状态函数，我们需要训练的是λ和μ。条件随机场主要应用在标注和切分有序数据上，尤其在自然语言处理、生物信息学、机器视觉、网络智能等方面

 

总结一下，概率图模型包括多种结合概率论和图论的模型，根据特定场景特定需求选择不同的模型，每种模型的参数都需要大量样本训练得出，每种模型都是用来根据训练出来的概率做最优结论选择的，比如根据训练出来的模型对句子做最正确的词性标注、实体标注、分词序列等，本文只是从理念上的解释和总结，真的用到某一种模型还是需要深入研究原理和公式推导以及编程实现，那就不是本文这种小篇幅能够解释的完的了，等我们后面要遇到必须用某一种模型来实现时再狠狠地深入一下。

## 大话自然语言处理中的囊中取物

大数据风靡的今天，不从里面挖出点有用的信息都不好意思见人，人工智能号称跨过奇点，统霸世界，从一句话里都识别不出一个命名实体？不会的，让我们大话自然语言处理的囊中取物，看看怎么样能让计算机像人一样看出一句话里哪个像人、哪个像物 

### 话说天下大事，分久必合，合久必分。

之前谈到中文分词把文本切分成一个一个词语，现在我们要反过来，把该拼一起的词再拼到一起，找到一个命名实体，比如：“亚太经合组织”

### 条件随机场的用武之地

上回书说到，概率图模型中的条件随机场适用于在一定观测值条件下决定的随机变量有有限个取值的情况，它特殊就特殊在给定观察序列X时某个特定的标记序列Y的概率是一个指数函数exp(∑λt+∑μs)，这也正符合最大熵原理。基于条件随机场的命名实体识别方法属于有监督的学习方法，需要利用已经标注好的大规模语料库进行训练，那么已经标注好的语料里面有什么样的特征能够让模型得以学习呢？

### 谈命名实体的放射性

为什么说命名实体是有放射性的呢？举个栗子：“中国积极参与亚太经合组织的活动”，这里面的“亚太经合组织”是一个命名实体，定睛一瞧，这个实体着实不凡啊，有“组织”两个字，这么说来这个实体是一种组织或机构，记住，下一次当你看到“组织”的时候和前面几个字组成的一定是一个命名实体。继续观察，在它之前辐射出了“参与”一次，经过大规模语料训练后能发现，才“参与”后面有较大概率跟着一个命名实体。继续观察，在它之后有“的活动”，那么说明前面很可能是一个组织者，组织者多半是一个命名实体。这就是基于条件随机场做命名实体识别的奥秘，这就是命名实体的放射性

### 特征模板

前面讲了放射性，那么设计特征模板就比较容易了，我们采用当前位置的前后n个位置上的字/词/字母/数字/标点等作为特征，因为是基于已经标注好的语料，所以这些特征是什么样的词性、词形都是已知的。

特征模板的选择是和具体我们要识别的实体类别有关系的，识别人名和识别机构名用的特征模板是不一样的，因为他们的特点就不一样，事实上识别中文人名和识别英文人名用的特征模板也是不一样的，因为他们的特点就不一样

### 且说命名实体

前面讲了一揽子原理，回过头来讲讲命名实体是什么，命名实体包括：人名(政治家、艺人等)、地名(城市、州、国家、建筑等)、组织机构名、时间、数字、专有名词(电影名、书名、项目名、电话号码等)、……。其实领域很多，不同人需求不一样，关注的范围也不一样。总之不外乎命名性指称、名词性指称和代词性指称

### 自古英雄周围总有谋士

基于条件随机场的命名实体方法虽好，但如何利用好还是需要各路谋士献计献策。有的人提出通过词形上下文训练模型，也就是给定词形上下文语境中产生实体的概率；有的人提出通过词性上下文训练模型，也就是给定词性上下文语境中产生实体的概率；有的人提出通过给定实体的词形串作为实体的概率；有的人提出通过给定实体的词性串作为实体的概率；当大家发现这四点总有不足时，有谋士提出：把四个结合起来！这真是：英雄代有人才出，能摆几出摆几出啊

### 语料训练那些事儿

语料训练那些事儿，且看我机器学习教程相关文章《机器学习精简入门教程》，预知后事如何，下回我也不分解了

## 让机器做词性自动标注的具体方法

分词、命名实体识别和词性标注这三项技术如果达不到很高的水平，是难以建立起高性能的自然语言处理系统，也就难以实现高质量的聊天机器人，而词性是帮助计算机理解语言含义的关键，本节来介绍一些词性标注的具体方法

### 何为词性

常说的词性包括：名、动、形、数、量、代、副、介、连、助、叹、拟声。但自然语言处理中要分辨的词性要更多更精细，比如：区别词、方位词、成语、习用语、机构团体、时间词等，多达100多种。

汉语词性标注最大的困难是“兼类”，也就是一个词在不同语境中有不同的词性，而且很难从形式上识别。

### 词性标注过程

为了解决词性标注无法达到100%准确的问题，词性标注一般要经过“标注”和“校验”两个过程，第一步“标注”根据规则或统计的方法做词性标注，第二步“校验”通过一致性检查和自动校对等方法来修正。
 
### 词性标注的具体方法

词性标注具体方法包括：基于统计模型的方法、基于规则的方法和两者结合的方法。下面我们分别来介绍。

###  基于统计模型的词性标注方法

提到基于统计模型，势必意味着我们要利用大量已经标注好的语料库来做训练，同时要先选择一个合适的训练用的数学模型，《自己动手做聊天机器人 十五-一篇文章读懂拿了图灵奖和诺贝尔奖的概率图模型》中我们介绍了概率图模型中的隐马尔科夫模型(HMM)比较适合词性标注这种基于观察序列来做标注的情形。语言模型选择好了，下面要做的就是基于语料库来训练模型参数，那么我们模型参数初值如何设置呢？这里面就有技巧了

### 隐马尔可夫模型参数初始化的技巧

模型参数初始化是在我们尚未利用语料库之前用最小的成本和最接近最优解的目标来设定初值。HMM是一种基于条件概率的生成式模型，所以模型参数是生成概率，那么我们不妨就假设每个词的生成概率就是它所有可能的词性个数的倒数，这个是计算最简单又最有可能接近最优解的生成概率了。每个词的所有可能的词性是我们已经有的词表里标记好的，这个词表的生成方法就比较简单了，我们不是有已经标注好的语料库嘛，很好统计。那么如果某个词在词表里没有呢？这时我们可以把它的生成概率初值设置为0。这就是隐马尔可夫模型参数初始化的技巧，总之原则就是用最小的成本和最接近最优解的目标来设定初值。一旦完成初始值设定后就可以利用前向后向算法进行训练了。

###  基于规则的词性标注方法

规则就是我们既定好一批搭配关系和上下文语境的规则，判断实际语境符合哪一种则按照规则来标注词性。这种方法比较古老，适合于既有规则，对于兼词的词性识别效果较好，但不适合于如今网络新词层出不穷、网络用语新规则的情况。于是乎，有人开始研究通过机器学习来自动提取规则，怎么提取呢？不是随便给一堆语料，它直接来生成规则，而是根据初始标注器标注出来的结果和人工标注的结果的差距，来生成一种修正标注的转换规则，这是一种错误驱动的学习方法。基于规则的方法还有一个好处在于：经过人工校总结出的大量有用信息可以补充和调整规则库，这是统计方法做不到的。

### 统计方法和规则方法相结合的词性标注方法

统计方法覆盖面比较广，新词老词通吃，常规非常规通吃，但对兼词、歧义等总是用经验判断，效果不好。规则方法对兼词、歧义识别比较擅长，但是规则总是覆盖不全。因此两者结合再好不过，先通过规则排歧，再通过统计标注，最后经过校对，可以得到正确的标注结果。在两者结合的词性标注方法中，有一种思路可以充分发挥两者优势，避免劣势，就是首选统计方法标注，同时计算计算它的置信度或错误率，这样来判断是否结果是否可疑，在可疑情况下采用规则方法来进行歧义消解，这样达到最佳效果。

### 词性标注的校验

做完词性标注并没有结束，需要经过校验来确定正确性以及修正结果。

第一种校验方法就是检查词性标注的一致性。一致性指的是在所有标注的结果中，具有相同语境下同一个词的标注是否都相同，那么是什么原因导致的这种不一致呢？一种情况就是这类词就是兼类词，可能被标记为不同词性。另一种情况是非兼类词，但是由于人工校验或者其他原因导致标记为不同词性。达到100%的一致性是不可能的，所以我们需要保证一致性处于某个范围内，由于词数目较多，词性较多，一致性指标无法通过某一种计算公式来求得，因此可以基于聚类和分类的方法，根据欧式距离来定义一致性指标，并设定一个阈值，保证一致性在阈值范围内。

第二种校验方法就是词性标注的自动校对。自动校对顾名思义就是不需要人参与，直接找出错误的标注并修正，这种方法更适用于一个词的词性标注通篇全错的情况，因为这种情况基于数据挖掘和规则学习方法来做判断会相对比较准确。通过大规模训练语料来生成词性校对决策表，然后根据这个决策表来找通篇全错的词性标注并做自动修正。

### 总结

词性标注的方法主要有基于统计和基于规则的方法，另外还包括后期校验的过程。词性标注是帮助计算机理解语言含义的关键，有了词性标注，我们才可以进一步确定句法和语义，才有可能让机器理解语言的含义，才有可能实现聊天机器人的梦想

## 神奇算法之句法分析树的生成

把一句话按照句法逻辑组织成一棵树，由人来做这件事是可行的，但是由机器来实现是不可思议的，然而算法世界就是这么神奇，把一个十分复杂的过程抽象成仅仅几步操作，甚至不足10行代码，就能让机器完成需要耗费人脑几十亿脑细胞的工作，本文我们来见识一下神奇的句法分析树生成算法 

### 句法分析

先来解释一下句法分析。句法分析分为句法结构分析和依存关系分析。

句法结构分析也就是短语结构分析，比如提取出句子中的名次短语、动词短语等，最关键的是人可以通过经验来判断的短语结构，那么怎么由机器来判断呢？

（有关依存关系分析的内容，具体可以看《自己动手做聊天机器人 十二-教你如何利用强大的中文语言技术平台做依存句法和语义依存分析》）

###  句法分析树

样子如下：

```
      -吃(v)-
|                |
我(rr)            肉(n)
```

### 句法结构分析基本方法

分为基于规则的分析方法和基于统计的分析方法。基于规则的方法存在很多局限性，所以我们采取基于统计的方法，目前最成功的是基于概率上下文无关文法(PCFG)。基于PCFG分析需要有如下几个要素：终结符集合、非终结符集合、规则集。

相对于先叙述理论再举实例的传统讲解方法，我更倾向于先给你展示一个简单的例子，先感受一下计算过程，然后再叙述理论，这样会更有趣。

例子是这样的：我们的终结符集合是：∑={我, 吃, 肉,……}，这个集合表示这三个字可以作为句法分析树的叶子节点，当然这个集合里还有很多很多的词

我们的非终结符集合是：N={S, VP, ……}，这个集合表示树的非页子节点，也就是连接多个节点表达某种关系的节点，这个集合里也是有很多元素

我们的规则集：R={

NN->我    0.5

Vt->吃     1.0

NN->肉   0.5

VP->Vt NN    1.0

S->NN VP 1.0

……

}

这里的句法规则符号可以参考词性标注，后面一列是模型训练出来的概率值，也就是在一个固定句法规则中NN的位置是“我”的概率是0.5，NN推出“肉”的概率是0.5，0.5+0.5=1，也就是左部相同的概率和一定是1。不知道你是否理解了这个规则的内涵

再换一种方法解释一下，有一种句法规则是：

```
S——|
|        |
NN    VP
          |——|
          Vt     NN
```

其中NN的位置可能是“我”，也可能是“肉”，是“我”的概率是0.5，是“肉”的概率是0.5，两个概率和必为1。其中Vt的位置一定是“吃”，也就是概率是1.0……。这样一说是不是就理解了？

规则集里实际上还有很多规则，只是列举出会用到的几个

以上的∑、N、R都是经过机器学习训练出来的数据集及概率，具体训练方法下面我们会讲到

那么如何根据以上的几个要素来生成句法分析树呢？

（1）“我”

词性是NN，推导概率是0.5，树的路径是“我”

（2）“吃”

词性是Vt，推导概率是1.0，树的路径是“吃”

（3）“肉”

词性是NN，概率是0.5，和Vt组合符合VP规则，推导概率是0.5*1.0*1.0=0.5，树的路径是“吃肉”

NN和VP组合符合S规则，推导概率是0.5*0.5*1.0=0.25，树的路径是“我吃肉”

 

所以最终的树结构是：

```
S——|
|        |
NN    VP
我      |——|
          Vt     NN
          吃     肉
```

上面的例子是比较简单的，实际的句子会更复杂，但是都是通过这样的动态规划算法完成的

提到动态规划算法，就少不了“选择”的过程，一句话的句法结构树可能有多种，我们只选择概率最大的那一种作为句子的最佳结构，这也是“基于概率”上下文无关文法的名字起源。

上面的计算过程总结起来就是：设W={ω1ω2ω3……}表示一个句子，其中的ω表示一个词(word)，利用动态规划算法计算非终结符A推导出W中子串ωiωi+1ωi+2……ωj的概率，假设概率为αij(A)，那么有如下递归公式：

αij(A)=P(A->ωi)

αij(A)=∑∑P(A->BC)αik(B)α(k+1)j(C)

以上两个式子好好理解一下其实就是上面“我吃肉”的计算过程

以上过程理解了之后你一定会问，这里面最关键的的非终结符、终结符以及规则集是怎么得来的，概率又是怎么确定的？下面我们就来说明

### 句法规则提取方法与PCFG的概率参数估计

这部分就是机器学习的知识了，有关机器学习可以参考《机器学习教程》

首先我们需要大量的树库，也就是训练数据。然后我们把树库中的句法规则提取出来生成我们想要的结构形式，并进行合并、归纳等处理，最终得到上面∑、N、R的样子。其中的概率参数计算方法是这样的：

先给定参数为一个随机初始值，然后采用EM迭代算法，不断训练数据，并计算每条规则使用次数作为最大似然计算得到概率的估值，这样不断迭代更新概率，最终得出的概率可以认为是符合最大似然估计的精确值。

### 总结一下

句法分析树生成算法是基于统计学习的原理，根据大量标注的语料库（树库），通过机器学习算法得出非终结符、终结符、规则集及其概率参数，然后利用动态规划算法生成每一句话的句法分析树，在句法分析树生成过程中如果遇到多种树结构，选择概率最大的那一种作为最佳句子结构

## 机器人是怎么理解“日后再说”的

日后再说这个成语到了当代可以说含义十分深刻，你懂的，但是如何让计算机懂得可能有两种含义的一个词到底是想表达哪个含义呢？这在自然语言处理中叫做词义消歧，从本节开始我们从基本的结构分析跨入语义分析，开始让计算机对语言做深层次的理解 

### 词义消歧

词义消歧是句子和篇章语义理解的基础，是必须解决的问题。任何一种语言都有大量具有多种含义的词汇，中文的“日”，英文的“bank”，法语的“prendre”……。

词义消歧可以通过机器学习的方法来解决。谈到机器学习就会分成有监督和无监督的机器学习。词义消歧有监督的机器学习方法也就是分类算法，即判断词义所属的分类。词义消歧无监督的机器学习方法也就是聚类算法，把词义聚成多类，每一类是一种含义。

### 有监督的词义消歧方法

#### 基于互信息的词义消歧方法

这个方法的名字不好理解，但是原理却非常简单：用两种语言对照着看，比如：中文“打人”对应英文“beat a man”，而中文“打酱油”对应英文“buy some sauce”。这样就知道当上下文语境里有“人”的时候“打”的含义是beat，当上下文语境里有“酱油”的时候“打”的含义是buy。按照这种思路，基于大量中英文对照的语料库训练出来的模型就可以用来做词义消歧了，这种方法就叫做基于“互信息”的词义消歧方法。讲到“互信息”还要说一下它的起源，它来源于信息论，表达的是一个随机变量中包含另一个随机变量的信息量(也就是英文信息中包含中文信息的信息量)，假设两个随机变量X、Y的概率分别是p(x), p(y)，它们的联合分布概率是p(x,y)，那么互信息计算公式是：

```
I(X; Y) = ∑∑p(x,y)log(p(x,y)/(p(x)p(y)))
```

以上公式是怎么推导出来的呢？比较简单，“互信息”可以理解为一个随机变量由于已知另一个随机变量而减少的不确定性(也就是理解中文时由于已知了英文的含义而让中文理解更确定了)，因为“不确定性”就是熵所表达的含义，所以：

```
I(X; Y) = H(X) - H(X|Y)
```

等式后面经过不断推导就可以得出上面的公式，对具体推导过程感兴趣可以百度一下。

那么我们在对语料不断迭代训练过程中I(X; Y)是不断减小的，算法终止的条件就是I(X; Y)不再减小。

基于互信息的词义消歧方法自然对机器翻译系统的效果是最好的，但它的缺点是：双语语料有限，多种语言能识别出歧义的情况也是有限的(比如中英文同一个词都有歧义就不行了)。

#### 基于贝叶斯分类器的消歧方法

提到贝叶斯那么一定少不了条件概率，这里的条件指的就是上下文语境这个条件，任何多义词的含义都是跟上下文语境相关的。假设语境(context)记作c，语义(semantic)记作s，多义词(word)记作w，那么我要计算的就是多义词w在语境c下具有语义s的概率，即：

p(s|c)

那么根据贝叶斯公式：

p(s|c) = p(c|s)p(s)/p(c)

我要计算的就是p(s|c)中s取某一个语义的最大概率，因为p(c)是既定的，所以只考虑分子的最大值：

s的估计=max(p(c|s)p(s))

因为语境c在自然语言处理中必须通过词来表达，也就是由多个v(词)组成，那么也就是计算：

max(p(s)∏p(v|s))

下面就是训练的过程了：

p(s)表达的是多义词w的某个语义s的概率，可以统计大量语料通过最大似然估计求得：

p(s) = N(s)/N(w)

p(v|s)表达的是多义词w的某个语义s的条件下出现词v的概率，可以统计大量语料通过最大似然估计求得：

p(v|s) = N(v, s)/N(s)

训练出p(s)和p(v|s)之后我们对一个多义词w消歧的过程就是计算(p(c|s)p(s))的最大概率的过程

### 无监督的词义消歧方法

完全无监督的词义消歧是不可能的，因为没有标注是无法定义是什么词义的，但是可以通过无监督的方法来做词义辨识。无监督的词义辨识其实也是一种贝叶斯分类器，和上面讲到的贝叶斯分类器消歧方法不同在于：这里的参数估计不是基于有标注的训练预料，而是先随机初始化参数p(v|s)，然后根据EM算法重新估计这个概率值，也就是对w的每一个上下文c计算p(c|s)，这样可以得到真实数据的似然值，回过来再重新估计p(v|s)，重新计算似然值，这样不断迭代不断更新模型参数，最终得到分类模型，可以对词进行分类，那么有歧义的词在不同语境中会被分到不同的类别里。

仔细思考一下这种方法，其实是基于单语言的上下文向量的，那么我们进一步思考下一话题，如果一个新的语境没有训练模型中一样的向量怎么来识别语义？

这里就涉及到向量相似性的概念了，我们可以通过计算两个向量之间夹角余弦值来比较相似性，即：

cos(a,b) = ∑ab/sqrt(∑a^2∑b^2)

### 机器人是怎么理解“日后再说”的

回到最初的话题，怎么让机器人理解“日后再说”，这本质上是一个词义消歧的问题，假设我们利用无监督的方法来辨识这个词义，那么就让机器人“阅读”大量语料进行“学习”，生成语义辨识模型，这样当它听到这样一则对话时：

有一位老嫖客去找小姐，小姐问他什么时候结账啊。嫖客说：“钱的事情日后再说。”就开始了，完事后，小姐对嫖客说：“给钱吧。”嫖客懵了，说：“不是说日后再说吗？”小姐说：“是啊，你现在不是已经日后了吗？”

辨识了这里的“日后再说”的词义后，它会心的笑了

## 语义角色标注的基本方法

浅层语义标注是行之有效的语言分析方法，基于语义角色的浅层分析方法可以描述句子中语义角色之间的关系，是语义分析的重要方法，也是篇章分析的基础，本节介绍基于机器学习的语义角色标注方法

### 语义角色

举个栗子：“我昨天吃了一块肉”，按照常规理解“我吃肉”应该是句子的核心，但是对于机器来说“我吃肉”实际上已经丢失了非常多的重要信息，没有了时间，没有了数量。为了让机器记录并提取出这些重要信息，句子的核心并不是“我吃肉”，而是以谓词“吃”为核心的全部信息。

“吃”是谓词，“我”是施事者，“肉”是受事者，“昨天”是事情发生的时间，“一块”是数量。语义角色标注就是要分析出这一些角色信息，从而可以让计算机提取出重要的结构化信息，来“理解”语言的含义。

### 语义角色标注的基本方法

语义角色标注需要依赖句法分析的结果进行，因为句法分析包括短语结构分析、浅层句法分析、依存关系分析，所以语义角色标注也分为：基于短语结构树的语义角色标注方法、基于浅层句法分析结果的语义角色标注方法、基于依存句法分析结果的语义角色标注方法。但无论哪种方法，过程都是：

句法分析->候选论元剪除->论元识别->论元标注->语义角色标注结果

其中论元剪除就是在较多候选项中去掉肯定不是论元的部分

其中论元识别是一个二值分类问题，即：是论元和不是论元

其中论元标注是一个多值分类问题

下面分别针对三种方法分别说明这几个过程的具体方法

### 基于短语结构树的语义角色标注方法

短语结构树是这样的结构：

```
S——|
|        |
NN    VP
我      |——|
          Vt     NN
          吃     肉
```

短语结构树里面已经表达了一种结构关系，因此语义角色标注的过程就是依赖于这个结构关系来设计的一种复杂策略，策略的内容随着语言结构的复杂而复杂化，因此我们举几个简单的策略来说明。

首先我们分析论元剪除的策略：

因为语义角色是以谓词为中心的，因此在短语结构树中我们也以谓词所在的节点为中心，先平行分析，比如这里的“吃”是谓词，和他并列的是“肉”，明显“肉”是受事者，那么设计什么样的策略能使得它成为候选论元呢？我们知道如果“肉”存在一个短语结构的话，那么一定会多处一个树分支，那么“肉”和“吃”一定不会在树的同一层，因此我们设计这样的策略来保证“肉”被选为候选论元：如果当前节点的兄弟节点和当前节点不是句法结构的并列关系，那么将它作为候选论元。当然还有其他策略不需要记得很清楚，现用现查就行了，但它的精髓就是基于短语结构树的结构特点来设计策略的。

然后就是论元识别过程了。论元识别是一个二值分类问题，因此一定是基于标注的语料库做机器学习的，机器学习的二值分类方法都是固定的，唯一的区别就是特征的设计，这里面一般设计如下特征效果比较好：谓词本身、短语结构树路径、短语类型、论元在谓词的位置、谓词语态、论元中心词、从属类别、论元第一个词和最后一个词、组合特征。

论元识别之后就是论元标注过程了。这又是一个利用机器学习的多值分类器进行的，具体方法不再赘述。

#### 基于依存句法分析结果和基于语块的语义角色标注方法

这两种语义角色标注方法和基于短语结构树的语义角色标注方法的主要区别在于论元剪除的过程，原因就是他们基于的句法结构不同。

基于依存句法分析结果的语义角色标注方法会基于依存句法直接提取出谓词-论元关系，这和依存关系的表述是很接近的，因此剪除策略的设计也就比较简单：以谓词作为当前节点，当前节点所有子节点都是候选论元，将当前节点的父节点作为当前节点重复以上过程直至到根节点为止。

基于依存句法分析结果的语义角色标注方法中的论元识别算法的特征设计也稍有不同，多了有关父子节点的一些特征。

有了以上几种语义角色标注方法一定会各有优缺点，因此就有人想到了多种方法相融合的方法，融合的方式可以是：加权求和、插值……，最终效果肯定是更好，就不多说了。

### 多说几句

语义角色标注当前还是不是非常有效，原因有诸多方面，比如：依赖于句法分析的准确性、领域适应能力差。因此不断有新方法来解决这些问题，比如说可以利用双语平行语料来弥补准确性的问题，中文不行英文来，英文不行法语来，反正多多益善，这确实有助于改进效果，但是成本提高了许多。语义角色标注还有一段相当长的路要走，希望学术界研究能不断开花结果吧
