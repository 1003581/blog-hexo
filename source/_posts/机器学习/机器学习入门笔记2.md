---
title: 机器学习入门笔记1
date: 2017-09-14 16:05:13
tags: machinelearning
categories: 机器学习
---
原文[http://www.shareditor.com/blogshow?blogId=28](http://www.shareditor.com/blogshow?blogId=28)
<!-- more -->
## 细解卷积神经网络

[原文](http://www.shareditor.com/blogshow?blogId=95)

深度学习首先要讲的就是卷积神经网络，因为卷积神经网络沿用了之前讲过的多层神经网络的具体算法，同时在图像识别领域得到了非常好的效果。本节介绍它的数学原理和一些应用中的问题解决方案，最后通过公式讲解样本训练的方法 

### 卷积运算

卷积英文是convolution(英文含义是：盘绕、弯曲、错综复杂)，数学表达是：

![image](http://upload-images.jianshu.io/upload_images/5952841-d2a7051c4e038752.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上面连续的情形如果不好理解，可以转成离散的来理解，其实就相当于两个多项式相乘，如：`(x*x+3*x+2)(2*x+5)`，计算他的方法是两个多项式的系数分别交叉相乘，最后相加。用一句话概括就是：多项式相乘，相当于系数向量的卷积。

如果再不好理解，我们可以通俗点来讲：卷积就相当于在一定范围内做平移并求平均值。比如说回声可以理解为原始声音的卷积结果，因为回声是原始声音经过很多物体反射回来声音揉在一起。再比如说回声可以理解为把信号分解成无穷多的冲击信号，然后再进行冲击响应的叠加。再比如说把一张图像做卷积运算，并把计算结果替换原来的像素点，可以实现一种特殊的模糊，这种模糊其实是一种新的特征提取，提取的特征就是图像的纹路。总之卷积就是先打乱，再叠加。

下面我们在看上面的积分公式，需要注意的是这里是对τ积分，不是对x积分。也就是说对于固定的x，找到x附近的所有变量，求两个函数的乘积，并求和。

### 卷积神经网络

英文简称CNN，大家并不陌生，因为你可能见过DNN(深度神经网络)、RNN(循环神经网络)。CNN主要应用领域是图像处理，它本质上是一个分类器。

卷积神经网络为什么这么深得人心呢？因为在卷积神经网络的第一层就是特征提取层，也就是不需要我们自己做特征提取的工作，而是直接把原始图像作为输入，这带来了很大的便利，归根结底还是归功于卷积运算的神奇。

那么第一层是怎么利用卷积运算做特征提取的呢？我们还是通过图像处理的例子来说明。参考生物学的视觉结构，当人眼观察一个事物的时候，并不是每个视神经细胞感知所有看到的“像素”，而是一个神经细胞负责一小块视野，也就是说假设看到的全部视野是1000像素，而神经细胞有10个，那么一个神经细胞就负责比1000/10得到的平均值大一圈的范围，也就是200像素，一个细胞负责200个像素，10个细胞一共是2000个像素，大于1000个像素，说明有重叠。这和上面卷积运算的原理很像。用一张图来表示如下：

![image](http://upload-images.jianshu.io/upload_images/5952841-ad304a3c0aed97ac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 什么是卷积核

先看下面这张图，这是计算`5*5`矩阵中间的`3*3`部分的卷积值

![image](http://upload-images.jianshu.io/upload_images/5952841-fad1486a0872ee21.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

绿色部分是一个5*5的矩阵，标橙的部分说明正在进行卷积计算，×1表示算上这个单元的值，×0表示不计算，这样得出的结果1×1+1×0+1×1+0×0+1×1+1×0+0×1+0×0+1×1=4，这样计算出了第一个元素的卷积

我们继续让这个橙色部分移动并计算，最终会得到如下结果：

![image](http://upload-images.jianshu.io/upload_images/5952841-daac9dd7ad4ed58d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

那么这里的橙色(标记×1或×0)的矩阵(一般都是奇数行奇数列)就叫做**卷积核**，即

```
1 0 1
0 1 0
1 0 1
```

卷积计算实际上是一种对图像元素的矩阵变换，是提取图像特征的方法，多种卷积核可以提取多种特征。每一种卷积核生成的图像都叫做一个通道，这回也就理解了photoshop中“通道”的概念了吧

一个卷积核覆盖的原始图像的范围(上面就是5*5矩阵范围)叫做感受野(receptive field)，这个概念来自于生物学

### 多层卷积

利用一次卷积运算(哪怕是多个卷积核)提取的特征往往是局部的，难以提取出比较全局的特征，因此需要在一层卷积基础上继续做卷积计算 ，这也就是多层卷积。例如下面这个示意图：（无效图）

[图片上传失败...(image-2b8768-1513913040485)]

这实际上有四层卷积、三层池化、加上一层全连接，经过这些计算后得出的特征再利用常规的机器学习分类算法(如soft-max)做分类训练。上面这个过程是一个真实的人脸识别的卷积神经网络。

### 池化pooling

上面讲到了池化，池化是一种降维的方法。按照卷积计算得出的特征向量维度大的惊人，不但会带来非常大的计算量，而且容易出现过拟合，解决过拟合的办法就是让模型尽量“泛化”，也就是再“模糊”一点，那么一种方法就是把图像中局部区域的特征做一个平滑压缩处理，这源于局部图像一些特征的相似性(即局部相关性原理)。

具体做法就是对卷积计算得出的特征在局部范围内算出一个平均值(或者取最大值、或者取随机采样值)作为特征值，那么这个局部范围(假如是`10*10`)，就被压缩成了`1*1`，压缩了100倍，这样虽然更“模糊”了，但是也更“泛化”了。通过取平均值来池化叫做平均池化，通过取最大值来池化叫做最大池化。

### 卷积神经网络训练过程

上面讲解了卷积神经网络的原理，那么既然是深度学习，要学习的参数在哪里呢？

上面我们讲的卷积核中的因子(×1或×0)其实就是需要学习的参数，也就是卷积核矩阵元素的值就是参数值。一个特征如果有9个值，1000个特征就有9000个值，再加上多个层，需要学习的参数还是比较多的。

和多层神经网络一样，为了方便用链式求导法则更新参数，我们设计sigmoid函数作为激活函数，我们同时也发现卷积计算实际上就是多层神经网络中的Wx矩阵乘法，同时要加上一个偏执变量b，那么前向传到的计算过程就是：

![image](http://upload-images.jianshu.io/upload_images/5952841-e44c6f8f40173edc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如果有更多层，计算方法相同

因为是有监督学习，所以模型计算出的y'和观察值y之间的偏差用于更新模型参数，反向传导的计算方法参考反向传导算法：

参数更新公式是：

![image](http://upload-images.jianshu.io/upload_images/5952841-9d397f45abd1c6e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

偏导计算公式是：

![image](http://upload-images.jianshu.io/upload_images/5952841-80fef96ac4ef34df.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中a的计算公式是：

![image](http://upload-images.jianshu.io/upload_images/5952841-5937ab438723ca84.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

残差δ的计算公式是：

![image](http://upload-images.jianshu.io/upload_images/5952841-f2c7cf3b91f9cfc0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![image](http://upload-images.jianshu.io/upload_images/5952841-27d25a4ccab64add.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上面是输出层残差的推导公式和计算方法，下面是隐藏层残差的推导公式和计算方法

## 深究熵的概念和公式以及最大熵原理

[原文](http://www.shareditor.com/blogshow?blogId=99)

在机器学习算法中，最常用的优化方式就是使熵最大，那么到底什么是熵呢？很多文章告诉了我们概念和公式，但是很少有人讲到这些公式都是怎么来的，那么就让我们来深究一下这里面的奥秘 

### 熵

熵的英文是entropy，本来是一个热力学术语，表示物质系统的混乱状态。

我们都知道信息熵计算公式是`H(U)=-∑(p logp)`，但是却不知道为什么，下面我们深入熵的本源来证明这个公式

假设下图是一个孤立的由3个分子构成一罐气体

![image](http://upload-images.jianshu.io/upload_images/5952841-d9d59d7c23bbb2aa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

那么这三个分子所处的位置有如下几种可能性：

![image](http://upload-images.jianshu.io/upload_images/5952841-48473c6043511951.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

图中不同颜色表示的是宏观状态(不区分每个分子的不同)，那么宏观状态一共有4种，而微观状态(每一种组合都是一种微观状态)一共有2^3=8种

再来看4个分子的情况

![image](http://upload-images.jianshu.io/upload_images/5952841-0bcdc3145c67890d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这时，宏观状态一共有5种，而微观状态一共有2^4=16种

事实上分子数目越多，微观数目会成指数型增长

这里面提到的宏观状态实际上就是熵的某一种表现，如果气体中各种宏观状态都有，那么熵就大，如果只存在一种宏观状态，那么熵就很小，如果把每个分子看做状态的形成元素，熵的计算就可以通过分子数目以某种参数求对数得到，这时我们已经了解了为什么熵公式中是对数关系

上面我们描述的一个系统(一罐气体)，假如我们有两罐气体，那么它们放在一起熵应该是可以相加的(就像上面由四种状态加了一个状态的一个分子变成5个状态)，即可加性，而微观状态是可以相乘的(每多一个分子，微观状态就会多出n-1种)，即相乘法则

综上，我们可以得出熵的计算公式是`S=k ln Ω`，其中k是一个常数，叫做玻尔兹曼常数，`Ω`是微观状态数，这个公式也满足了上面的可加性和相乘法则，即`S1+S2=k ln (Ω1Ω2)`

### 最大熵

在机器学习中我们总是运用最大熵原理来优化模型参数，那么什么样的熵是最大熵，为什么它就是最优的

这还是要从物理学的原理来说明，我们知道当没有外力的情况下气体是不断膨胀的而不会自动收缩，两个温度不同的物体接触时总是从高温物体向低温物体传导热量而不可逆。我们知道宏观状态越多熵越大，那么气体膨胀实际上是由较少的宏观状态向较多的宏观状态在变化，热传导也是一样，如此说来，一个孤立系统总是朝着熵增加的方向变化，熵越大越稳定，到最稳定时熵达到最大，这就是熵增原理

换句话说：熵是孤立系统的无序度的量度，平衡态时熵最大

将熵增原理也可以扩大到一切自发过程的普遍规律，比如如果你不收拾屋子，那么屋子一定会变得越来越乱而不会越来越干净整洁，扩大到统计学上来讲，屋子乱的概率更大，也就是说孤立系统中一切实际过程总是从概率小的状态向概率大的状态的转变过程，并且不可逆

### 信息熵

1948年，信息论之父香农发表的《通信的数学理论》中提出了“信息熵”的概念，从此信息熵对通信和计算机行业产生了巨大的影响。那么他到底说了些什么呢？

一个随机变量`ξ`有`A1、A2、A3……`共n个不同的结果，每个结果出现的概率是`p1、p2、p3……`，那么我们把`ξ`的不确定度定义为信息熵，参考上面物理学熵的定义，`A1、A2、A3……`可以理解为不同的微观状态，那么看起来信息熵应该是`log n`喽？不然，因为这个随机变量`ξ`一次只能取一个值而不是多个值，所以应该按概率把`ξ`劈开，劈成n份，每份的微观状态数分别是`1/p1`、`1/p2`、`1/p3`……，这样这n份的熵分别是`log 1/p1`、`log 1/p2`、`log 1/p3`……，再根据熵的可加性原理，得到整体随机变量`ξ`的信息熵是`∑(p log 1/p)`，即`H(ξ) = -∑(p log p)`

### 最大熵原理

继续看上面的信息熵公式，从公式可以看出，出现各种随机结果可能性越大，不确定性就越大，熵就越大。相反，如果只可能出现一种结果，那么熵就为0，因为这时p=1，-∑(p log p)=0

举个例子，投1000次硬币，最有可能的概率是正面1/2，负面1/2，因此熵是H(X) = -(0.5log0.5+0.5log0.5) = -0.5*math.log(2,1/2)*2 = -0.5*-1*2 = 1

那么假设只会出现正面，熵是H(X) = -1log1 = 0

实际上哪种是最符合实际情况的呢？显然是第一种，这就是最大熵模型的原理：在机器学习中之所以优化最大熵公式能训练出最接近正确值的参数值，是因为这是“最符合实际”的可能。换句有哲理的话说：熵越大越趋向于自然，越没有偏见

### 最大熵模型

机器学习中用到的最大熵模型是一个定义在条件概率分布P(Y|X)上的条件熵。其中X、Y分别对应着数据的输入和输出，根据最大熵原理，当熵最大时，模型最符合实际情况。那么这个条件熵应该是什么样的呢？

条件概率分布P(Y|X)上的条件熵可以理解为在X发生的前提下，Y发生所“新”带来的熵，表示为H(Y|X)，那么有

```
H(Y|X) = H(X,Y) - H(X)
```

其中H(X,Y)表示X、Y的联合熵，表示X、Y都发生时的熵，H(Y|X)的计算公式推导如下：

![image](http://upload-images.jianshu.io/upload_images/5952841-cb10c75d8759190d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

因此我们在机器学习中想方设法优化的就是这个东东，由于这里的p(x,y)无法统计，因此我们转成p(x)p(y|x)，这样得到公式如下：

```
H(Y|X) = -∑p(x)p(y|x)log p(y|x)
```

那么机器学习训练的过程实际就是求解p(y|x)的过程，其中p(x)可以通过x的最大似然估计直接求得

### 总结

至此，我们介绍完了熵的概念和公式以及最大熵原理和最大熵模型公式的由来，总之，熵来源于热力学，扩展于信息论，应用在机器学习领域，它表达的是一种无序状态，也是最趋向于自然、最符合实际的情况。为了更深入感受最大熵模型的魅力，后续我会把最大熵模型的应用不断渗透到机器学习教程的具体算法中

## 逻辑回归公式的数学推导

[原文](http://www.shareditor.com/blogshow?blogId=102)

机器学习中一些重要的公式，比如逻辑回归概率公式，多数情况下我们知道何时拿来用，但是它们都是怎么得来的呢，本节让我们详细探讨下 

### 逻辑回归中的数学推导

逻辑回归模型是基于这样的逻辑分布得出的模型

F(x) = 1/(1+e^x)

由此也得出了二项逻辑回归分布是：

P(Y=1|x) = e^(wx+b)/(1+e^(wx+b))

P(Y=0|x) = 1/(1+e^(wx+b))

也得出了多项逻辑回归分布是：

P(Y=k|x) =  e^(wx)/(1+∑e^(wx))

那么这个 1/(1+e^x)到底是怎么来的呢？我们来证明这一公式

首先假设0、1分布当Y=1的概率为

P(Y=1) = φ

那么

P(Y=0) = 1-φ

把他们变成统一的形式可以是：

P(y; φ) = φ^y (1-φ)^(1-y)

解释一下，这里如果y=0，那么前一项是1，就是p(y;φ) = 1-φ，而如果y=1，那么后一项就为1，就是p(y;φ) = φ

下面继续推导，我们知道有一个等式：a = e^(ln a)

那么把右面改成指数形式如下：

P(y; φ) = φ^y (1-φ)^(1-y) = e^(log(φ^y (1-φ)^(1-y))) = e ^ (y logφ + (1-y) log(1-φ)) = e^(log(φ/(1-φ))y+log(1-φ))

因为任何分布都能写成一种指数形式的分布：

p(y; η) = b(y) e^(ηT(η) - a(η))

这里面我们按照对应关系得出η=log(φ/(1-φ))，那么φ/(1-φ) = e^η，那么解出φ = 1/(1+e^(-η))

所以得出P(Y=1) = φ =  1/(1+e^(-η))

大功告成，终于知道逻辑回归公式是怎么来的了

## R语言特征工程实战

特征工程是机器学习过程中和模型训练同样重要的部分，特征如何提取、如何处理、如何选择、如何使用都是特征工程的范畴，特征工程需要具备数据分析的能力，那些称为数据科学家的人一定是有很强的特征工程能力的人。R语言是大数据领域的主流语言之一，本文主要介绍用R语言的图形工具做特征工程的实战方法

### R语言介绍

熟悉R语言的朋友请直接略过。R语言是贝尔实验室开发的S语言(数据统计分析和作图的解释型语言)的一个分支，主要用于统计分析和绘图，R可以理解为是一种数学计算软件，可编程，有很多有用的函数库和数据集。

### R的安装和使用

在 https://mirrors.tuna.tsinghua.edu.cn/CRAN/ 下载对应操作系统的安装包安装。安装好后单独创建一个目录作为工作目录(因为R会自动在目录里创建一些有用的隐藏文件，用来存储必要的数据)

执行

```
R
```

即可进入R的交互运行环境

简单看一个实例看一下R是如何工作的：

```
[root@centos:~/Developer/r_work $] R

R version 3.3.1 (2016-06-21) -- "Bug in Your Hair"
Copyright (C) 2016 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin13.4.0 (64-bit)

> x <- c(1,2,3,4,5,6,7,8,9,10)
> y <- x*x
> plot(x,y,type="l")
>
```

以上看得出我们画了y = x^2的曲线

R语言的语法和C类似，但是稍有不同，R语言里向量和矩阵的操作和python的sci-learn类似，但是稍有不同：

1. R的赋值语句的符号是"<-"而不是"="

2. R里的向量用c()函数定义，R里没有真正的矩阵类型，矩阵就是一系列向量组成的list结构

有时候如果我们想要加载一个库发现没有安装，就像这样：

```
> library(xgboost)
Error in library(xgboost) : 不存在叫‘xgboost’这个名字的程辑包
```

那么就这样来安装：

```
> install.packages("xgboost")
```

输入后会提示选择下载镜像，选择好后点ok就能自动安装完成，这时就可以正常加载了：

```
> library(xgboost)
>
```

想了解R语言的全部用法，推荐《权威的R语言入门教程《R导论》-丁国徽译.pdf》，请自行下载阅读，也可以继续看我下面的内容边用边学

### 特征工程

按我的经验，特征工程就是选择和使用特征的过程和方法，这个说起来容易，做起来真的不易，想要对实际问题设计一套机器学习方法，几乎大部分时间都花在了特征工程上，相反最后的模型开发花不了多长时间(因为都是拿来就用了)，再有需要花一点时间的就是最后的模型参数调优了。花费时间排序一般是：特征工程>模型调参>模型开发

### Titanic数据集特征工程实战

Titanic数据集是这样的数据：Titanic(泰坦尼克号)沉船灾难死亡了很多人也有部分人成功得救，数据集里包括了这些字段：乘客级别、姓名、性别、年龄、船上的兄弟姐妹数、船上的父母子女数、船票编号、票价、客舱编号、登船港口、是否得救。

我们要做的事情就是把Titanic数据集中部分数据作为训练数据，然后用来根据测试数据中的字段值来预测这位乘客是否得救

### 数据加载

训练数据可以在 https://www.kaggle.com/c/titanic/download/train.csv 下载，测试数据可以在 https://www.kaggle.com/c/titanic/download/test.csv 下载

下面开始我们的R语言特征工程，创建一个工作目录r_work，下载train.csv和test.csv到这个目录，看下里面的内容：

```
[root@centos:~/Developer/r_work $] head train.csv
PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked
1,0,3,"Braund, Mr. Owen Harris",male,22,1,0,A/5 21171,7.25,,S
2,1,1,"Cumings, Mrs. John Bradley (Florence Briggs Thayer)",female,38,1,0,PC 17599,71.2833,C85,C
3,1,3,"Heikkinen, Miss. Laina",female,26,0,0,STON/O2. 3101282,7.925,,S
4,1,1,"Futrelle, Mrs. Jacques Heath (Lily May Peel)",female,35,1,0,113803,53.1,C123,S
5,0,3,"Allen, Mr. William Henry",male,35,0,0,373450,8.05,,S
6,0,3,"Moran, Mr. James",male,,0,0,330877,8.4583,,Q
7,0,1,"McCarthy, Mr. Timothy J",male,54,0,0,17463,51.8625,E46,S
8,0,3,"Palsson, Master. Gosta Leonard",male,2,3,1,349909,21.075,,S
9,1,3,"Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)",female,27,0,2,347742,11.1333,,S
```

我们看到文件内容是用逗号分隔的多个字段，第一行是schema，第二行开始是数据部分，其中还有很多空值，事实上csv就是Comma-Separated Values，也就是用“逗号分隔的数值”，它也可以用excel直接打开成表格形式

R语言为我们提供了加载csv文件的函数，如下：

```
> train <- read.csv('train.csv', stringsAsFactors = F)
> test <- read.csv('test.csv', stringsAsFactors = F)
```

如果想看train和test变量的类型，可以执行：

```
> mode(train)
[1] "list"
```

我们看到类型是列表类型

如果想预览数据内容，可以执行：

```
> str(train)
'data.frame':   891 obs. of  12 variables:
 $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...
 $ Survived   : int  0 1 1 1 0 0 0 0 1 1 ...
 $ Pclass     : int  3 1 3 1 3 3 1 3 3 2 ...
 $ Name       : chr  "Braund, Mr. Owen Harris" "Cumings, Mrs. John Bradley (Florence Briggs Thayer)" "Heikkinen, Miss. Laina" "Futrelle, Mrs. Jacques Heath (Lily May Peel)" ...
 $ Sex        : chr  "male" "female" "female" "female" ...
 $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...
 $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...
 $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...
 $ Ticket     : chr  "A/5 21171" "PC 17599" "STON/O2. 3101282" "113803" ...
 $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...
 $ Cabin      : chr  "" "C85" "" "C123" ...
 $ Embarked   : chr  "S" "C" "S" "S" ...
```

可以看到其实train和test变量把原始的csv文件解析成了特定的数据结构，train里有891行、12列，每一列的字段名、类型以及可能的值都能预览到

因为test数据集也是真实数据的一部分，所以在做特征工程的时候可以把test和train合并到一起，生成full这个变量，后面我们都分析full：

```
> library('dplyr')
> full  <- bind_rows(train, test)
```

### 头衔特征的提取

因为并不是所有的字段都应该用来作为训练的特征，也不是只有给定的字段才能作为特征，下面我们开始我们的特征选择工作，首先我们从乘客的姓名入手，我们看到每一个姓名都是这样的结构："名字, Mr/Mrs/Capt等. 姓"，这里面的"Mr/Mrs/Capt等"其实是一种称谓(Title)，虽然人物的姓名想必和是否得救无关，但是称谓也许和是否得救有关，我们把所有的Title都筛出来：

```
> table(gsub('(.*, )|(\\..*)', '', full$Name))

        Capt          Col          Don         Dona           Dr     Jonkheer
           1            4            1            1            8            1
        Lady        Major       Master         Miss         Mlle          Mme
           1            2           61          260            2            1
          Mr          Mrs           Ms          Rev          Sir the Countess
         757          197            2            8            1            1
```

解释一下，这里面的full$Name表示取full里的Name字段的内容，gsub是做字符串替换，table是把结果做一个分类统计(相当于group by title)，得出数目

通过结果我们看到不同Title的人数目差别比较大

我们把这个Title加到full的属性里：

```
> full$Title <- gsub('(.*, )|(\\..*)', '', full$Name)
```

这时我们可以按性别和title分两级统计(相当于group by sex, title):

```
> table(full$Sex, full$Title)

         Capt Col Don Dona  Dr Jonkheer Lady Major Master Miss Mlle Mme  Mr Mrs
  female    0   0   0    1   1        0    1     0      0  260    2   1   0 197
  male      1   4   1    0   7        1    0     2     61    0    0   0 757   0

          Ms Rev Sir the Countess
  female   2   0   0            1
  male     0   8   1            0
```

为了让这个特征更具有辨别性，我们想办法去掉那些稀有的值，比如总次数小于10的，我们都把title改成“Rare Title”

```
> rare_title <- c('Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer')
> full$Title[full$Title %in% rare_title]  <- 'Rare Title'
```

同时把具有相近含义的title做个归一化

```
> full$Title[full$Title == 'Mlle']        <- 'Miss'
> full$Title[full$Title == 'Ms']          <- 'Miss'
> full$Title[full$Title == 'Mme']         <- 'Mrs'
```

这回我们看下title和是否得救的关系情况

```
> table(full$Title, full$Survived)

               0   1
  Master      17  23
  Miss        55 130
  Mr         436  81
  Mrs         26 100
  Rare Title  15   8
```

还不够直观，我们可以通过马赛克图来形象的看：

```
> mosaicplot(table(full$Sex, full$Title), shade=TRUE)
```

这回看出比例情况的差异了，比如title为Mr的死亡和得救的比例比较明显，说明这和是否得救关系密切，title作为一个特征是非常有意义的

这样第一个具有代表意义的特征就提取完了

### 家庭成员数特征的提取

看过电影的应该了解当时的场景，大家是按照一定秩序逃生的，所以很有可能上有老下有小的家庭会被优先救援，所以我们统计一下一个家庭成员的数目和是否得救有没有关系。

为了计算家庭成员数目，我们只要计算父母子女兄弟姐妹的数目加上自己就可以，所以：

```
> full$Fsize <- full$SibSp + full$Parch + 1
```

下面我们做一个Fsize和是否得救的图像

```
> library("ggplot2")
> library('ggthemes')
> ggplot(full[1:891,], aes(x = Fsize, fill = factor(Survived))) + geom_bar(stat='count', position='dodge') + scale_x_continuous(breaks=c(1:11)) + labs(x = 'Family Size') + theme_few()
```

我们先解释一下上面的ggplot语句

第一个参数full[1:891,]表示我们取全部数据的前891行的所有列，取891是因为train数据一共有891行

aes(x = Fsize, fill = factor(Survived))表示坐标轴的x轴我们取Fsize的值，这里的fill是指用什么变量填充统计值，factor(Survived)表示把Survived当做一种因子，也就是只有0或1两种“情况”而不是数值0和1，这样才能分成红绿两部分统计，不然如果去掉factor()函数包裹就会像这个样子(相当于把0和1加了起来)：

这里的“+”表示多个图层，是ggplot的用法

geom_bar就是画柱状图，其中stat='count'表示统计总数目，也就是相当于count(*) group by factor(Survived)，position表示重叠的点放到什么位置，这里设置的是“dodge”表示规避开的展示方式，如果设置为"fill"就会是这样的效果：

scale_x_continuous(breaks=c(1:11))就是说x轴取值范围是1到11，labs(x = 'Family Size')是说x轴的label是'Family Size'，theme_few()就是简要主题

下面我们详细分析一下这个图说明了什么事情。我们来比较不同家庭成员数目里面成功逃生的和死亡的总数的比例情况可以看出来：家庭人数是1或者大于4的情况下红色比例较大，也就是死亡的多，而人数为2、3、4的情况下逃生的多，因此家庭成员数是一个有意义的特征，那么把这个特征总结成singleton、small、large三种情况，即：

```
> full$FsizeD[full$Fsize == 1] <- 'singleton'
> full$FsizeD[full$Fsize < 5 & full$Fsize > 1] <- 'small'
> full$FsizeD[full$Fsize > 4] <- 'large'
```

再看下马赛克图：

```
> mosaicplot(table(full$FsizeD, full$Survived), main='Family Size by Survival', shade=TRUE)
```

从图中可以看出差异明显，特征有意义

### 模型训练

处理好特征我们就可以开始建立模型和训练模型了，我们选择随机森林作为模型训练。首先我们要把要作为factor的变量转成factor：

```
> factor_vars <- c('PassengerId','Pclass','Sex','Embarked','Title','FsizeD')
> full[factor_vars] <- lapply(full[factor_vars], function(x) as.factor(x))
```

然后我们重新提取出train数据和test数据

```
> train <- full[1:891,]
> test <- full[892:1309,]
```

接下来开始训练我们的模型

```
> library('randomForest')
> set.seed(754)
> rf_model <- randomForest(factor(Survived) ~ Pclass + Sex + Embarked + Title + FsizeD, data = train)
```

下面画出我们的模型误差变化：

```
> plot(rf_model, ylim=c(0,0.36))
> legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
```

图像表达的是不同树个数情况下的误差率，黑色是整体情况，绿色是成功获救的情况，红色是死亡的情况，可以看出通过我们给定的几个特征，对死亡的预测误差更小更准确

我们还可以利用importance函数计算特征重要度：

```
> importance(rf_model)
         MeanDecreaseGini
Pclass          40.273719
Sex             53.240211
Embarked         8.566492
Title           85.214085
FsizeD          23.543209
```

可以看出特征按重要程度从高到底排序是：Title > Sex > Pclass > FsizeD > Embarked

### 数据预测

有了训练好的模型，我们可以进行数据预测了

```
> prediction <- predict(rf_model, test)
```

这样prediction中就存储了预测好的结果，以0、1表示

为了能输出我们的结果，我们把test数据中的PassengerId和prediction组合成csv数据输出

```
> solution <- data.frame(PassengerID = test$PassengerId, Survived = prediction)
> write.csv(solution, file = 'solution.csv', row.names = F)
```

最终的solution.csv的内容如下：

```
[root@centos:~/Developer/r_work $] head solution.csv
"PassengerID","Survived"
"892","0"
"893","1"
"894","0"
"895","0"
"896","1"
"897","0"
"898","1"
"899","0"
"900","1"
```

本文部分内容参考：https://www.kaggle.com/mrisdal/titanic/exploring-survival-on-the-titanic/comments

## 看数据科学家是如何找回丢失的数据的

[原文1](http://www.shareditor.com/blogshow?blogId=107)

[原文2](http://www.shareditor.com/blogshow?blogId=108)

在做特征工程过程中，经常遇到某些样本缺失了某个特征的值，影响我们的机器学习过程，如果是较小的样本集数据科学家可不会直接舍弃这些样本，而是利用有效的手段把丢失的数据找回来，他们是怎么找回的呢？我接下来的几篇文章会通过实例讲几种缺失值补全的方法 

### 补全数据的纯手工方案

我们以泰坦尼克号数据集为例

先重温一下这个数据集里面都有哪些字段：

PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked

分别表示：

样本编号、是否得救、乘客级别、姓名、性别、年龄、船上的兄弟姐妹数、船上的父母子女数、船票编号、票价、客舱编号、登船港口。

我们检查一下Embarked这个字段哪些乘客是缺失的：

```
> full$PassengerId[full$Embarked == '']
[1] 62  830
```

看来在这1309位乘客中PassengerId 为62和830的乘客缺失了Embarked字段值，那么我们如何来补全这个数据呢？我们分析一下哪个字段可能和Embarked(登船港口)的值有关，我们猜测票价有可能和Embarked有关，但是不同级别的票价一定又是不一样的，那么我们可以看一下不同级别票价的统计规律，庆幸的是Embarked只有三个取值：C Q S分别表示C = Cherbourg; Q = Queenstown; S = Southampton

我们先来看一下62  830的票价和乘客级别是多少：

```
> full[c(62, 830), 'Fare']
[1] 80 80
> full[c(62, 830), 'Pclass']
[1] 1 1
```

等级都是1级，票价都是80

现在我们再看下这三个港口对应不同级别的乘客平均票价是多少，在此之前我们先排除掉62  830这两位乘客的数据：

```
> library("dplyr")
> embark_fare <- full %>% filter(PassengerId != 62 & PassengerId != 830)
```

下面我们利用强大的ggplot2画出盒图(boxplot)，首先说一下什么是盒图，盒图由五个数值点组成：最小值(min)，下四分位数(Q1)，中位数(median)，上四分位数(Q3)，最大值(max)。也可以往盒图里面加入平均值(mean)。下四分位数、中位数、上四分位数组成一个“带有隔间的盒子”。上四分位数到最大值之间建立一条延伸线，这个延伸线成为“胡须(whisker)”。盒图用来反映离散数据的分布情况。

下面我们画出不同Embarked、不同等级乘客对应的Fare的盒图

```
> library("ggplot2")
> library('ggthemes')
> ggplot(embark_fare, aes(x = Embarked, y = Fare, fill = factor(Pclass))) +geom_boxplot()+geom_hline(aes(yintercept=80),colour='red', linetype='dashed', lwd=2)+theme_few()
```

讲解一下这个命令，`geom_boxplot`表示画盒图，`geom_hline`表示沿着横轴方向画线，如果想沿着纵轴那么就用`geom_vline`，lwd表示线宽

为了能找到和62, 830两位乘客相似的情况，单独把Fare为80的位置画出了一条横线，用来参照。我们发现Pclass=1的乘客Fare均值最接近80的是C港口，因此我们把这两位乘客的Embarked就赋值为C：

```
> full$Embarked[c(62, 830)] <- 'C'
```

当然我们还可以画这样一张图来看待这个事情：

```
> ggplot(full[full$Pclass == '1' & full$Embarked == 'C', ],
+ aes(x = Fare)) +
+ geom_density(fill = '#99d6ff', alpha=0.4) +
+ geom_vline(aes(xintercept=median(Fare, na.rm=T)),
+ colour='red', linetype='dashed', lwd=1) +
+ geom_vline(aes(xintercept=80),colour='green',linetype='dashed', lwd=1) +
+ theme_few()
```

讲解一下：这里选择Pclass==1，Embarked == 'C'的数据，画出了概率密度曲线，同时把Fare的均值画了一条红色的竖线，也在Fare=80的位置画了一条绿色的竖线作为参照，可以直观看出均值和80很接近

本文部分内容参考：https://www.kaggle.com/mrisdal/titanic/exploring-survival-on-the-titanic/comments
