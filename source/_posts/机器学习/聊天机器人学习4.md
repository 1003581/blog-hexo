---
title: 聊天机器人学习4
date: 2017-09-14 16:04:36
tags: chatbot
categories: 机器学习
---

[原文](http://www.shareditor.com/bloglistbytag/?tagname=%E8%87%AA%E5%B7%B1%E5%8A%A8%E6%89%8B%E5%81%9A%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA)

<!-- more -->

## 用深度学习来做自动问答的一般方法

聊天机器人本质上是一个范问答系统，既然是问答系统就离不开候选答案的选择，利用深度学习的方法可以帮助我们找到最佳的答案，本节我们来讲述一下用深度学习来做自动问答的一般方法 

### 语料库的获取方法

对于一个范问答系统，一般我们从互联网上收集语料信息，比如百度、谷歌等，用这些结果构建问答对组成的语料库。然后把这些语料库分成多个部分：训练集、开发集、测试集

问答系统训练其实是训练一个怎么在一堆答案里找到一个正确答案的模型，那么为了让样本更有效，在训练过程中我们不把所有答案都放到一个向量空间中，而是对他们做个分组，首先，我们在语料库里采集样本，收集每一个问题对应的500个答案集合，其中这500个里面有正向的样本，也会随机选一些负向样本放里面，这样就能突出这个正向样本的作用了

### 基于CNN的系统设计

CNN的三个优点：sparse interaction(稀疏的交互)，parameter sharing(参数共享)，equivalent respresentation(等价表示)。正是由于这三方面的优点，才更适合于自动问答系统中的答案选择模型的训练。

我们设计卷积公式表示如下（不了解卷积的含义请见《机器学习教程 十五-细解卷积神经网络》）：

![img](http://upload-images.jianshu.io/upload_images/5952841-ddccf1f2f4083a55.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

假设每个词用三维向量表示，左边是4个词，右边是卷积矩阵，那么得到输出为：

![img](http://upload-images.jianshu.io/upload_images/5952841-ee2510f3993e40a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如果基于这个结果做1-MaxPool池化，那么就取o中的最大值

### 通用的训练方法

训练时获取问题的词向量Vq(这里面词向量可以使用google的word2vec来训练,有关word2vec的内容可以看《自己动手做聊天机器人 二十五-google的文本挖掘深度学习工具word2vec的实现原理》)，和一个正向答案的词向量Va+，和一个负向答案的词向量Va-， 然后比较问题和这两个答案的相似度，两个相似度的差值如果大于一个阈值m就用来更新模型参数，然后继续在候选池里选答案，小于m就不更新模型，即优化函数为：

![img](http://upload-images.jianshu.io/upload_images/5952841-a81d71220e72528d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

参数更新方式和其他卷积神经网络方式相同，都是梯度下降、链式求导

对于测试数据，计算问题和候选答案的cos距离，相似度最大的那个就是正确答案的预测

### 神经网络结构设计

以下是六种结构设计，解释一下，其中HL表示hide layer隐藏层，它的激活函数设计成z = tanh(Wx+B)，CNN是卷积层，P是池化层，池化步长为1，T是tanh层，P+T的输出是向量表示，最终的输出是两个向量的cos相似度

图中HL或CNN连起来的表示他们共享相同的权重。CNN的输出是几维的取决于做多少个卷积特征，如果有4个卷积，那么结果就是4*3的矩阵(这里面的3在下一步被池化后就变成1维了)

![img](http://upload-images.jianshu.io/upload_images/5952841-a597b9f94b80e21a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

以上结构的效果在论文《Applying Deep Learning To Answer Selection- A Study And An Open Task》中有详细说明，这里不赘述

### 总结

要把深度学习运用到聊天机器人中，关键在于以下几点：

1. 对几种神经网络结构的选择、组合、优化

2. 因为是有关自然语言处理，所以少不了能让机器识别的词向量

3. 当涉及到相似或匹配关系时要考虑相似度计算，典型的方法是cos距离

4. 如果需求涉及到文本序列的全局信息就用CNN或LSTM

5. 当精度不高时可以加层

6. 当计算量过大时别忘了参数共享和池化

## 脑洞大开：基于美剧字幕的聊天语料库建设方案

要让聊天机器人进行学习，需要海量的聊天语料库，但是网上的语料库基本上都是有各种标注的文章，并没有可用的对话语料，虽然有一些社区的帖子数据，但是既要花大把银子还不知道质量如何。笔者突然灵机一动，找到一个妙招能获取海量高质聊天语料，这下聊天机器人再也不愁语料数据了。 

### 美剧字幕

是的，你没有看错，我就是这样获取海量高质聊天语料的。外文电影或电视剧的字幕文件是一个天然的聊天语料，尤其是对话比较多的美剧最佳。为了能下载大量美剧字幕，我打算抓取字幕库网站www.zimuku.net，当然你也可以选择其他网站抓取。

### 自动抓取字幕

有关爬虫相关内容请见我的另一篇文章《教你成为全栈工程师(Full Stack Developer) 三十-十分钟掌握最强大的python爬虫》。在这里我直接贴上我的抓取器重要代码(代码共享在了https://github.com/warmheartli/ChatBotCourse)：

```
# coding:utf-8

import sys
reload(sys)
sys.setdefaultencoding( "utf-8" )

import scrapy
from w3lib.html import remove_tags
from subtitle_crawler.items import SubtitleCrawlerItem

class SubTitleSpider(scrapy.Spider):
    name = "subtitle"
    allowed_domains = ["zimuku.net"]
    start_urls = [
            "http://www.zimuku.net/search?q=&t=onlyst&ad=1&p=20",
            "http://www.zimuku.net/search?q=&t=onlyst&ad=1&p=21",
            "http://www.zimuku.net/search?q=&t=onlyst&ad=1&p=22",
    ]

    def parse(self, response):
        hrefs = response.selector.xpath('//div[contains(@class, "persub")]/h1/a/@href').extract()
        for href in hrefs:
            url = response.urljoin(href)
            request = scrapy.Request(url, callback=self.parse_detail)
            yield request

    def parse_detail(self, response):
        url = response.selector.xpath('//li[contains(@class, "dlsub")]/div/a/@href').extract()[0]
        print "processing: ", url
        request = scrapy.Request(url, callback=self.parse_file)
        yield request

    def parse_file(self, response):
        body = response.body
        item = SubtitleCrawlerItem()
        item['url'] = response.url
        item['body'] = body
        return item
```

下面是pipeline.py代码：

```
class SubtitleCrawlerPipeline(object):
    def process_item(self, item, spider):
        url = item['url']
        file_name = url.replace('/','_').replace(':','_')
        fp = open('result/'+file_name, 'w')
        fp.write(item['body'])
        fp.close()
        return item
```

看下我抓取的最终效果

```
[root@centos:~/Developer/ChatBotCourse/subtitle $] ls result/|head -1
http___shooter.zimuku.net_download_265300_Hick.2011.720p.BluRay.x264.YIFY.rar
[root@centos:~/Developer/ChatBotCourse/subtitle $] ls result/|wc -l
82575
[root@centos:~/Developer/ChatBotCourse/subtitle $] du -hs result/
16G     result/
```

### 字幕文件的解压方法

linux下怎么解压zip文件

直接执行unzip file.zip即可

linux下怎么解压rar文件

http://www.rarlab.com/download.htm

wget http://www.rarlab.com/rar/rarlinux-x64-5.4.0.tar.gz

tar zxvf rarlinux-x64-5.4.0.tar.gz

./rar/unrar试试

解压命令：

unrar x file.rar

 

linux下怎么解压7z文件

http://downloads.sourceforge.net/project/p7zip下载源文件，解压后执行make编译后bin/7za可用，用法

bin/7za x file.7z

### 最终字幕的处理方式

有关解压出来的文本字幕文件的处理，我后面的文章会详细讲解如何分词、如何组合，敬请期待。 

## 重磅：近1GB的三千万聊天语料供出

经过半个月的倾力打造，建设好的聊天语料库包含三千多万条简体中文高质量聊天语料，近1G的纯文本数据。此语料库全部基于2万部影视剧字幕，经由爬取、分类、解压、语言识别、编码识别、编码转换、过滤清洗等一系列繁琐过程。把整个建设过程分享出来供大家玩耍。 

注意：本文提到的程序和脚本都分享在https://github.com/warmheartli/ChatBotCourse 。如需直接获取最终语料库，请见文章末尾。

### 第一步：爬取影视剧字幕

请见我的这篇文章《二十八-脑洞大开：基于美剧字幕的聊天语料库建设方案》

### 第二步：压缩格式分类

下载的字幕有zip格式和rar格式，因为数量比较多，需要做筛选分类，以便后面的处理，这步看似简单实则不易，因为要解决：文件多无法ls的问题、文件名带特殊字符的问题、文件名重名误覆盖问题、扩展名千奇百怪的问题，我写成了python脚本mv_zip.py如下：

```
import glob
import os
import fnmatch
import shutil
import sys

def iterfindfiles(path, fnexp):
    for root, dirs, files in os.walk(path):
        for filename in fnmatch.filter(files, fnexp):
            yield os.path.join(root, filename)

i=0
for filename in iterfindfiles(r"./input/", "*.zip"):
    i=i+1
    newfilename = "zip/" + str(i) + "_" + os.path.basename(filename)
    print filename + " <===> " + newfilename
    shutil.move(filename, newfilename)
```

其中的扩展名根据压缩文件可能有的扩展名修改成`*.rar`、`*.RAR`、`*.zip`、`*.ZIP`等

### 第三步：解压

解压这一步需要根据所用的操作系统下载不同的解压工具，建议使用unrar和unzip，为了解决解压后文件名可能重名覆盖的问题，我总结出如下两句脚本来实现批量解压：

```
i=0; for file in `ls`; do mkdir output/${i}; echo "unzip $file -d output/${i}";unzip -P abc $file -d output/${i} > /dev/null; ((i++)); done
i=0; for file in `ls`; do mkdir output/${i}; echo "${i} unrar x $file output/${i}";unrar x $file output/${i} > /dev/null; ((i++)); done
```

### 第四步：srt、ass、ssa字幕文件分类整理

当你下载大量字幕并解压后你会发现字幕文件类型有很多种，包括srt、lrc、ass、ssa、sup、idx、str、vtt，但是整体量级上来看srt、ass、ssa占绝对优势，因此简单起见，我们抛弃掉其他格式，只保留这三种，具体分类整理的脚本可以参考第二部压缩格式分类的方法按扩展名整理

### 第五步：清理目录

在我边整理边分析的过程中发现，我为了避免重名把文件放到不同目录里后，如果再经过一步文件类型整理，会产生非常多的空目录，每次ls都要拉好几屏，所以写了一个自动清理空目录的脚本clear_empty_dir.py，如下：

```
import glob
import os
import fnmatch
import shutil
import sys

def iterfindfiles(path, fnexp):
    for root, dirs, files in os.walk(path):
        if 0 == len(files) and len(dirs) == 0:
            print root
            os.rmdir(root)

iterfindfiles(r"./input/", "")
```

### 第六步：清理非字幕文件

在整个字幕文件分析过程中，总有很多其他文件干扰你的视线，比如txt、html、doc、docx，因为不是我们想要的，因此干脆直接干掉，批量删除脚本del_file.py如下：

```
import glob
import os
import fnmatch
import shutil
import sys

def iterfindfiles(path, fnexp):
    for root, dirs, files in os.walk(path):
        for filename in fnmatch.filter(files, fnexp):
            yield os.path.join(root, filename)

for suffix in ("*.mp4", "*.txt", "*.JPG", "*.htm", "*.doc", "*.docx", "*.nfo", "*.sub", "*.idx"):
    for filename in iterfindfiles(r"./input/", suffix):
        print filename
        os.remove(filename)
```

### 第七步：多层解压缩

把抓取到的字幕压缩包解压后有的文件里面依然还有压缩包，继续解压才能看到字幕文件，因此上面这些步骤再来一次，不过要做好心理准备，没准需要再来n次！

### 第八步：舍弃剩余的少量文件

经过以上几步的处理后剩下一批无扩展名的、特殊扩展名如：“srt.简体”，7z等、少量压缩文件，总体不超过50M，想想伟大思想家马克思教导我们要抓主要矛盾，因此这部分我们直接抛弃掉

### 第九步：编码识别与转码

字幕文件就是这样的没有规范，乃至于各种编码齐聚，什么utf-8、utf-16、gbk、unicode、iso8859琳琅满目应有尽有，我们要统一到一种编码方便使用，索性我们统一到utf-8，get_charset_and_conv.py如下:

```
import chardet
import sys
import os

if __name__ == '__main__':
    if len(sys.argv) == 2:
        for root, dirs, files in os.walk(sys.argv[1]):
            for file in files:
                file_path = root + "/" + file
                f = open(file_path,'r')
                data = f.read()
                f.close()
                encoding = chardet.detect(data)["encoding"]
                if encoding not in ("UTF-8-SIG", "UTF-16LE", "utf-8", "ascii"):
                    try:
                        gb_content = data.decode("gb18030")
                        gb_content.encode('utf-8')
                        f = open(file_path, 'w')
                        f.write(gb_content.encode('utf-8'))
                        f.close()
                    except:
                        print "except:", file_path
```

### 第十步：筛选中文

考虑到我朝广大人民的爱国热情，我只做中文，所以什么英文、韩文、日文、俄文、火星文、鸟语……全都不要，参考extract_sentence_srt.py如下：

```
# coding:utf-8
import chardet
import os
import re

cn=ur"([\u4e00-\u9fa5]+)"
pattern_cn = re.compile(cn)
jp1=ur"([\u3040-\u309F]+)"
pattern_jp1 = re.compile(jp1)
jp2=ur"([\u30A0-\u30FF]+)"
pattern_jp2 = re.compile(jp2)

for root, dirs, files in os.walk("./srt"):
    file_count = len(files)
    if file_count > 0:
        for index, file in enumerate(files):
            f = open(root + "/" + file, "r")
            content = f.read()
            f.close()
            encoding = chardet.detect(content)["encoding"]
            try:
                for sentence in content.decode(encoding).split('\n'):
                    if len(sentence) > 0:
                        match_cn =  pattern_cn.findall(sentence)
                        match_jp1 =  pattern_jp1.findall(sentence)
                        match_jp2 =  pattern_jp2.findall(sentence)
                        sentence = sentence.strip()
                        if len(match_cn)>0 and len(match_jp1)==0 and len(match_jp2) == 0 and len(sentence)>1 and len(sentence.split(' ')) < 10:
                            print sentence.encode('utf-8')
            except:
                continue
```

### 第十一步：字幕中的句子提取

不同格式的字幕有特定的格式，除了句子之外还有很多字幕的控制语句，我们一律过滤掉，只提取我们想要的重点内容，因为不同的格式都不一样，在这里不一一举例了，感兴趣可以去我的github查看，在这里单独列出ssa格式字幕的部分代码供参考：

```
if line.find('Dialogue') == 0 and len(line) < 500:
    fields = line.split(',')
    sentence = fields[len(fields)-1]
    tag_fields = sentence.split('}')
    if len(tag_fields) > 1:
        sentence = tag_fields[len(tag_fields)-1]
```

### 第十二步：内容过滤

经过上面几步的处理，其实已经形成了完整的语料库了，只是里面还有一些不像聊天的内容我们需要进一步做优化，包括：过滤特殊的unicode字符、过滤特殊的关键词（如：字幕、时间轴、校对……）、去除字幕样式标签、去除html标签、去除连续特殊字符、去除转义字符、去除剧集信息等，具体代码如下：

```
# coding:utf-8
import sys
import re
import chardet

if __name__ == '__main__':
    #illegal=ur"([\u2000-\u2010]+)"
    illegal=ur"([\u0000-\u2010]+)"
    pattern_illegals = [re.compile(ur"([\u2000-\u2010]+)"), re.compile(ur"([\u0090-\u0099]+)")]
    filters = ["字幕", "时间轴:", "校对:", "翻译:", "后期:", "监制:"]
    filters.append("时间轴：")
    filters.append("校对：")
    filters.append("翻译：")
    filters.append("后期：")
    filters.append("监制：")
    filters.append("禁止用作任何商业盈利行为")
    filters.append("http")
    htmltagregex = re.compile(r'<[^>]+>',re.S)
    brace_regex = re.compile(r'\{.*\}',re.S)
    slash_regex = re.compile(r'\\\w',re.S)
    repeat_regex = re.compile(r'[-=]{10}',re.S)
    f = open("./corpus/all.out", "r")
    count=0
    while True:
        line = f.readline()
        if line:
            line = line.strip()

            # 编码识别，不是utf-8就过滤
            gb_content = ''
            try:
                gb_content = line.decode("utf-8")
            except Exception as e:
                sys.stderr.write("decode error:  ", line)
                continue

            # 中文识别，不是中文就过滤
            need_continue = False
            for pattern_illegal in pattern_illegals:
                match_illegal = pattern_illegal.findall(gb_content)
                if len(match_illegal) > 0:
                    sys.stderr.write("match_illegal error: %s\n" % line)
                    need_continue = True
                    break
            if need_continue:
                continue

            # 关键词过滤
            need_continue = False
            for filter in filters:
                try:
                    line.index(filter)
                    sys.stderr.write("filter keyword of %s %s\n" % (filter, line))
                    need_continue = True
                    break
                except:
                    pass
            if need_continue:
                continue

            # 去掉剧集信息
            if re.match('.*第.*季.*', line):
                sys.stderr.write("filter copora %s\n" % line)
                continue
            if re.match('.*第.*集.*', line):
                sys.stderr.write("filter copora %s\n" % line)
                continue
            if re.match('.*第.*帧.*', line):
                sys.stderr.write("filter copora %s\n" % line)
                continue

            # 去html标签
            line = htmltagregex.sub('',line)

            # 去花括号修饰
            line = brace_regex.sub('', line)

            # 去转义
            line = slash_regex.sub('', line)

            # 去重复
            new_line = repeat_regex.sub('', line)
            if len(new_line) != len(line):
                continue

            # 去特殊字符
            line = line.replace('-', '').strip()

            if len(line) > 0:
                sys.stdout.write("%s\n" % line)
            count+=1
        else:
            break
    f.close()
    pass
```

数据样例如下：

```
这是什么
是寄给医院的
井崎…为什么？
是为了小雪的事情
怎么回事？
您不记得了吗
在她说小雪…
就是在这种非常时期和我们一起舍弃休息时间来工作的护士失踪时…
医生 小雪她失踪了
你不是回了一句「是吗」吗
是吗…
不 对不起
跟我道歉也没用啊
而且我们都知道您是因为夫人的事情而操劳
但是 我想小聪是受不了医生一副漠不关心的样子
事到如今再责备医生也没有用了
是我的错吗…
我就是这个意思 您听不出来吗
我也难以接受
……
```

## 第一版聊天机器人诞生——吃了字幕长大的小二兔

在上一节中我分享了建设好的影视剧字幕聊天语料库，本节基于这个语料库开发我们的聊天机器人，因为是第一版，所以机器人的思绪还有点小乱，答非所问、驴唇不对马嘴得比较搞笑，大家凑合玩 

### 第一版思路

首先要考虑到我的影视剧字幕聊天语料库特点，它是把影视剧里面的说话内容一句一句以回车换行罗列的三千多万条中国话，那么相邻的第二句其实就很可能是第一句的最好的回答，另外，如果对于一个问句有很多种回答，那么我们可以根据相关程度以及历史聊天记录来把所有回答排个序，找到最优的那个，这么说来这是一个搜索和排序的过程。对！没错！我们可以借助搜索技术来做第一版。

### lucene+ik

lucene是一款开源免费的搜索引擎库，java语言开发。ik全称是IKAnalyzer，是一个开源中文切词工具。我们可以利用这两个工具来对语料库做切词建索引，并通过文本搜索的方式做文本相关性检索，然后把下一句取出来作为答案候选集，然后再通过各种方式做答案排序，当然这个排序是很有学问的，聊天机器人有没有智能一半程度上体现在了这里(还有一半体现在对问题的分析上)，本节我们的主要目的是打通这一套机制，至于“智能”这件事我们以后逐个拆解开来不断研究

### 建索引

首先用eclipse创建一个maven工程，如下：

maven帮我们自动生成了pom.xml文件，这配置了包依赖信息，我们在dependencies标签中添加如下依赖：

```
<dependency>
    <groupId>org.apache.lucene</groupId>
    <artifactId>lucene-core</artifactId>
    <version>4.10.4</version>
</dependency>
<dependency>
    <groupId>org.apache.lucene</groupId>
    <artifactId>lucene-queryparser</artifactId>
    <version>4.10.4</version>
</dependency>
<dependency>
    <groupId>org.apache.lucene</groupId>
    <artifactId>lucene-analyzers-common</artifactId>
    <version>4.10.4</version>
</dependency>
<dependency>
    <groupId>io.netty</groupId>
    <artifactId>netty-all</artifactId>
    <version>5.0.0.Alpha2</version>
</dependency>
<dependency>
    <groupId>com.alibaba</groupId>
    <artifactId>fastjson</artifactId>
    <version>1.1.41</version>
</dependency>
```

并在project标签中增加如下配置，使得依赖的jar包都能自动拷贝到lib目录下：

```
<build>
  <plugins>
    <plugin>
      <groupId>org.apache.maven.plugins</groupId>
      <artifactId>maven-dependency-plugin</artifactId>
      <executions>
        <execution>
          <id>copy-dependencies</id>
          <phase>prepare-package</phase>
          <goals>
            <goal>copy-dependencies</goal>
          </goals>
          <configuration>
            <outputDirectory>${project.build.directory}/lib</outputDirectory>
            <overWriteReleases>false</overWriteReleases>
            <overWriteSnapshots>false</overWriteSnapshots>
            <overWriteIfNewer>true</overWriteIfNewer>
          </configuration>
        </execution>
      </executions>
    </plugin>
    <plugin>
      <groupId>org.apache.maven.plugins</groupId>
      <artifactId>maven-jar-plugin</artifactId>
      <configuration>
        <archive>
          <manifest>
            <addClasspath>true</addClasspath>
            <classpathPrefix>lib/</classpathPrefix>
            <mainClass>theMainClass</mainClass>
          </manifest>
        </archive>
      </configuration>
    </plugin>
  </plugins>
</build>
```

从https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/ik-analyzer/IK%20Analyzer%202012FF_hf1_source.rar下载ik的源代码并把其中的src/org目录拷贝到chatbotv1工程的src/main/java下，然后刷新maven工程，效果如下：

![img](http://upload-images.jianshu.io/upload_images/5952841-dbb7cb8cbec84000.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在com.shareditor.chatbotv1包下maven帮我们自动生成了App.java，为了辨识我们改成Indexer.java，关键代码如下：

```
Analyzer analyzer = new IKAnalyzer(true);
IndexWriterConfig iwc = new IndexWriterConfig(Version.LUCENE_4_9, analyzer);
iwc.setOpenMode(OpenMode.CREATE);
iwc.setUseCompoundFile(true);
IndexWriter indexWriter = new IndexWriter(FSDirectory.open(new File(indexPath)), iwc);

BufferedReader br = new BufferedReader(new InputStreamReader(
        new FileInputStream(corpusPath), "UTF-8"));
String line = "";
String last = "";
long lineNum = 0;
while ((line = br.readLine()) != null) {
    line = line.trim();

    if (0 == line.length()) {
        continue;
    }

    if (!last.equals("")) {
        Document doc = new Document();
        doc.add(new TextField("question", last, Store.YES));
        doc.add(new StoredField("answer", line));
        indexWriter.addDocument(doc);
    }
    last = line;
    lineNum++;
    if (lineNum % 100000 == 0) {
        System.out.println("add doc " + lineNum);
    }
}
br.close();

indexWriter.forceMerge(1);
indexWriter.close();
```

编译好后拷贝src/main/resources下的所有文件到target目录下，并在target目录下执行

```
java -cp $CLASSPATH:./lib/:./chatbotv1-0.0.1-SNAPSHOT.jar com.shareditor.chatbotv1.Indexer ../../subtitle/raw_subtitles/subtitle.corpus ./index
```

最终生成的索引目录index通过lukeall-4.9.0.jar查看如下：

![img](http://upload-images.jianshu.io/upload_images/5952841-4c50ba514f6d7967.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 检索服务

基于netty创建一个http服务server，代码共享在https://github.com/warmheartli/ChatBotCourse的chatbotv1目录下，关键代码如下：

```
Analyzer analyzer = new IKAnalyzer(true);
QueryParser qp = new QueryParser(Version.LUCENE_4_9, "question", analyzer);
if (topDocs.totalHits == 0) {
    qp.setDefaultOperator(Operator.AND);
    query = qp.parse(q);
    System.out.println(query.toString());
    indexSearcher.search(query, collector);
    topDocs = collector.topDocs();
}

if (topDocs.totalHits == 0) {
    qp.setDefaultOperator(Operator.OR);
    query = qp.parse(q);
    System.out.println(query.toString());
    indexSearcher.search(query, collector);
    topDocs = collector.topDocs();
}


ret.put("total", topDocs.totalHits);
ret.put("q", q);
JSONArray result = new JSONArray();
for (ScoreDoc d : topDocs.scoreDocs) {
    Document doc = indexSearcher.doc(d.doc);
    String question = doc.get("question");
    String answer = doc.get("answer");
    JSONObject item = new JSONObject();
    item.put("question", question);
    item.put("answer", answer);
    item.put("score", d.score);
    item.put("doc", d.doc);
    result.add(item);
}
ret.put("result", result);
```

其实就是查询建好的索引，通过query词做切词拼lucene query，然后检索索引的question字段，匹配上的返回answer字段的值作为候选集，使用时挑出候选集里的一条作为答案

这个server可以通过http访问，如http://127.0.0.1:8765/?q=hello（注意：如果是中文需要转成urlcode发送，因为java端读取时按照urlcode解析），server的启动方法是：

```
java -cp $CLASSPATH:./lib/:./chatbotv1-0.0.1-SNAPSHOT.jar com.shareditor.chatbotv1.Searcher
```

### 聊天界面

先看下我们的界面是什么样的，然后再说怎么做的

首先需要有一个可以展示聊天内容的框框，我们选择ckeditor，因为它支持html格式内容的展示，然后就是一个输入框和发送按钮，html代码如下：

```
<div class="col-sm-4 col-xs-10">
    <div class="row">
        <textarea id="chatarea">
            <div style='color: blue; text-align: left; padding: 5px;'>机器人: 喂，大哥您好，您终于肯跟我聊天了,来侃侃呗,我来者不拒!</div>
            <div style='color: blue; text-align: left; padding: 5px;'>机器人: 啥?你问我怎么这么聪明会聊天?因为我刚刚吃了一堆影视剧字幕!</div>
        </textarea>
    </div>
    <br />

    <div class="row">
        <div class="input-group">
            <input type="text" id="input" class="form-control" autofocus="autofocus" onkeydown="submitByEnter()" />
            <span class="input-group-btn">
            <button class="btn btn-default" type="button" onclick="submit()">发送</button>
          </span>
        </div>
    </div>
</div>



<script type="text/javascript">

        CKEDITOR.replace('chatarea',
                {
                    readOnly: true,
                    toolbar: ['Source'],
                    height: 500,
                    removePlugins: 'elementspath',
                    resize_enabled: false,
                    allowedContent: true
                });
   
</script>
```

为了调用上面的聊天server，需要实现一个发送请求获取结果的控制器，如下：

```
public function queryAction(Request $request)
{
    $q = $request->get('input');
    $opts = array(
        'http'=>array(
            'method'=>"GET",
            'timeout'=>60,
        )
    );
    $context = stream_context_create($opts);
    $clientIp = $request->getClientIp();
    $response = file_get_contents('http://127.0.0.1:8765/?q=' . urlencode($q) . '&clientIp=' . $clientIp, false, $context);
    $res = json_decode($response, true);
    $total = $res['total'];
    $result = '';
    if ($total > 0) {
        $result = $res['result'][0]['answer'];
    }
    return new Response($result);
}
```

这个控制器的路由配置为：

```
chatbot_query:
    path:     /chatbot/query
    defaults: { _controller: AppBundle:ChatBot:query }
```

因为聊天server响应时间比较长，为了不导致web界面卡住，我们在执行submit的时候异步发请求和收结果，如下：

```
    var xmlHttp;
    function submit() {
        if (window.ActiveXObject) {
            xmlHttp = new ActiveXObject("Microsoft.XMLHTTP");
        }
        else if (window.XMLHttpRequest) {
            xmlHttp = new XMLHttpRequest();
        }
        var input = $("#input").val().trim();
        if (input == '') {
            jQuery('#input').val('');
            return;
        }
        addText(input, false);
        jQuery('#input').val('');
        var datastr = "input=" + input;
        datastr = encodeURI(datastr);
        var url = "/chatbot/query";
        xmlHttp.open("POST", url, true);
        xmlHttp.onreadystatechange = callback;
        xmlHttp.setRequestHeader("Content-type", "application/x-www-form-urlencoded");
        xmlHttp.send(datastr);
    }

    function callback() {
        if (xmlHttp.readyState == 4 && xmlHttp.status == 200) {
            var responseText = xmlHttp.responseText;
            addText(responseText, true);
        }
    }
```

这里的addText是往ckeditor里添加一段文本，方法如下：

```
function addText(text, is_response) {
    var oldText = CKEDITOR.instances.chatarea.getData();
    var prefix = '';
    if (is_response) {
        prefix = "<div style='color: blue; text-align: left; padding: 5px;'>机器人: "
    } else {
        prefix = "<div style='color: darkgreen; text-align: right; padding: 5px;'>我: "
    }
    CKEDITOR.instances.chatarea.setData(oldText + "" + prefix + text + "</div>");
}
```

以上所有代码全都共享在 https://github.com/warmheartli/ChatBotCourse 和 https://github.com/warmheartli/shareditor.com 中供参考
