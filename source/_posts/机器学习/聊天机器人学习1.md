---
title: 聊天机器人学习1
date: 2017-09-14 16:04:36
tags: chatbot
categories: 机器学习
---

[原文](http://www.shareditor.com/bloglistbytag/?tagname=%E8%87%AA%E5%B7%B1%E5%8A%A8%E6%89%8B%E5%81%9A%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA)

<!-- more -->

## 涉及知识

人工智能一直以来是人类的梦想，造一台可以为你做一切事情并且有情感的机器人，像哆啦A梦一样，现在这已经不是一个梦了：iPhone里会说话的siri、会下棋的阿法狗、小度机器人、大白......，他们都能够具有智能，和人类交互，帮人类解决问题，这听起来非常神奇，实际上我们自己也可以做一个这样的机器人，从今天开始分享我将我学习和制作的过程 
  
智能机器人可以做到的事情可以很复杂：文字、语音、视频识别与合成；自然语言理解、人机对话；以及驱动硬件设备形成的“机器”人。作为一个只有技术和时间而没有金钱的IT人士，我仅做自然语言和人工智能相关的内容，不涉及硬件，也不涉及不擅长的多媒体识别和合成。所以索性就做一个可以和你说话，帮你解决问题的聊天机器人吧。

聊天机器人涉及到的知识主要是自然语言处理，当然这包括了：语言分析和理解、语言生成、机器学习、人机对话、信息检索、信息传输与信息存储、文本分类、自动文摘、数学方法、语言资源、系统评测等内容，同时少不了的是支撑着一切的编程技术

在我的桌上摆了很多有关自然语言处理、机器学习、深度学习、数学等方面的书籍，为了和大家分享我的经历、学到的知识和每一阶段的成果，我每天会花两个小时以上时间或翻书或总结或编码或整理或写文章，或许文章几天才能更新一篇，但我希望每一篇都是有价值的，或许文章里的知识讲解的不是非常深入，但我希望可以为你指明方向，对于晦涩难懂的内容，我尽量用简朴幽默的方式说出来，目的就是让每一位读者都能有收获，并朝着我们的目标一起前进。

## 初识NLTK库

### 安装和使用

安装

```
pip install nltk
```

下载数据

```python
import nltk
nltk.download()
```

选择book下载，下载较慢，推荐找网络资源。

使用

```from
from nltk.book import *
```

你会看到可以正常加载书籍如下：

```
*** Introductory Examples for the NLTK Book ***
Loading text1, ..., text9 and sent1, ..., sent9
Type the name of the text or sentence to view it.
Type: 'texts()' or 'sents()' to list the materials.
text1: Moby Dick by Herman Melville 1851
text2: Sense and Sensibility by Jane Austen 1811
text3: The Book of Genesis
text4: Inaugural Address Corpus
text5: Chat Corpus
text6: Monty Python and the Holy Grail
text7: Wall Street Journal
text8: Personals Corpus
text9: The Man Who Was Thursday by G . K . Chesterton 1908
```

这里面的text*都是一个一个的书籍节点，直接输入text1会输出书籍标题：

```
text1
```

```
<Text: Moby Dick by Herman Melville 1851>
```

### 搜索文本

执行

```python
text1.concordance("former")
```

会显示20个包含former的语句上下文

```
Displaying 20 of 20 matches:
s of the sea , appeared . Among the former , one was of a most monstrous size 
ce , borrowed from the chaplain ' s former sea - farings . Between the marble 
s him with a fresh lance , when the former one has been badly twisted , or elb
 , though smaller than those of the former order , nevertheless retain a propo
fficial is still retained , but his former dignity is sadly abridged . At pres
 tested reality of his might had in former legendary times thrown its shadow b
g associated with the experience of former perils ; for what knows he , this N
ns and places in which , on various former voyages of various ships , sperm wh
. So that though Moby Dick had in a former year been seen , for example , on w
ed by the defection of seven of his former associates , and stung by the mocki
no part in the mutiny , he told the former that he had a good mind to flog the
 so for ever got the start of their former captain , had he been at all minded
 head is cut off whole , but in the former the lips and tongue are separately 
nd the right . While the ear of the former has an external opening , that of t
in small detached companies , as in former times , are now frequently met with
ence on the coast of Greenland , in former times , of a Dutch village called S
x months before he wheeled out of a former equinox at Aries ! From storm to st
Sperm Whale , for example , that in former years ( the latter part of the last
les no longer haunt many grounds in former years abounding with them , hence t
ering was but the direct issue of a former woe ; and he too plainly seemed to 
```

我们还可以搜索相关词，比如：

```python
text1.similar("ship")
```

```
whale boat sea captain world way head time crew man other pequod line
deck body fishery air boats side voyage
```

输入了ship，查找了boat，都是近义词

我们还可以查看某个词在文章里出现的位置：

```python
text4.dispersion_plot(["citizens", "democracy", "freedom", "duties", "America"])
```

### 词统计

len(text1)：返回总字数

set(text1)：返回文本的所有词集合

len(set(text4))：返回文本总词数

text4.count("is")：返回“is”这个词出现的总次数

FreqDist(text1)：统计文章的词频并按从大到小排序存到一个列表里

fdist1 = FreqDist(text1);fdist1.plot(50, cumulative=True)：统计词频，并输出累计图像

纵轴表示累加了横轴里的词之后总词数是多少，这样看来，这些词加起来几乎达到了文章的总词数

fdist1.hapaxes()：返回只出现一次的词

text4.collocations()：频繁的双联词

### 自然语言处理关键点

词意理解：中国队大胜美国队；中国队大败美国队。“胜”、“败”一对反义词，却表达同样的意思：中国赢了，美国输了。这需要机器能够自动分析出谁胜谁负

自动生成语言：自动生成语言基于语言的自动理解，不理解就无法自动生成

机器翻译：现在机器翻译已经很多了，但是还很难达到最佳，比如我们把中文翻译成英文，再翻译成中文，再翻译成英文，来回10轮，发现和最初差别还是非常大的。

人机对话：这也是我们想做到的最终目标，这里有一个叫做“图灵测试”的方式，也就是在5分钟之内回答提出问题的30%即通过，能通过则认为有智能了。

自然语言处理分两派，一派是基于规则的，也就是完全从语法句法等出发，按照语言的规则来分析和处理，这在上个世纪经历了很多年的试验宣告失败，因为规则太多太多，而且很多语言都不按套路出牌，想象你追赶你的影子，你跑的快他跑的更快，你永远都追不上它。另一派是基于统计的，也就是收集大量的语料数据，通过统计学习的方式来理解语言，这在当代越来越受重视而且已经成为趋势，因为随着硬件技术的发展，大数据存储和计算已经不是问题，无论有什么样的规则，语言都是有统计规律的，当然基于统计也存在缺陷，那就是“小概率事件总是不会发生的”导致总有一些问题解决不了。

下一节我们就基于统计的方案来解决语料的问题。

## 语料与词汇资源

当代自然语言处理都是基于统计的，统计自然需要很多样本，因此语料和词汇资源是必不可少的，本节介绍语料和词汇资源的重要性和获取方式 

### NLTK语料库

NLTK包含多种语料库，举一个例子：Gutenberg语料库，执行：

```python
import nltk
nltk.corpus.gutenberg.fileids()
```

返回Gutenberg语料库的文件标识符

```
[u'austen-emma.txt', u'austen-persuasion.txt', u'austen-sense.txt', u'bible-kjv.txt', u'blake-poems.txt', u'bryant-stories.txt', u'burgess-busterbrown.txt', u'carroll-alice.txt', u'chesterton-ball.txt', u'chesterton-brown.txt', u'chesterton-thursday.txt', u'edgeworth-parents.txt', u'melville-moby_dick.txt', u'milton-paradise.txt', u'shakespeare-caesar.txt', u'shakespeare-hamlet.txt', u'shakespeare-macbeth.txt', u'whitman-leaves.txt']
```

`nltk.corpus.gutenberg`就是gutenberg语料库的阅读器，它有很多实用的方法，比如：

`nltk.corpus.gutenberg.raw('chesterton-brown.txt')`：输出chesterton-brown.txt文章的原始内容

`nltk.corpus.gutenberg.words('chesterton-brown.txt')`：输出chesterton-brown.txt文章的单词列表

`nltk.corpus.gutenberg.sents('chesterton-brown.txt')`：输出chesterton-brown.txt文章的句子列表

类似的语料库还有：

`from nltk.corpus import webtext`：网络文本语料库，网络和聊天文本

`from nltk.corpus import brown`：布朗语料库，按照文本分类好的500个不同来源的文本

`from nltk.corpus import reuters`：路透社语料库，1万多个新闻文档

`from nltk.corpus import inaugural`：就职演说语料库，55个总统的演说

### 语料库的一般结构

以上各种语料库都是分别建立的，因此会稍有一些区别，但是不外乎以下几种组织结构：散养式（孤立的多篇文章）、分类式（按照类别组织，相互之间没有交集）、交叉式（一篇文章可能属于多个类）、渐变式（语法随着时间发生变化）

语料库的通用接口

`fileids()`：返回语料库中的文件

`categories()`：返回语料库中的分类

`raw()`：返回语料库的原始内容

`words()`：返回语料库中的词汇

`sents()`：返回语料库句子

`abspath()`：指定文件在磁盘上的位置

`open()`：打开语料库的文件流

### 加载自己的语料库

收集自己的语料文件（文本文件）到某路径下（比如/tmp)，然后执行：

```
from nltk.corpus import PlaintextCorpusReader
corpus_root = '/tmp'
wordlists = PlaintextCorpusReader(corpus_root, '.*')
wordlists.fileids()
```

就可以列出自己语料库的各个文件了，也可以使用如`wordlists.sents('a.txt')`和`wordlists.words('a.txt')`等方法来获取句子和词信息

### 条件频率分布

条件分布大家都比较熟悉了，就是在一定条件下某个事件的概率分布。自然语言的条件频率分布就是指定条件下某个事件的频率分布。

比如要输出在布朗语料库中每个类别条件下每个词的概率：

```
# coding:utf-8

import sys
reload(sys)
sys.setdefaultencoding( "utf-8" )

import nltk
from nltk.corpus import brown

# 链表推导式，genre是brown语料库里的所有类别列表，word是这个类别中的词汇列表
# (genre, word)就是类别加词汇对
genre_word = [(genre, word)
        for genre in brown.categories()
        for word in brown.words(categories=genre)
        ]

# 创建条件频率分布
cfd = nltk.ConditionalFreqDist(genre_word)

# 指定条件和样本作图
cfd.plot(conditions=['news','adventure'], samples=[u'stock', u'sunbonnet', u'Elevated', u'narcotic', u'four', u'woods', u'railing', u'Until', u'aggression', u'marching', u'looking', u'eligible', u'electricity', u'$25-a-plate', u'consulate', u'Casey', u'all-county', u'Belgians', u'Western', u'1959-60', u'Duhagon', u'sinking', u'1,119', u'co-operation', u'Famed', u'regional', u'Charitable', u'appropriation', u'yellow', u'uncertain', u'Heights', u'bringing', u'prize', u'Loen', u'Publique', u'wooden', u'Loeb', u'963', u'specialties', u'Sands', u'succession', u'Paul', u'Phyfe'])
```

注意：这里如果把plot直接换成tabulate ，那么就是输出表格形式，和图像表达的意思相同

我们还可以利用条件频率分布，按照最大条件概率生成双连词，最终生成一个随机文本

这可以直接使用bigrams()函数，它的功能是生成词对链表。

创建python文件如下：

```
# coding:utf-8

import sys
reload(sys)
sys.setdefaultencoding( "utf-8" )

import nltk

# 循环10次，从cfdist中取当前单词最大概率的连词,并打印出来
def generate_model(cfdist, word, num=10):
    for i in range(num):
        print word,
        word = cfdist[word].max()

# 加载语料库
text = nltk.corpus.genesis.words('english-kjv.txt')

# 生成双连词
bigrams = nltk.bigrams(text)

# 生成条件频率分布
cfd = nltk.ConditionalFreqDist(bigrams)

# 以the开头，生成随机串
generate_model(cfd, 'the')
```

执行效果如下：

```
the land of the land of the land of the
```

the的最大概率的双连词是land，land最大概率双连词是of，of最大概率双连词是the，所以后面就循环了

 ### 其他词典资源

有一些仅是词或短语以及一些相关信息的集合，叫做词典资源。

词汇列表语料库：`nltk.corpus.words.words()`，所有英文单词，这个可以用来识别语法错误

停用词语料库：`nltk.corpus.stopwords.words`，用来识别那些最频繁出现的没有意义的词

发音词典：`nltk.corpus.cmudict.dict()`，用来输出每个英文单词的发音

比较词表：`nltk.corpus.swadesh`，多种语言核心200多个词的对照，可以作为语言翻译的基础

同义词集：`WordNet`，面向语义的英语词典，由同义词集组成，并组织成一个网络

## 何须动手？完全自动化对语料做词性标注

全人工对语料做词性标注就像蚂蚁一样忙忙碌碌，是非常耗费声明的，如果有一个机器能够完全自动化地，给它一篇语料，它迅速给你一片标注，这样才甚好，本节就来讨论一下怎么样能无需动手对语料做自动化的词性标注 

英文词干提取器

```python
import nltk
porter = nltk.PorterStemmer()
print porter.stem('lying')
```

输出`lie`

词性标注器

```python
import nltk
text = nltk.word_tokenize("And now for something completely different")
print nltk.pos_tag(text)
```

```
[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ')]
```

其中CC是连接词，RB是副词，IN是介词，NN是名次，JJ是形容词

这是一句完整的话，实际上pos_tag是处理一个词序列，会根据句子来动态判断，比如：

```python
print nltk.pos_tag(['i','love','you'])
```

```
[('i', 'NN'), ('love', 'VBP'), ('you', 'PRP')]`
```

这里的love识别为动词

而：

```
print nltk.pos_tag(['love','and','hate'])
```

```
[('love', 'NN'), ('and', 'CC'), ('hate', 'NN')]
```

这里的love识别为名词

nltk中多数都是英文的词性标注语料库，如果我们想自己标注一批语料库该怎么办呢？

nltk提供了比较方便的方法：

```python
tagged_token = nltk.tag.str2tuple('fly/NN')
print tagged_token
```

```
('fly', 'NN')
```

这里的`nltk.tag.str2tuple`可以把`fly/NN`这种字符串转成一个二元组，事实上nltk的语料库中都是这种字符串形式的标注，那么我们如果把语料库标记成：

```python
sent = '我/NN 是/IN 一个/AT 大/JJ 傻×/NN'
print [nltk.tag.str2tuple(t) for t in sent.split()]
```

```
[('\xe6\x88\x91', 'NN'), ('\xe6\x98\xaf', 'IN'), ('\xe4\xb8\x80\xe4\xb8\xaa', 'AT'), ('\xe5\xa4\xa7', 'JJ'), ('\xe5\x82\xbb\xc3\x97', 'NN')]
```

这么说来，中文也是可以支持的，恩~

我们来看一下布朗语料库中的标注：

```python
nltk.corpus.brown.tagged_words()
```

```
[(u'The', u'AT'), (u'Fulton', u'NP-TL'), ...]
```

事实上nltk也有中文的语料库，我们来下载下来：

执行`nltk.download()`，选择Corpora里的sinica_treebank下载

sinica就是台湾话中的中国研究院

我们看一下这个中文语料库里有什么内容，创建`cn_tag.py`，内容如下：

```python
# coding:utf-8

import sys
reload(sys)
sys.setdefaultencoding( "utf-8" )
import nltk

for word in nltk.corpus.sinica_treebank.tagged_words():
    print word[0], word[1]
```

执行后输出：

```
一 Neu
友情 Nad
嘉珍 Nba
和 Caa
我 Nhaa
住在 VC1
同一條 DM
巷子 Nab
我們 Nhaa
是 V_11
……
```

第一列是中文的词汇，第二列是标注好的词性

我们发现这里面都是繁体，因为是基于台湾的语料生成的，想要简体中文还得自己想办法。不过有人已经帮我们做了这部分工作，那就是jieba切词， https://github.com/fxsjy/jieba ，强烈推荐，可以自己加载自己的语料，进行中文切词，并且能够自动做词性标注

### 词性自动标注

面对一片新的语料库(比如我们从未处理过中文，只有一批批的中文语料，现在让我们做词性自动标注)，如何实现词性自动标注？有如下几种标注方法：

默认标注器：不管什么词，都标注为频率最高的一种词性。比如经过分析，所有中文语料里的词是名次的概率是13%最大，那么我们的默认标注器就全部标注为名次。这种标注器一般作为其他标注器处理之后的最后一道门，即：不知道是什么词？那么他是名次。默认标注器用DefaultTagger来实现，具体用法如下：

```python
# coding:utf-8

import sys
reload(sys)
sys.setdefaultencoding( "utf-8" )
import nltk

default_tagger = nltk.DefaultTagger('NN')
raw = '我 累 个 去'
tokens = nltk.word_tokenize(raw)
tags = default_tagger.tag(tokens)
print tags
```

```
[('\xe6\x88\x91', 'NN'), ('\xe7\xb4\xaf', 'NN'), ('\xe4\xb8\xaa', 'NN'), ('\xe5\x8e', 'NN'), ('\xbb', 'NN')]
```

正则表达式标注器：满足特定正则表达式的认为是某种词性，比如凡是带“们”的都认为是代词(PRO)。正则表达式标注器通RegexpTagge实现，用法如下：

```python
pattern = [(r'.*们$','PRO')]
tagger = nltk.RegexpTagger(pattern)
print tagger.tag(nltk.word_tokenize('我们 累 个 去 你们 和 他们 啊'))
```

```
[('\xe6\x88\x91\xe4', None), ('\xbb', None), ('\xac', None), ('\xe7\xb4\xaf', None), ('\xe4\xb8\xaa', None), ('\xe5\x8e', None), ('\xbb', None), ('\xe4\xbd\xa0\xe4', None), ('\xbb', None), ('\xac', None), ('\xe5\x92\x8c', None), ('\xe4', None), ('\xbb', None), ('\x96\xe4', None), ('\xbb', None), ('\xac', None), ('\xe5\x95\x8a', None)]
```

查询标注器：找出最频繁的n个词以及它的词性，然后用这个信息去查找语料库，匹配的就标记上，剩余的词使用默认标注器(回退)。这一般使用一元标注的方式，见下面。

一元标注：基于已经标注的语料库做训练，然后用训练好的模型来标注新的语料，使用方法如下：

```python
import nltk
from nltk.corpus import brown
tagged_sents = [[(u'我', u'PRO'), (u'小兔', u'NN')]]
unigram_tagger = nltk.UnigramTagger(tagged_sents)
sents = brown.sents(categories='news')
sents = [[u'我', u'你', u'小兔']]
tags = unigram_tagger.tag(sents[0])
print tags
```

```
[(u'\u6211', u'PRO'), (u'\u4f60', None), (u'\u5c0f\u5154', u'NN')]
```

这里的tagged_sents是用于训练的语料库，我们也可以直接用已有的标注好的语料库，比如：

```python
brown_tagged_sents = brown.tagged_sents(categories='news')
```

二元标注和多元标注：一元标注指的是只考虑当前这个词，不考虑上下文。二元标注器指的是考虑它前面的词的标注，用法只需要把上面的UnigramTagger换成BigramTagger。同理三元标注换成TrigramTagger

组合标注器：为了提高精度和覆盖率，我们对多种标注器组合，比如组合二元标注器、一元标注器和默认标注器，如下：

```python
t0 = nltk.DefaultTagger('NN')
t1 = nltk.UnigramTagger(train_sents, backoff=t0) 
t2 = nltk.BigramTagger(train_sents, backoff=t1) 
```

标注器的存储：训练好的标注器为了持久化，可以存储到硬盘，具体方法如下：

```python
from cPickle import dump
output = open('t2.pkl', 'wb')
dump(t2, output, -1)
output.close()
```

使用时也可以加载，如下：

```
from cPickle import load
input = open('t2.pkl', 'rb')
tagger = load(input) 
input.close() 
```

## 自然语言处理中的文本分类

文本分类是机器学习在自然语言处理中的最常用也是最基础的应用，机器学习相关内容可以直接看我的有关scikit-learn相关教程，本节直接涉及nltk中的机器学习相关内容 

### 先来一段前戏

机器学习的过程是训练模型和使用模型的过程，训练就是基于已知数据做统计学习，使用就是用统计学习好的模型来计算未知的数据。

机器学习分为有监督学习和无监督学习，文本分类也分为有监督的分类和无监督的分类。有监督就是训练的样本数据有了确定的判断，基于这些已有的判断来断定新的数据，无监督就是训练的样本数据没有什么判断，完全自发的生成结论。

无论监督学习还是无监督学习，都是通过某种算法来实现，而这种算法可以有多重选择，贝叶斯就是其中一种。在多种算法中如何选择最适合的，这才是机器学习最难的事情，也是最高境界。

### nltk中的贝叶斯分类器

贝叶斯是概率论的鼻祖，贝叶斯定理是关于随机事件的条件概率的一则定理，贝叶斯公式是：

P(B|A)=P(A|B)P(B)/P(A)；即,已知P(A|B)，P(A)和P(B)可以计算出P(B|A)。

贝叶斯分类器就是基于贝叶斯概率理论设计的分类器算法，nltk库中已经实现，具体用法如下：

```python
# coding:utf-8

import sys
reload(sys)
sys.setdefaultencoding( "utf-8" )
import nltk

my_train_set = [
        ({'feature1':u'a'},'1'),
        ({'feature1':u'a'},'2'),
        ({'feature1':u'a'},'3'),
        ({'feature1':u'a'},'3'),
        ({'feature1':u'b'},'2'),
        ({'feature1':u'b'},'2'),
        ({'feature1':u'b'},'2'),
        ({'feature1':u'b'},'2'),
        ({'feature1':u'b'},'2'),
        ({'feature1':u'b'},'2'),
        ]
classifier = nltk.NaiveBayesClassifier.train(my_train_set)
print classifier.classify({'feature1':u'a'})
print classifier.classify({'feature1':u'b'})
```

```
3
2
```

执行后判断特征a和特征b的分类分别是3和2

因为训练集中特征是a的分类是3的最多，所以会归类为3

当然实际中训练样本的数量要多的多，特征要多的多

### 文档分类

不管是什么分类，最重要的是要知道哪些特征是最能反映这个分类的特点，也就是特征选取。文档分类使用的特征就是最能代表这个分类的词。

因为对文档分类要经过训练和预测两个过程，而特征的提取是这两个过程都需要的，所以，习惯上我们会把特征提取单独抽象出来作为一个公共方法，比如：

**以下代码有问题***

```python
import nltk
from nltk.corpus import movie_reviews
all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())
all_words.plot(50, cumulative=True)
word_features = all_words.keys()[:2000]
def document_features(document): 
    for word in word_features: 
        features['contains(%s)' % word] = (word in document_words) 
    return features 
```

这是一个简单的特征提取过程，前两行找到movie_reviews语料库中出现词频最高的2000个词作为特征，下面定义的函数就是特征提取函数，每个特征都是形如`contains(***)`的key，value就是True或False，表示这个词是否在文档中出现

那么我们训练的过程就是：

```python
featuresets = [(document_features(d), c) for (d,c) in documents]
classifier = nltk.NaiveBayesClassifier.train(featuresets)
```

要预测一个新的文档时：

```python
classifier.classify(document_features(d))
```

通过

```
classifier.show_most_informative_features(5)
```

可以找到最优信息量的特征，这对我们选取特征是非常有帮助的

### 其他文本分类

文本分类除了文档分类外还有许多其他类型的分类，比如：

词性标注：属于一种文本分类，一般是基于上下文语境的文本分类

句子分割：属于标点符号的分类任务，它的特征一般选取为单独句子标识符的合并链表、数据特征（下一个词是否大写、前一个词是什么、前一个词长度……）

识别对话行为类型：对话行为类型是指问候、问题、回答、断言、说明等

识别文字蕴含：即一个句子是否能得出另外一个句子的结论，这可以认为是真假标签的分类任务。这是一个有挑战的事情

## 教你怎么从一句话里提取出十句话的信息

按照之前理解的内容，对一句话做处理，最多是切成一个一个的词，再标注上词性，仅此而已，然而事实并非如此，一句话还可以做更多的文章，我们本节见分晓 

### 什么？还能结构化？

任何语言的每一句话之所以称为“话”，是因为它有一定的句子结构，除了一个个独立的词之外，他们之间还存在着某种关系。如果任何一句话可以由任何词构成，可长可短，那么这是一个非结构化的信息，计算机是很难理解并做计算的，但是如果能够以某种方式把句子转化成结构化的形式，计算机就可以理解了。

实事上，人脑在理解一句话的时候也暗暗地在做着由非结构化到结构化的工作。

比如说：“我下午要和小明在公司讨论一个技术问题”。这是一片非结构化的词语拼成的一句话，但是这里面有很多隐含信息：

1）小明是一个实体

2）参与者有两个：我和小明

3）地点设定是：公司

4）要做的事情是：讨论

5）讨论的内容是：问题

6）这个问题是一个技术问题

7）公司是一个地点

8）讨论是一种行为

9）我和小明有某种关系

10）下午是一个时间

上面这些信息有一些是专门针对这个句子的，有一些是常理性的，对于针对句子的信息有利于理解这句话，对于常理性的信息可以积累下来用来以后理解其他句子。

那么怎么才能把非结构化的句子转成结构化的信息呢？要做的工作除了断句、分词、词性标注之外，还要做的一个关键事情就是分块。

### 分块

分块就是根据句子中的词和词性，按照某种规则组合在一起形成一个个分块，每个分块代表一个实体。常见的实体包括：组织、人员、地点、日期、时间等

以上面的例子为例，首先我们做名词短语分块（NP-chunking），比如：技术问题。名词短语分块通过词性标记和一些规则就可以识别出来，也可以通过机器学习的方法识别

除了名词短语分块还有很多其他分块：介词短语（PP，比如：以我……）、动词短语（VP，比如：打人）、句子（S，我是人）

### 分块如何标记和存储呢？

可以采用IOB标记，I(inside，内部)、O(outside，外部)、B(begin, 开始)，一个块的开始标记为B，块内的标识符序列标注为I，所有其他标识符标注为O

也可以用树结构来存储分块，用树结构可以解决IOB无法标注的另一类分块，那就是多级分块。多级分块就是一句话可以有多重分块方法，比如：我以我的最高权利惩罚你。这里面“最高权利”、“我的最高权利”、“以我的最高权利”是不同类型分块形成一种多级分块，这是无法通过IOB标记的，但是用树结构可以。这也叫做级联分块。具体树结构举个例子：

```
(S
    (NP 小明) 
    (VP
        (V 追赶) 
        (NP
            (Det 一只) 
            (N 兔子)))) 
```

这是不是让你想到了语法树？

### 关系抽取

通过上面的分块可以很容易识别出实体，那么关系抽取实际就是找出实体和实体之间的关系，这是自然语言处理一个质的跨越，实体识别让机器认知了一种事物，关系识别让机器掌握了一个真相。

关系抽取的第一个方法就是找到(X, a, Y)这种三元组，其中X和Y都是实体，a是表达关系的字符串，这完全可以通过正则来识别，因为不同语言有这不同的语法规则，所以方法都是不同的，比如中文里的“爱”可以作为这里的a，但是“和”、“因为”等就不能作为这里的a

### 编程实现

下面介绍部分有关分块的代码，因为中文标注好分块的语料没有找到，所以只能沿用英文语料来说明，但是原理是一样的

conll2000语料中已经有标注好的分块信息，如下：

```python
from nltk.corpus import conll2000
print conll2000.chunked_sents('train.txt')[99]
```

```
(S
  (PP Over/IN)
  (NP a/DT cup/NN)
  (PP of/IN)
  (NP coffee/NN)
  ,/,
  (NP Mr./NNP Stone/NNP)
  (VP told/VBD)
  (NP his/PRP$ story/NN)
  ./.)
```

我们可以基于这些标注数据做训练，由于这种存储结构比较特殊，所以就不单独基于这种结构实现parser了，只说下跟前面讲的机器学习一样，只要基于这部分数据做训练，然后再用来标注新的语料就行了

## 文法分析还是基于特征好啊

语法分析固然重要，但要想覆盖语言的全部，需要进一步扩展到文法分析，文法分析可以基于规则，但是工作量难以想象，基于特征的文法分析不但可穷举，而且可以方便用计算机存储和计算，本节简单做一个介绍，更深层次的内容还需要继续关注后面的系列文章

### 语法和文法

还记得上一节中的这个吗？

```
(S
    (NP 小明) 
    (VP
        (V 追赶) 
        (NP
            (Det 一只) 
            (N 兔子)))) 
```

这里面的N表示名词，Det表示限定词，NP表示名词短语，V表示动词，VP表示动词短语，S表示句子

这种句子分析方法叫做语法分析

因为句子可以无限组合无限扩展，所以单纯用语法分析来完成自然语言处理这件事情是不可能的，所以出现了文法分析

文法是一个潜在的无限的句子集合的一个紧凑的特性，它是通过一组形式化模型来表示的，文法可以覆盖所有结构的句子，对一个句子做文法分析，就是把句子往文法模型上靠，如果同时符合多种文法，那就是有歧义的句子

最重要的结论：文法结构范围相当广泛，无法用规则类的方法来处理，只有利用基于特征的方法才能处理

### 文法特征结构

文法特征举例：单词最后一个字母、词性标签、文法类别、正字拼写、指示物、关系、施事角色、受事角色

因为文法特征是一种kv，所以特征结构的存储形式是字典

不是什么样的句子都能提取出每一个文法特征的，需要满足一定的条件，这需要通过一系列的检查手段来达到，包括：句法协议（比如this dog就是对的，而these dog就是错的）、属性和约束、术语

### 特征结构的处理

nltk帮我实现了特征结构：

```python
import nltk
fs1 = nltk.FeatStruct(TENSE='past', NUM='sg')
print fs1
fs2 = nltk.FeatStruct(POS='N', AGR=fs1)
print fs2
```

```
[ NUM   = 'sg'   ]
[ TENSE = 'past' ]

[ AGR = [ NUM   = 'sg'   ] ]
[       [ TENSE = 'past' ] ]
[                          ]
[ POS = 'N'                ]
```

在nltk的库里已经有了一些产生式文法描述可以直接使用，位置在：

```
ls /usr/share/nltk_data/grammars/book_grammars
```

```
background.fol  discourse.fcfg  drt.fcfg  feat0.fcfg  feat1.fcfg  german.fcfg  simple-sem.fcfg  sql0.fcfg  sql1.fcfg  storage.fcfg
```

我们看其中最简单的一个sql0.fcfg，这是一个查找国家城市的sql语句的文法：

```
% start S

S[SEM=(?np + WHERE + ?vp)] -> NP[SEM=?np] VP[SEM=?vp]

VP[SEM=(?v + ?pp)] -> IV[SEM=?v] PP[SEM=?pp]
VP[SEM=(?v + ?ap)] -> IV[SEM=?v] AP[SEM=?ap]
NP[SEM=(?det + ?n)] -> Det[SEM=?det] N[SEM=?n]
PP[SEM=(?p + ?np)] -> P[SEM=?p] NP[SEM=?np]
AP[SEM=?pp] -> A[SEM=?a] PP[SEM=?pp]

NP[SEM='Country="greece"'] -> 'Greece'
NP[SEM='Country="china"'] -> 'China'

Det[SEM='SELECT'] -> 'Which' | 'What'

N[SEM='City FROM city_table'] -> 'cities'

IV[SEM=''] -> 'are'
A[SEM=''] -> 'located'
P[SEM=''] -> 'in'
```

### 解释一下

这里面从上到下是从最大范围到最小范围一个个的解释，S是句子

我们来加载这个文法描述，并试验如下：

```python
import nltk
from nltk import load_parser
cp = load_parser('grammars/book_grammars/sql0.fcfg')
query = 'What cities are located in China'
tokens = query.split()
for tree in cp.parse(tokens):
    print tree
```

```
(S[SEM=(SELECT, City FROM city_table, WHERE, , , Country="china")]
  (NP[SEM=(SELECT, City FROM city_table)]
    (Det[SEM='SELECT'] What)
    (N[SEM='City FROM city_table'] cities))
  (VP[SEM=(, , Country="china")]
    (IV[SEM=''] are)
    (AP[SEM=(, Country="china")]
      (A[SEM=''] located)
      (PP[SEM=(, Country="china")]
        (P[SEM=''] in)
        (NP[SEM='Country="china"'] China)))))
```

我们可以看到用特征结构可以建立对大量广泛的语言学现象的简介分析

## 重温自然语言处理

### 自然语言处理怎么学？

先学会倒着学，倒回去看上面那句话：不管三七二十一先用起来，然后再系统地学习

nltk是最经典的自然语言处理的python库，不知道怎么用的看前几篇文章吧，先把它用起来，最起码做出来一个词性标注的小工具

### 自然语言处理学什么？

这门学科的知识可是相当的广泛，广泛到你不需要掌握任何知识就可以直接学，因为你不可能掌握它依赖的全部知识，所以就直接冲过去吧。。。

话说回来，它到底包括哪些知识呢？如果把这些知识比作难关的话，我数一数，整整九九八十一难

第一难：语言学。直接懵逼了吧？语言学啥玩意，怎么说话？还是怎么学说话？其实大家也都说不清语言学是什么东东，但是我知道大家在这方面都在研究啥，有的在研究语言描述的学问，有的在研究语言理论的学问，有的在研究不同语言对比的学问，有的在研究语言共同点上的学问，有的在研究语言发展的历史，有的在研究语言的结构，总之用一个字来形容那是一个涉猎广泛啊

第二难：语音学。再一次懵逼！有人说：我知道！语音学就是怎么发声。赞一个，回答的那是相当不完全对啊！你以为你是学唱歌吗？语音学研究领域分三块：一块是研究声音是怎么发出来的（同学你说对了一点）；一块是研究声音是怎么传递的；一块是研究声音是怎么接收的。这尼玛不是物理吗？怎么还整出语言学来了？其实这是一个交叉学科，交叉了语言学，交叉了生物学

第三难：概率论。啥？怎么到处都是概率论啊？听人说今年又某某某得了诺贝尔经济学奖了，我定睛一看，尼玛，这不是研究的概率论嘛，这也能得经济学奖，真是得数学者得天下啊。废话少说，概率论跟自然语言处理有什么关系？我知道了，说话是一个概率问题，我经常说“尼玛”，那我再次说“尼玛”的概率就高，嗯~沾边了。提到概率论那就少不了这些：贝叶斯老爷爷、马尔可夫大叔……

第四难：信息论。提到信息论肯定第一个想到的是“香浓”啊，有点流口水了，香农老爷爷提出的熵理论影响那是相当巨大啊，没有它估计就没有我们计算机人事什么事了，因为没有他就没有互联网了。还有人说没有图灵就没有计算机了，他咋不说没有他们俩就没有地球了呢？

第五难：机器学习。机器学习是我的最爱，得聊的正式一点，咳咳！机器学习啊——得好好学

第六难：形式语言与自动机。我滴妈啊！我跪了！刚说图灵图灵就来了。说白了，形式语言就是把语言搞的很形式，换句话说就是本来你能懂的东西，搞成你不能懂的东西，那就是形式语言啦！不信？你听：短语结构语言、上下文有关语言、上下文无关语言、正则语言，懂了吗？而自动机呢包括：图灵机、有穷自动机、下推自动机、线性有界自动机。你可能会问了，这么多自动机那得要多少汽油啊？该死的翻译怎么就把这么高大上的英文给翻译成这么晦涩呢，自动机英文叫automata，表达的是自动形成一些信息，也就是说根据前面能自动判断出后面。形式语言用在自然语言处理上我理解，都有语言俩字，可是这自动机有什么用呢？这您还真问着了，您见过拼写检查吗？这就是用的自动机，用处杠杠的！

第七难：语言知识库。你见过科幻电影里的机器人手捧着电话线就能知道一切的镜头吧，互联网上有无数文本内容，用我们抽象的话说那都是知识，但是简单放在电脑里那就是一串字符串，怎么才能让它以知识的形式存储呢？首先得让计算机能分析语言，那么语料就是它学习的基础、是种子，然后有了基础再让它把语言里的知识存储起来，这就形成了语言知识库

第八难：语言模型。模型顾名思义就是“模子”，就是“往上靠”的意思，怎么靠上去更吻合就怎么靠，这就是语言模型。怎么？没懂？好那我用形式化的语言再来说一下：把很多已经整理好的模子放在那里，遇到一个新内容的时候看看属于哪种格式，那就按照这种模子来解释。嗯~你肯定懂了

第九难：分词、实体识别、词性标注。这部分开始纯语言处理了，前几节也简单讲过这部分内容，分词就是让计算机自动区分出汉字组成的词语，因为一个词语一个意思嘛。实体识别就是再分词之后能够根据各种短语形式判断出哪个词表示的是一个物体或组织或人名或……。词性标注就是给你一句话，你能识别出“名动形、数量代、副介连助叹拟声”。

第十难：句法分析。句法分析类似于小学时学的主谓宾补定状的区分，只是要在这基础上组合成短语，也就是把一个非结构化的句子分析称结构化的数据结构

第十一难：语义分析。看起来一步一步越来越深入了。语义是基于句法分析，进一步理解句子的意思，最重要的就是消除歧义，人姑且还会理解出歧义来呢，何况一个机器

第十二难：篇章分析。一堆堆的句子，每个都分析明白了，但是一堆句子组合成的篇章又怎么才能联系起来呢？你得总结出本文的中心思想不是？这他娘的是小学语文里最难的一道题

以上这些内容就是自然语言处理的几大难了，什么？说好的九九八十一难呢？你还不嫌多啊？你还真想变成孙猴子吗？能把这几关过了就不错了！

### 自然语言处理和聊天机器人什么关系？

说到这里，索性就说说下自然语言处理的应用领域吧。

第一个应用：机器翻译。机器翻译方法有很多，我不做，也不说，想学自己看去

第二个应用：语音翻译。跟上一个不是一个吗？不是的，这是语音，那个是机器

第三个应用：文本分类与情感分析。别看两个词，其实是一种事——分类。前面有很多篇文章将文本分类的，可以看看，还有代码噢。

第四个应用：信息检索与问答系统。终于说到重点了，累死我了。这里的问答系统就是我们的聊天机器人。后面会着重讲这个应用，我不断读论文，不断给大家分享哈，别着急，乖！

第五个应用：自动文摘和信息抽取。看过百度搜索里显示的摘要吗？他们多么的精简，而且描述了网页里的中心内容，多漂亮啊！可惜多数都没做到自动文摘。所以这是一个高技术难度的问题。

第六个应用：人机对话。融合了语音识别、口语情感分析、再加上问答系统的全部内容，是自然语言处理的最高境界，离机器人统霸世界不远了。

以及这么多数不完的应用：文本挖掘、舆情分析、隐喻计算、文字编辑和自动校对、作文自动评分、OCR、说话人识别验证……

好！自然语言处理就温习到这里，让我们上阵出发！

## 聊天机器人应该怎么做

聊天机器人到底该怎么做呢？我日思夜想，于是乎我做了一个梦，梦里面我完成了我的聊天机器人，它叫chatbot，经过我的一番盘问，它向我叙述了它的诞生记 

### 聊天机器人是可行的

我：chatbot，你好！

chatbot：你也好！

我：聊天机器人可行吗？

chatbot：你不要怀疑这是天方夜谭，我不就在这里吗？世界上还有很多跟我一样聪明的机器人呢，你听过IBM公司在2010年就研发出来了的Watson问答系统吗？它可比我要聪明100倍呢

我：噢，想起来了，据说Watson在智力竞赛中竟然战胜了人类选手。但是我了解到它有一些缺陷：因为它还只是对信息检索技术的综合运用，并没有进行各种语义关系的深刻计算，所以它能回答的问题也仅限于实事类的问题，所以它能赢得也就是知识类的智力竞赛，如果你给它出个脑筋急转弯，它就不行了

chatbot：是的呢，所以你任重道远啊

### 聊天机器人工作原理是什么

我：chatbot，我问的每一句话，你都是怎么处理并回答我的呢？

chatbot：我身体里有三个重要模块：提问处理模块、检索模块、答案抽取模块。三个模块一起工作，就能回答你的问题啦

我：是嘛，那么这个提问处理模块是怎么工作的呢？

chatbot：提问处理模块要做三项重要工作：查询关键词生成、答案类型确定、句法和语义分析。

我：那么这个查询关。。。

chatbot：别急别急，听我一个一个讲给你听。查询关键词生成，就是从你的提问中提取出关键的几个关键词，因为我本身是一个空壳子，需要去网上查找资料才能回答你，而但网上资料那么多，我该查哪些呢？所以你的提问就有用啦，我找几个中心词，再关联出几个扩展词，上网一搜，一大批资料就来啦，当然这些都是原始资料，我后面要继续处理。再说答案类型确定，这项工作是为了确定你的提问属于哪一类的，如果你问的是时间、地点，和你问的是技术方案，那我后面要做的处理是不一样的。最后再说这个句法和语义分析，这是对你问题的深层含义做一个剖析，比如你的问题是：聊天机器人怎么做？那么我要知道你要问的是聊天机器人的研发方法

我：原来是这样，提问处理模块这三项工作我了解了，那么检索模块是怎么工作的呢？

chatbot：检索模块跟搜索引擎比较像，就是根据查询关键词所信息检索，返回句子或段落，这部分就是下一步要处理的原料

我：那么答案抽取模块呢？

chatbot：答案抽取模块可以说是计算量最大的部分了，它要通过分析和推理从检索出的句子或段落里抽取出和提问一致的实体，再根据概率最大对候选答案排序，注意这里是“候选答案”噢，也就是很难给出一个完全正确的结果，很有可能给出多个结果，最后还在再选出一个来

我：那么我只要实现这三个模块，就能做成一个你喽？

chatbot：是的

### 聊天机器人的关键技术

我：chatbot，小弟我知识匮乏，能不能告诉我都需要学哪些关键技术才能完成我的梦想

chatbot：小弟。。。我还没满月。说到关键技术，那我可要列一列了，你的任务艰巨了：

1）海量文本知识表示：网络文本资源获取、机器学习方法、大规模语义计算和推理、知识表示体系、知识库构建；

2）问句解析：中文分词、词性标注、实体标注、概念类别标注、句法分析、语义分析、逻辑结构标注、指代消解、关联关系标注、问句分类（简单问句还是复杂问句、实体型还是段落型还是篇章级问题）、答案类别确定；

3）答案生成与过滤：候选答案抽取、关系推演（并列关系还是递进关系还是因果关系）、吻合程度判断、噪声过滤

### 聊天机器人的技术方法

我：chatbot，我对聊天机器人的相关技术总算有所了解了，但是我具体要用什么方法呢？

chatbot：看你这么好学，那我就多给你讲一讲。聊天机器人的技术可以分成四种类型：1）基于检索的技术；2）基于模式匹配的技术；3）基于自然语言理解的技术；4）基于统计翻译模型的技术。这几种技术并不是都要实现，而是选其一，听我给你说说他们的优缺点，你就知道该选哪一种了。基于检索的技术就是信息检索技术，它简单易实现，但无法从句法关系和语义关系给出答案，也就是搞不定推理问题，所以直接pass掉。基于模式匹配的技术就是把问题往已经梳理好的几种模式上去靠，这种做推理简单，但是模式我们涵盖不全，所以也pass掉。基于自然语言理解就是把浅层分析加上句法分析、语义分析都融入进来做的补充和改进。基于统计翻译就是把问句里的疑问词留出来，然后和候选答案资料做配对，能对齐了就是答案，对不齐就对不起了，所以pass掉。选哪个知道了吗？

我：知道了！基于自然语言理解的技术！so easy！妈妈再也不用担心我的学习！o(╯□╰)o

## 半个小时搞定词性标注与关键词提取

想要做到和人聊天，首先得先读懂对方在说什么，所以问句解析是整个聊天过程的第一步，问句解析是一个涉及知识非常全面的过程，几乎涵盖了自然语言处理的全部，本节让我们尝试一下如何分析一个问句 

### 问句解析的过程

一般问句解析需要进行分词、词性标注、命名实体识别、关键词提取、句法分析以及查询问句分类等。这些事情我们从头开始做无非是重复造轮子，傻子才会这么做，人之所以为人是因为会使用工具。网络上有关中文的NLP工具有很多，介绍几个不错的：

第一个要数哈工大的LTP(语言技术平台)了，它可以做中文分词、词性标注、命名实体识别、依存句法分析、语义角色标注等丰富、 高效、精准的自然语言处理技术

第二个就是博森科技了，它除了做中文分词、词性标注、命名实体识别、依存文法之外还可以做情感分析、关键词提取、新闻分类、语义联想、时间转换、新闻摘要等，但因为是商业化的公司，除了分词和词性标注免费之外全都收费

第三个就是jieba分词，这个开源小工具分词和词性标注做的挺不错的，但是其他方面还欠缺一下，如果只是中文分词的需求完全可以满足

第四个就是中科院张华平博士的NLPIR汉语分词系统，也能支持关键词提取

我们优先选择NLPIR

### NLPIR使用

文档在 http://pynlpir.readthedocs.io/en/latest/ 

首先安装pynlpir库

```
pip install pynlpir
pynlpir update
```

若出现授权错误，则去 https://github.com/NLPIR-team/NLPIR/tree/master/License/license%20for%20a%20month/NLPIR-ICTCLAS%E5%88%86%E8%AF%8D%E7%B3%BB%E7%BB%9F%E6%8E%88%E6%9D%83 下载授权文件`NLPIR.user`， 覆盖到 `/usr/local/lib/python2.7/dist-packages/pynlpir/Data/`

写个小程序测试一下分词效果：

```python
# coding:utf-8

import sys
reload(sys)
sys.setdefaultencoding( "utf-8" )

import pynlpir

pynlpir.open()
s = '聊天机器人到底该怎么做呢？'
segments = pynlpir.segment(s)
for segment in segments:
    print segment[0], '\t', segment[1]

pynlpir.close()
```

```
聊天  verb
机器人     noun
到底  adverb
该   verb
怎么  pronoun
做   verb
呢   modal particle
？   punctuation mark
```

下面我们再继续试下关键词提取效果：

```python
# coding:utf-8

import sys
reload(sys)
sys.setdefaultencoding( "utf-8" )

import pynlpir

pynlpir.open()
s = '聊天机器人到底该怎么做呢？'
key_words = pynlpir.get_key_words(s, weighted=True)
for key_word in key_words:
    print key_word[0], '\t', key_word[1]

pynlpir.close()
```

```
聊天  2.0
机器人     2.0
```

从这个小程序来看，分词和关键词提取效果很好

下面我们再来试验一个，这一次我们把分析功能全打开，部分代码如下：

```python
# coding:utf-8

import sys
reload(sys)
sys.setdefaultencoding( "utf-8" )

import pynlpir

pynlpir.open()
s = '海洋是如何形成的'
segments = pynlpir.segment(s, pos_names='all', pos_english=False)
for segment in segments:
    print segment[0], '\t', segment[1]

key_words = pynlpir.get_key_words(s, weighted=True)
for key_word in key_words:
    print key_word[0], '\t', key_word[1]

pynlpir.close()
```

```
海洋  noun
是   verb:verb 是
如何  pronoun:interrogative pronoun:predicate interrogative pronoun
形成  verb
的   particle:particle 的/底

海洋  2.0
形成  2.0
```

如果我们把segments在加上一个参数pos_english=False，也就是不使用英语，那么输出就是

```
海洋  名词
是   动词:动词"是"
如何  代词:疑问代词:谓词性疑问代词
形成  动词
的   助词:的／底

海洋  2.0
形成  2.0
```

### 解释一下

这里的segment是切词的意思，返回的是tuple(token, pos)，其中token就是切出来的词，pos就是语言属性

调用segment方法指定的pos_names参数可以是'all', 'child', 'parent'，默认是parent， 表示获取该词性的最顶级词性，child表示获取该词性的最具体的信息，all表示获取该词性相关的所有词性信息，相当于从其顶级词性到该词性的一条路径

### 词性分类表

查看nlpir的源代码中的pynlpir/docs/pos_map.rst，可以看出全部词性分类及其子类别如下：

```
    POS_MAP = {
        'n': ('名词', 'noun', {
            'nr': ('人名', 'personal name', {
                'nr1': ('汉语姓氏', 'Chinese surname'),
                'nr2': ('汉语名字', 'Chinese given name'),
                'nrj': ('日语人名', 'Japanese personal name'),
                'nrf': ('音译人名', 'transcribed personal name')
            }),
            'ns': ('地名', 'toponym', {
                'nsf': ('音译地名', 'transcribed toponym'),
            }),
            'nt': ('机构团体名', 'organization/group name'),
            'nz': ('其它专名', 'other proper noun'),
            'nl': ('名词性惯用语', 'noun phrase'),
            'ng': ('名词性语素', 'noun morpheme'),
        }),
        't': ('时间词', 'time word', {
            'tg': ('时间词性语素', 'time morpheme'),
        }),
        's': ('处所词', 'locative word'),
        'f': ('方位词', 'noun of locality'),
        'v': ('动词', 'verb', {
            'vd': ('副动词', 'auxiliary verb'),
            'vn': ('名动词', 'noun-verb'),
            'vshi': ('动词"是"', 'verb 是'),
            'vyou': ('动词"有"', 'verb 有'),
            'vf': ('趋向动词', 'directional verb'),
            'vx': ('行事动词', 'performative verb'),
            'vi': ('不及物动词', 'intransitive verb'),
            'vl': ('动词性惯用语', 'verb phrase'),
            'vg': ('动词性语素', 'verb morpheme'),
        }),
        'a': ('形容词', 'adjective', {
            'ad': ('副形词', 'auxiliary adjective'),
            'an': ('名形词', 'noun-adjective'),
            'ag': ('形容词性语素', 'adjective morpheme'),
            'al': ('形容词性惯用语', 'adjective phrase'),
        }),
        'b': ('区别词', 'distinguishing word', {
            'bl': ('区别词性惯用语', 'distinguishing phrase'),
        }),
        'z': ('状态词', 'status word'),
       'r': ('代词', 'pronoun', {
            'rr': ('人称代词', 'personal pronoun'),
            'rz': ('指示代词', 'demonstrative pronoun', {
                'rzt': ('时间指示代词', 'temporal demonstrative pronoun'),
                'rzs': ('处所指示代词', 'locative demonstrative pronoun'),
                'rzv': ('谓词性指示代词', 'predicate demonstrative pronoun'),
            }),
            'ry': ('疑问代词', 'interrogative pronoun', {
                'ryt': ('时间疑问代词', 'temporal interrogative pronoun'),
                'rys': ('处所疑问代词', 'locative interrogative pronoun'),
                'ryv': ('谓词性疑问代词', 'predicate interrogative pronoun'),
            }),
            'rg': ('代词性语素', 'pronoun morpheme'),
        }),
        'm': ('数词', 'numeral', {
            'mq': ('数量词', 'numeral-plus-classifier compound'),
        }),
        'q': ('量词', 'classifier', {
            'qv': ('动量词', 'verbal classifier'),
            'qt': ('时量词', 'temporal classifier'),
        }),
        'd': ('副词', 'adverb'),
        'p': ('介词', 'preposition', {
            'pba': ('介词“把”', 'preposition 把'),
            'pbei': ('介词“被”', 'preposition 被'),
        }),
        'c': ('连词', 'conjunction', {
            'cc': ('并列连词', 'coordinating conjunction'),
        }),
        'u': ('助词', 'particle', {
            'uzhe': ('着', 'particle 着'),
            'ule': ('了／喽', 'particle 了/喽'),
            'uguo': ('过', 'particle 过'),
            'ude1': ('的／底', 'particle 的/底'),
            'ude2': ('地', 'particle 地'),
            'ude3': ('得', 'particle 得'),
            'usuo': ('所', 'particle 所'),
            'udeng': ('等／等等／云云', 'particle 等/等等/云云'),
            'uyy': ('一样／一般／似的／般', 'particle 一样/一般/似的/般'),
            'udh': ('的话', 'particle 的话'),
            'uls': ('来讲／来说／而言／说来', 'particle 来讲/来说/而言/说来'),
            'uzhi': ('之', 'particle 之'),
            'ulian': ('连', 'particle 连'),
        }),
       'e': ('叹词', 'interjection'),
        'y': ('语气词', 'modal particle'),
        'o': ('拟声词', 'onomatopoeia'),
        'h': ('前缀', 'prefix'),
        'k': ('后缀' 'suffix'),
        'x': ('字符串', 'string', {
            'xe': ('Email字符串', 'email address'),
            'xs': ('微博会话分隔符', 'hashtag'),
            'xm': ('表情符合', 'emoticon'),
            'xu': ('网址URL', 'URL'),
            'xx': ('非语素字', 'non-morpheme character'),
        }),
        'w': ('标点符号', 'punctuation mark', {
            'wkz': ('左括号', 'left parenthesis/bracket'),
            'wky': ('右括号', 'right parenthesis/bracket'),
            'wyz': ('左引号', 'left quotation mark'),
            'wyy': ('右引号', 'right quotation mark'),
            'wj': ('句号', 'period'),
            'ww': ('问号', 'question mark'),
            'wt': ('叹号', 'exclamation mark'),
            'wd': ('逗号', 'comma'),
            'wf': ('分号', 'semicolon'),
            'wn': ('顿号', 'enumeration comma'),
            'wm': ('冒号', 'colon'),
            'ws': ('省略号', 'ellipsis'),
            'wp': ('破折号', 'dash'),
            'wb': ('百分号千分号', 'percent/per mille sign'),
            'wh': ('单位符号', 'unit of measure sign'),
        }),
    }
```

好，这回我们一下子完成了分词、词性标注、关键词提取。命名实体识别、句法分析以及查询问句分类我们之后再研究

输出文本，输出提切结果，

```
# coding:utf-8

import sys
reload(sys)
sys.setdefaultencoding( "utf-8" )

import pynlpir

pynlpir.open()
s = raw_input('请输入语句：')
segments = pynlpir.segment(s, pos_names='all', pos_english=False)
for segment in segments:
    print segment[0], '\t', segment[1]

key_words = pynlpir.get_key_words(s, weighted=True)
for key_word in key_words:
    print key_word[0], '\t', key_word[1]

pynlpir.close()
```
